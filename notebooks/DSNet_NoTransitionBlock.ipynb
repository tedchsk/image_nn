{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35db0662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPUs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import ast\n",
    "import time\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor \n",
    "from torchsummary import summary\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPUs\")\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40935789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "{'train': 50000, 'test': 5000, 'validation': 5000}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(43)\n",
    "batch_size = 32\n",
    "\n",
    "### for CIFAR 10\n",
    "# stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "## for CIFAR 100\n",
    "stats = ((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(*stats),\n",
    "    torchvision.transforms.RandomCrop(32, padding=4, padding_mode='constant'),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR100(root=\"data\", train=True, download=True, transform=transform)\n",
    "train_size = len(train_set)\n",
    "test_set = torchvision.datasets.CIFAR100(root=\"data\", train=False, download=True, transform=transform)\n",
    "test_set, validation_set = torch.utils.data.random_split(test_set, [5000, 5000])\n",
    "test_size = len(test_set)\n",
    "validation_size = len(validation_set)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size, num_workers=4, pin_memory=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "data_loaders = {\"train\": train_loader, \"test\": test_loader, \"validation\": validation_loader}\n",
    "dataset_sizes = {\"train\": train_size, \"test\": test_size, \"validation\": validation_size}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd9fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic DSNet. Given input [in_channels, height, width], \n",
    "    - First pass through Conv2d(in_channels, in_channels) + BatchNorm + ReLU \n",
    "        -> Output dimensions: [in_channels, height, width] (1)\n",
    "    - Then, add with the (\"normalization and channel-wise weight\")(input)\n",
    "        -> Output dimensions: [in_channels, height width] (2)\n",
    "    - Pass through another Conv2d(outchannels, outchannels) + BN + ReLU\n",
    "        -> Output dimensions: [in_channels, height, width] (3)\n",
    "    - Add again with (\"normalized + channel-wise weight\")(1) and (\"normalized + channel-wise weight\")(2)\n",
    "        -> Output dimensions: [in_channels, height, width]\n",
    "    Caveat: The normalization and channel-wise weight is not shared.\n",
    "    Attributes:\n",
    "        in_planes: # of Input channels\n",
    "        n_models: Number of layers. Have to specify here as we need to connect all the layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, planes, n_models, device=torch.device(\"cpu\"), stride=1, down=False, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.channel_wise_w_list = []  # Result is list of list of weights at each steps\n",
    "        self.norm_layers = nn.ModuleList([])\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        if down:\n",
    "            inplanes = planes//2\n",
    "            self.downsample = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            inplanes = planes\n",
    "\n",
    "        for i in range(n_models):\n",
    "            if i == 0:\n",
    "                first_conv = nn.Conv2d(inplanes, planes, kernel_size=3, padding=1, stride=stride)\n",
    "            else:\n",
    "                first_conv = nn.Conv2d(planes, planes, kernel_size=3, padding=1, stride=1)\n",
    "            \n",
    "            self.layers.append(nn.Sequential(\n",
    "                first_conv,\n",
    "                nn.BatchNorm2d(planes),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(planes, planes, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            ))\n",
    "\n",
    "            self.norm_layers.append(\n",
    "                nn.ModuleList([nn.GroupNorm(num_groups=4, num_channels=planes).to(device) for _ in range(i+1)])\n",
    "            )\n",
    "\n",
    "            # One variable for each channel for each time, [[w00], [w10, w11], [w20, w21, w22], ...]\n",
    "            self.channel_wise_w_list.append(\n",
    "                [nn.Parameter(torch.randn(1, planes, 1, 1).to(device), requires_grad=True)\n",
    "                 for _ in range(i+1)]\n",
    "            )\n",
    "            \n",
    "            for j, p_list in enumerate(self.channel_wise_w_list):\n",
    "                for k, p in enumerate(p_list):\n",
    "                    self.register_parameter(\"channel_weight_{}_{}\".format(j,k), p)\n",
    "                       \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            original_x = x\n",
    "            x = self.downsample(x)\n",
    "        \n",
    "        # Consisting of output of each layer.\n",
    "        outputs = [x]\n",
    "        \n",
    "        for i,(layer, ch_ws, norm_layer) in enumerate\\\n",
    "        (zip(self.layers, self.channel_wise_w_list, self.norm_layers)):\n",
    "            \n",
    "            if i==0 and self.downsample is not None:\n",
    "                output = layer(original_x)\n",
    "            else:\n",
    "                output = layer(outputs[-1])\n",
    "\n",
    "            assert len(outputs) == len(ch_ws), \"Length not equal\"\n",
    "            dense_normalized_inputs = [norm(o) * ch_weight\n",
    "                                       for o, ch_weight, norm in zip(outputs, ch_ws, norm_layer)]\n",
    "            for dense_normalized_input in dense_normalized_inputs:\n",
    "\n",
    "                output += dense_normalized_input\n",
    "\n",
    "            output = self.relu(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "        return outputs[-1]\n",
    "\n",
    "\n",
    "\n",
    "class DSNet(nn.Module):\n",
    "    \"\"\"Defining the whole model. \n",
    "    In high level: \n",
    "        - Input -> [batch, 3, height, width]\n",
    "        - Beginning Layer -> [batch, 3, height, width]\n",
    "        - First Block: n*BasicBlock(16) -> [batch, 16, height, width]\n",
    "        - Transition: TransitionBlock(16, 32) -> [batch, 32, height, width]\n",
    "        - Second Block: n*BasicBlock(32) -> [batch, 32, height, width]\n",
    "        - Transition: TransitionBlock(32, 64) -> [batch, 32, height, width]\n",
    "        - Third Block: n*BasicBlock(64) -> [batch, 64, height, width]\n",
    "        - FinalLayer: AdaptiveAvgPool2d + Linear(64, num_classes)\n",
    "    Attributes:\n",
    "        model_n: # of layers, based on CIFAR-ResNet \n",
    "        num_classes: Number of classes\n",
    "        device: needed for GPU vs CPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_n, num_classes: int = 10, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_layers = nn.ModuleList([])\n",
    "        self.model_n = model_n\n",
    "        self.device = device\n",
    "\n",
    "        # begining layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # ResNet blocks [16, 32, 64]\n",
    "        # first block, 16 channels\n",
    "        self.residual_layers.append(BasicBlock(16, self.model_n, device).to(device))\n",
    "        \n",
    "        # second block, 32 channels\n",
    "        self.residual_layers.append(BasicBlock(32, self.model_n, device, stride=2, down=True).to(device))\n",
    "\n",
    "        # third block, 64 channels\n",
    "        self.residual_layers.append(BasicBlock(64, self.model_n, device, stride=2, down=True).to(device))\n",
    "\n",
    "\n",
    "        # output layers\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # begining layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # DSNet blocks\n",
    "        for i, layer in enumerate(self.residual_layers):\n",
    "            x = layer(x)\n",
    "\n",
    "        # output layers\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438e57c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             448\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "              ReLU-3           [-1, 16, 32, 32]               0\n",
      "            Conv2d-4           [-1, 16, 32, 32]           2,320\n",
      "       BatchNorm2d-5           [-1, 16, 32, 32]              32\n",
      "              ReLU-6           [-1, 16, 32, 32]               0\n",
      "            Conv2d-7           [-1, 16, 32, 32]           2,320\n",
      "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
      "         GroupNorm-9           [-1, 16, 32, 32]              32\n",
      "             ReLU-10           [-1, 16, 32, 32]               0\n",
      "           Conv2d-11           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-12           [-1, 16, 32, 32]              32\n",
      "             ReLU-13           [-1, 16, 32, 32]               0\n",
      "           Conv2d-14           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-15           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-16           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-17           [-1, 16, 32, 32]              32\n",
      "             ReLU-18           [-1, 16, 32, 32]               0\n",
      "           Conv2d-19           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-20           [-1, 16, 32, 32]              32\n",
      "             ReLU-21           [-1, 16, 32, 32]               0\n",
      "           Conv2d-22           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-23           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-24           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-25           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-26           [-1, 16, 32, 32]              32\n",
      "             ReLU-27           [-1, 16, 32, 32]               0\n",
      "           Conv2d-28           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-29           [-1, 16, 32, 32]              32\n",
      "             ReLU-30           [-1, 16, 32, 32]               0\n",
      "           Conv2d-31           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-32           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-33           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-34           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-35           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-36           [-1, 16, 32, 32]              32\n",
      "             ReLU-37           [-1, 16, 32, 32]               0\n",
      "           Conv2d-38           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-39           [-1, 16, 32, 32]              32\n",
      "             ReLU-40           [-1, 16, 32, 32]               0\n",
      "           Conv2d-41           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-42           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-43           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-44           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-45           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-46           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-47           [-1, 16, 32, 32]              32\n",
      "             ReLU-48           [-1, 16, 32, 32]               0\n",
      "           Conv2d-49           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-50           [-1, 16, 32, 32]              32\n",
      "             ReLU-51           [-1, 16, 32, 32]               0\n",
      "           Conv2d-52           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-53           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-54           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-55           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-56           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-57           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-58           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-59           [-1, 16, 32, 32]              32\n",
      "             ReLU-60           [-1, 16, 32, 32]               0\n",
      "           Conv2d-61           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-62           [-1, 16, 32, 32]              32\n",
      "             ReLU-63           [-1, 16, 32, 32]               0\n",
      "           Conv2d-64           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-65           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-66           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-67           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-68           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-69           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-70           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-71           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-72           [-1, 16, 32, 32]              32\n",
      "             ReLU-73           [-1, 16, 32, 32]               0\n",
      "           Conv2d-74           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-75           [-1, 16, 32, 32]              32\n",
      "             ReLU-76           [-1, 16, 32, 32]               0\n",
      "           Conv2d-77           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-78           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-79           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-80           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-81           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-82           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-83           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-84           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-85           [-1, 16, 32, 32]              32\n",
      "        GroupNorm-86           [-1, 16, 32, 32]              32\n",
      "             ReLU-87           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-88           [-1, 16, 32, 32]               0\n",
      "           Conv2d-89           [-1, 32, 16, 16]             544\n",
      "           Conv2d-90           [-1, 32, 16, 16]           4,640\n",
      "      BatchNorm2d-91           [-1, 32, 16, 16]              64\n",
      "             ReLU-92           [-1, 32, 16, 16]               0\n",
      "           Conv2d-93           [-1, 32, 16, 16]           9,248\n",
      "      BatchNorm2d-94           [-1, 32, 16, 16]              64\n",
      "        GroupNorm-95           [-1, 32, 16, 16]              64\n",
      "             ReLU-96           [-1, 32, 16, 16]               0\n",
      "           Conv2d-97           [-1, 32, 16, 16]           9,248\n",
      "      BatchNorm2d-98           [-1, 32, 16, 16]              64\n",
      "             ReLU-99           [-1, 32, 16, 16]               0\n",
      "          Conv2d-100           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-101           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-102           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-103           [-1, 32, 16, 16]              64\n",
      "            ReLU-104           [-1, 32, 16, 16]               0\n",
      "          Conv2d-105           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-106           [-1, 32, 16, 16]              64\n",
      "            ReLU-107           [-1, 32, 16, 16]               0\n",
      "          Conv2d-108           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-109           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-110           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-111           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-112           [-1, 32, 16, 16]              64\n",
      "            ReLU-113           [-1, 32, 16, 16]               0\n",
      "          Conv2d-114           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-115           [-1, 32, 16, 16]              64\n",
      "            ReLU-116           [-1, 32, 16, 16]               0\n",
      "          Conv2d-117           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-118           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-119           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-120           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-121           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-122           [-1, 32, 16, 16]              64\n",
      "            ReLU-123           [-1, 32, 16, 16]               0\n",
      "          Conv2d-124           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-125           [-1, 32, 16, 16]              64\n",
      "            ReLU-126           [-1, 32, 16, 16]               0\n",
      "          Conv2d-127           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-128           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-129           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-130           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-131           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-132           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-133           [-1, 32, 16, 16]              64\n",
      "            ReLU-134           [-1, 32, 16, 16]               0\n",
      "          Conv2d-135           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-136           [-1, 32, 16, 16]              64\n",
      "            ReLU-137           [-1, 32, 16, 16]               0\n",
      "          Conv2d-138           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-139           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-140           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-141           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-142           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-143           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-144           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-145           [-1, 32, 16, 16]              64\n",
      "            ReLU-146           [-1, 32, 16, 16]               0\n",
      "          Conv2d-147           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-148           [-1, 32, 16, 16]              64\n",
      "            ReLU-149           [-1, 32, 16, 16]               0\n",
      "          Conv2d-150           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-151           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-152           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-153           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-154           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-155           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-156           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-157           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-158           [-1, 32, 16, 16]              64\n",
      "            ReLU-159           [-1, 32, 16, 16]               0\n",
      "          Conv2d-160           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-161           [-1, 32, 16, 16]              64\n",
      "            ReLU-162           [-1, 32, 16, 16]               0\n",
      "          Conv2d-163           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-164           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-165           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-166           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-167           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-168           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-169           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-170           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-171           [-1, 32, 16, 16]              64\n",
      "       GroupNorm-172           [-1, 32, 16, 16]              64\n",
      "            ReLU-173           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-174           [-1, 32, 16, 16]               0\n",
      "          Conv2d-175             [-1, 64, 8, 8]           2,112\n",
      "          Conv2d-176             [-1, 64, 8, 8]          18,496\n",
      "     BatchNorm2d-177             [-1, 64, 8, 8]             128\n",
      "            ReLU-178             [-1, 64, 8, 8]               0\n",
      "          Conv2d-179             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-180             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-181             [-1, 64, 8, 8]             128\n",
      "            ReLU-182             [-1, 64, 8, 8]               0\n",
      "          Conv2d-183             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-184             [-1, 64, 8, 8]             128\n",
      "            ReLU-185             [-1, 64, 8, 8]               0\n",
      "          Conv2d-186             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-187             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-188             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-189             [-1, 64, 8, 8]             128\n",
      "            ReLU-190             [-1, 64, 8, 8]               0\n",
      "          Conv2d-191             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-192             [-1, 64, 8, 8]             128\n",
      "            ReLU-193             [-1, 64, 8, 8]               0\n",
      "          Conv2d-194             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-195             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-196             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-197             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-198             [-1, 64, 8, 8]             128\n",
      "            ReLU-199             [-1, 64, 8, 8]               0\n",
      "          Conv2d-200             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-201             [-1, 64, 8, 8]             128\n",
      "            ReLU-202             [-1, 64, 8, 8]               0\n",
      "          Conv2d-203             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-204             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-205             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-206             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-207             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-208             [-1, 64, 8, 8]             128\n",
      "            ReLU-209             [-1, 64, 8, 8]               0\n",
      "          Conv2d-210             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-211             [-1, 64, 8, 8]             128\n",
      "            ReLU-212             [-1, 64, 8, 8]               0\n",
      "          Conv2d-213             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-214             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-215             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-216             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-217             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-218             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-219             [-1, 64, 8, 8]             128\n",
      "            ReLU-220             [-1, 64, 8, 8]               0\n",
      "          Conv2d-221             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-222             [-1, 64, 8, 8]             128\n",
      "            ReLU-223             [-1, 64, 8, 8]               0\n",
      "          Conv2d-224             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-225             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-226             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-227             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-228             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-229             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-230             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-231             [-1, 64, 8, 8]             128\n",
      "            ReLU-232             [-1, 64, 8, 8]               0\n",
      "          Conv2d-233             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-234             [-1, 64, 8, 8]             128\n",
      "            ReLU-235             [-1, 64, 8, 8]               0\n",
      "          Conv2d-236             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-237             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-238             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-239             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-240             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-241             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-242             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-243             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-244             [-1, 64, 8, 8]             128\n",
      "            ReLU-245             [-1, 64, 8, 8]               0\n",
      "          Conv2d-246             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-247             [-1, 64, 8, 8]             128\n",
      "            ReLU-248             [-1, 64, 8, 8]               0\n",
      "          Conv2d-249             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-250             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-251             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-252             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-253             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-254             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-255             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-256             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-257             [-1, 64, 8, 8]             128\n",
      "       GroupNorm-258             [-1, 64, 8, 8]             128\n",
      "            ReLU-259             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-260             [-1, 64, 8, 8]               0\n",
      "AdaptiveAvgPool2d-261             [-1, 64, 1, 1]               0\n",
      "          Linear-262                  [-1, 100]           6,500\n",
      "================================================================\n",
      "Total params: 774,180\n",
      "Trainable params: 774,180\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 19.06\n",
      "Params size (MB): 2.95\n",
      "Estimated Total Size (MB): 22.03\n",
      "----------------------------------------------------------------\n",
      "Total Number of Parameters: 778212\n"
     ]
    }
   ],
   "source": [
    "#### Train Configurations, based on DSNet and ResNet paper\n",
    "model_n = 8\n",
    "epochs = 100 ### should be 180\n",
    "milestones = [int(epochs*0.5), int(epochs*0.75)]\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "gamma = 0.1\n",
    "lr = 0.1\n",
    "\n",
    "model = DSNet(model_n, num_classes=100, device=device)\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "summary(model, (3, 32, 32))\n",
    "print('Total Number of Parameters:', sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2354be0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:  model:ResNet(small)  model_n: 16  batch size: 32  optimizer:SGD  lr: 0.1  epochs: 100\n",
      "----------------------------- Train --------------------------------\n",
      "Epoch 1/100\n",
      "------------------------------\n",
      "{'time': 279.71594, 'train_loss': 4.01924, 'train_acc': 0.08278, 'val_loss': 3.5675, 'val_acc': 0.148}\n",
      "Epoch 2/100\n",
      "------------------------------\n",
      "{'time': 292.86714, 'train_loss': 3.30653, 'train_acc': 0.19474, 'val_loss': 3.19971, 'val_acc': 0.2188}\n",
      "Epoch 3/100\n",
      "------------------------------\n",
      "{'time': 283.46673, 'train_loss': 2.9321, 'train_acc': 0.26618, 'val_loss': 2.77584, 'val_acc': 0.3004}\n",
      "Epoch 4/100\n",
      "------------------------------\n",
      "{'time': 289.4087, 'train_loss': 2.62582, 'train_acc': 0.32358, 'val_loss': 2.60833, 'val_acc': 0.338}\n",
      "Epoch 5/100\n",
      "------------------------------\n",
      "{'time': 287.04795, 'train_loss': 2.38185, 'train_acc': 0.37352, 'val_loss': 2.40697, 'val_acc': 0.377}\n",
      "Epoch 6/100\n",
      "------------------------------\n",
      "{'time': 285.86358, 'train_loss': 2.18485, 'train_acc': 0.4187, 'val_loss': 2.17742, 'val_acc': 0.4182}\n",
      "Epoch 7/100\n",
      "------------------------------\n",
      "{'time': 291.26085, 'train_loss': 2.033, 'train_acc': 0.45334, 'val_loss': 2.06672, 'val_acc': 0.4574}\n",
      "Epoch 8/100\n",
      "------------------------------\n",
      "{'time': 283.48974, 'train_loss': 1.90816, 'train_acc': 0.48002, 'val_loss': 1.94815, 'val_acc': 0.4804}\n",
      "Epoch 9/100\n",
      "------------------------------\n",
      "{'time': 295.78143, 'train_loss': 1.79731, 'train_acc': 0.5071, 'val_loss': 1.91883, 'val_acc': 0.4778}\n",
      "Epoch 10/100\n",
      "------------------------------\n",
      "{'time': 274.10009, 'train_loss': 1.70842, 'train_acc': 0.52624, 'val_loss': 1.86977, 'val_acc': 0.5074}\n",
      "Epoch 11/100\n",
      "------------------------------\n",
      "{'time': 291.85367, 'train_loss': 1.62718, 'train_acc': 0.54486, 'val_loss': 1.89755, 'val_acc': 0.5014}\n",
      "Epoch 12/100\n",
      "------------------------------\n",
      "{'time': 279.77089, 'train_loss': 1.55822, 'train_acc': 0.56166, 'val_loss': 1.70277, 'val_acc': 0.5414}\n",
      "Epoch 13/100\n",
      "------------------------------\n",
      "{'time': 293.44129, 'train_loss': 1.4944, 'train_acc': 0.57784, 'val_loss': 1.67454, 'val_acc': 0.5434}\n",
      "Epoch 14/100\n",
      "------------------------------\n",
      "{'time': 286.88862, 'train_loss': 1.43932, 'train_acc': 0.58776, 'val_loss': 1.71401, 'val_acc': 0.54}\n",
      "Epoch 15/100\n",
      "------------------------------\n",
      "{'time': 285.71909, 'train_loss': 1.39019, 'train_acc': 0.60178, 'val_loss': 1.60829, 'val_acc': 0.5582}\n",
      "Epoch 16/100\n",
      "------------------------------\n",
      "{'time': 286.17251, 'train_loss': 1.3374, 'train_acc': 0.61498, 'val_loss': 1.62461, 'val_acc': 0.5638}\n",
      "Epoch 17/100\n",
      "------------------------------\n",
      "{'time': 284.11122, 'train_loss': 1.29358, 'train_acc': 0.62626, 'val_loss': 1.72252, 'val_acc': 0.5472}\n",
      "Epoch 18/100\n",
      "------------------------------\n",
      "{'time': 292.41913, 'train_loss': 1.24349, 'train_acc': 0.64092, 'val_loss': 1.60494, 'val_acc': 0.5602}\n",
      "Epoch 19/100\n",
      "------------------------------\n",
      "{'time': 277.41919, 'train_loss': 1.20839, 'train_acc': 0.64992, 'val_loss': 1.58854, 'val_acc': 0.575}\n",
      "Epoch 20/100\n",
      "------------------------------\n",
      "{'time': 295.05322, 'train_loss': 1.17486, 'train_acc': 0.65704, 'val_loss': 1.61312, 'val_acc': 0.5714}\n",
      "Epoch 21/100\n",
      "------------------------------\n",
      "{'time': 276.03584, 'train_loss': 1.14602, 'train_acc': 0.66452, 'val_loss': 1.5444, 'val_acc': 0.5912}\n",
      "Epoch 22/100\n",
      "------------------------------\n",
      "{'time': 292.68884, 'train_loss': 1.10266, 'train_acc': 0.67352, 'val_loss': 1.50779, 'val_acc': 0.5932}\n",
      "Epoch 23/100\n",
      "------------------------------\n",
      "{'time': 279.37951, 'train_loss': 1.07944, 'train_acc': 0.68182, 'val_loss': 1.5194, 'val_acc': 0.5854}\n",
      "Epoch 24/100\n",
      "------------------------------\n",
      "{'time': 291.41705, 'train_loss': 1.04385, 'train_acc': 0.69022, 'val_loss': 1.46972, 'val_acc': 0.6086}\n",
      "Epoch 25/100\n",
      "------------------------------\n",
      "{'time': 288.55626, 'train_loss': 1.02187, 'train_acc': 0.69666, 'val_loss': 1.58676, 'val_acc': 0.5838}\n",
      "Epoch 26/100\n",
      "------------------------------\n",
      "{'time': 291.40165, 'train_loss': 0.98893, 'train_acc': 0.70236, 'val_loss': 1.47189, 'val_acc': 0.6028}\n",
      "Epoch 27/100\n",
      "------------------------------\n",
      "{'time': 290.6249, 'train_loss': 0.96454, 'train_acc': 0.71134, 'val_loss': 1.52489, 'val_acc': 0.6024}\n",
      "Epoch 28/100\n",
      "------------------------------\n",
      "{'time': 285.47747, 'train_loss': 0.93902, 'train_acc': 0.71952, 'val_loss': 1.51231, 'val_acc': 0.6014}\n",
      "Epoch 29/100\n",
      "------------------------------\n",
      "{'time': 293.27627, 'train_loss': 0.92155, 'train_acc': 0.72166, 'val_loss': 1.55129, 'val_acc': 0.5956}\n",
      "Epoch 30/100\n",
      "------------------------------\n",
      "{'time': 282.07997, 'train_loss': 0.88609, 'train_acc': 0.73102, 'val_loss': 1.5671, 'val_acc': 0.6072}\n",
      "Epoch 31/100\n",
      "------------------------------\n",
      "{'time': 297.88973, 'train_loss': 0.86498, 'train_acc': 0.73906, 'val_loss': 1.53529, 'val_acc': 0.6004}\n",
      "Epoch 32/100\n",
      "------------------------------\n",
      "{'time': 275.49001, 'train_loss': 0.8579, 'train_acc': 0.73826, 'val_loss': 1.64262, 'val_acc': 0.5926}\n",
      "Epoch 33/100\n",
      "------------------------------\n",
      "{'time': 295.45555, 'train_loss': 0.83008, 'train_acc': 0.74678, 'val_loss': 1.53088, 'val_acc': 0.6056}\n",
      "Epoch 34/100\n",
      "------------------------------\n",
      "{'time': 281.77529, 'train_loss': 0.81711, 'train_acc': 0.74934, 'val_loss': 1.5377, 'val_acc': 0.6122}\n",
      "Epoch 35/100\n",
      "------------------------------\n",
      "{'time': 295.31181, 'train_loss': 0.78685, 'train_acc': 0.75646, 'val_loss': 1.54254, 'val_acc': 0.6132}\n",
      "Epoch 36/100\n",
      "------------------------------\n",
      "{'time': 286.99365, 'train_loss': 0.78243, 'train_acc': 0.75898, 'val_loss': 1.56437, 'val_acc': 0.6112}\n",
      "Epoch 37/100\n",
      "------------------------------\n",
      "{'time': 293.34631, 'train_loss': 0.75952, 'train_acc': 0.76638, 'val_loss': 1.63481, 'val_acc': 0.6064}\n",
      "Epoch 38/100\n",
      "------------------------------\n",
      "{'time': 292.69826, 'train_loss': 0.74417, 'train_acc': 0.76876, 'val_loss': 1.58837, 'val_acc': 0.622}\n",
      "Epoch 39/100\n",
      "------------------------------\n",
      "{'time': 289.64125, 'train_loss': 0.7199, 'train_acc': 0.7758, 'val_loss': 1.55458, 'val_acc': 0.614}\n",
      "Epoch 40/100\n",
      "------------------------------\n",
      "{'time': 291.75659, 'train_loss': 0.70843, 'train_acc': 0.77886, 'val_loss': 1.58495, 'val_acc': 0.6164}\n",
      "Epoch 41/100\n",
      "------------------------------\n",
      "{'time': 286.19564, 'train_loss': 0.69186, 'train_acc': 0.78534, 'val_loss': 1.57529, 'val_acc': 0.614}\n",
      "Epoch 42/100\n",
      "------------------------------\n",
      "{'time': 295.23311, 'train_loss': 0.67613, 'train_acc': 0.79058, 'val_loss': 1.55117, 'val_acc': 0.6138}\n",
      "Epoch 43/100\n",
      "------------------------------\n",
      "{'time': 282.49215, 'train_loss': 0.66542, 'train_acc': 0.79232, 'val_loss': 1.58139, 'val_acc': 0.617}\n",
      "Epoch 44/100\n",
      "------------------------------\n",
      "{'time': 293.24578, 'train_loss': 0.64377, 'train_acc': 0.79814, 'val_loss': 1.61034, 'val_acc': 0.6144}\n",
      "Epoch 45/100\n",
      "------------------------------\n",
      "{'time': 281.51, 'train_loss': 0.63201, 'train_acc': 0.80154, 'val_loss': 1.62471, 'val_acc': 0.6172}\n",
      "Epoch 46/100\n",
      "------------------------------\n",
      "{'time': 296.1278, 'train_loss': 0.61788, 'train_acc': 0.80332, 'val_loss': 1.61395, 'val_acc': 0.6186}\n",
      "Epoch 47/100\n",
      "------------------------------\n",
      "{'time': 281.44702, 'train_loss': 0.60862, 'train_acc': 0.80738, 'val_loss': 1.58878, 'val_acc': 0.6146}\n",
      "Epoch 48/100\n",
      "------------------------------\n",
      "{'time': 292.21481, 'train_loss': 0.59939, 'train_acc': 0.80802, 'val_loss': 1.56994, 'val_acc': 0.6294}\n",
      "Epoch 49/100\n",
      "------------------------------\n",
      "{'time': 283.84981, 'train_loss': 0.58258, 'train_acc': 0.81392, 'val_loss': 1.57306, 'val_acc': 0.6272}\n",
      "Epoch 50/100\n",
      "------------------------------\n",
      "{'time': 292.23861, 'train_loss': 0.57146, 'train_acc': 0.81886, 'val_loss': 1.56253, 'val_acc': 0.6302}\n",
      "Epoch 51/100\n",
      "------------------------------\n",
      "{'time': 285.06109, 'train_loss': 0.37386, 'train_acc': 0.8805, 'val_loss': 1.46137, 'val_acc': 0.67}\n",
      "Epoch 52/100\n",
      "------------------------------\n",
      "{'time': 288.07225, 'train_loss': 0.30402, 'train_acc': 0.90362, 'val_loss': 1.46802, 'val_acc': 0.6646}\n",
      "Epoch 53/100\n",
      "------------------------------\n",
      "{'time': 287.78562, 'train_loss': 0.27625, 'train_acc': 0.91124, 'val_loss': 1.50659, 'val_acc': 0.6744}\n",
      "Epoch 54/100\n",
      "------------------------------\n",
      "{'time': 287.92667, 'train_loss': 0.259, 'train_acc': 0.9164, 'val_loss': 1.50838, 'val_acc': 0.6714}\n",
      "Epoch 55/100\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 287.78348, 'train_loss': 0.24747, 'train_acc': 0.92142, 'val_loss': 1.55283, 'val_acc': 0.6694}\n",
      "Epoch 56/100\n",
      "------------------------------\n",
      "{'time': 288.41983, 'train_loss': 0.2316, 'train_acc': 0.92496, 'val_loss': 1.60121, 'val_acc': 0.6682}\n",
      "Epoch 57/100\n",
      "------------------------------\n",
      "{'time': 295.69739, 'train_loss': 0.22243, 'train_acc': 0.9282, 'val_loss': 1.60443, 'val_acc': 0.6688}\n",
      "Epoch 58/100\n",
      "------------------------------\n",
      "{'time': 281.26394, 'train_loss': 0.21477, 'train_acc': 0.93126, 'val_loss': 1.61174, 'val_acc': 0.6742}\n",
      "Epoch 59/100\n",
      "------------------------------\n",
      "{'time': 299.68252, 'train_loss': 0.20878, 'train_acc': 0.932, 'val_loss': 1.61015, 'val_acc': 0.6754}\n",
      "Epoch 60/100\n",
      "------------------------------\n",
      "{'time': 285.37876, 'train_loss': 0.20375, 'train_acc': 0.93278, 'val_loss': 1.67472, 'val_acc': 0.6594}\n",
      "Epoch 61/100\n",
      "------------------------------\n",
      "{'time': 297.01413, 'train_loss': 0.19225, 'train_acc': 0.93768, 'val_loss': 1.62396, 'val_acc': 0.6768}\n",
      "Epoch 62/100\n",
      "------------------------------\n",
      "{'time': 271.02047, 'train_loss': 0.19239, 'train_acc': 0.93746, 'val_loss': 1.69501, 'val_acc': 0.6694}\n",
      "Epoch 63/100\n",
      "------------------------------\n",
      "{'time': 292.03603, 'train_loss': 0.18491, 'train_acc': 0.94004, 'val_loss': 1.705, 'val_acc': 0.6788}\n",
      "Epoch 64/100\n",
      "------------------------------\n",
      "{'time': 281.6856, 'train_loss': 0.17806, 'train_acc': 0.94112, 'val_loss': 1.72558, 'val_acc': 0.6654}\n",
      "Epoch 65/100\n",
      "------------------------------\n",
      "{'time': 294.72986, 'train_loss': 0.17397, 'train_acc': 0.94278, 'val_loss': 1.70287, 'val_acc': 0.6768}\n",
      "Epoch 66/100\n",
      "------------------------------\n",
      "{'time': 289.77559, 'train_loss': 0.17058, 'train_acc': 0.9431, 'val_loss': 1.71545, 'val_acc': 0.677}\n",
      "Epoch 67/100\n",
      "------------------------------\n",
      "{'time': 293.4054, 'train_loss': 0.16843, 'train_acc': 0.94438, 'val_loss': 1.75431, 'val_acc': 0.6728}\n",
      "Epoch 68/100\n",
      "------------------------------\n",
      "{'time': 289.8932, 'train_loss': 0.16201, 'train_acc': 0.94626, 'val_loss': 1.74317, 'val_acc': 0.668}\n",
      "Epoch 69/100\n",
      "------------------------------\n",
      "{'time': 288.02086, 'train_loss': 0.15283, 'train_acc': 0.95014, 'val_loss': 1.76229, 'val_acc': 0.6722}\n",
      "Epoch 70/100\n",
      "------------------------------\n",
      "{'time': 295.01833, 'train_loss': 0.15413, 'train_acc': 0.94886, 'val_loss': 1.78983, 'val_acc': 0.669}\n",
      "Epoch 71/100\n",
      "------------------------------\n",
      "{'time': 288.47436, 'train_loss': 0.15131, 'train_acc': 0.94974, 'val_loss': 1.78304, 'val_acc': 0.6718}\n",
      "Epoch 72/100\n",
      "------------------------------\n",
      "{'time': 292.68975, 'train_loss': 0.14583, 'train_acc': 0.9524, 'val_loss': 1.76344, 'val_acc': 0.674}\n",
      "Epoch 73/100\n",
      "------------------------------\n",
      "{'time': 285.56419, 'train_loss': 0.14065, 'train_acc': 0.95298, 'val_loss': 1.87746, 'val_acc': 0.6672}\n",
      "Epoch 74/100\n",
      "------------------------------\n",
      "{'time': 295.20501, 'train_loss': 0.13868, 'train_acc': 0.95428, 'val_loss': 1.86707, 'val_acc': 0.666}\n",
      "Epoch 75/100\n",
      "------------------------------\n",
      "{'time': 278.92053, 'train_loss': 0.13984, 'train_acc': 0.95324, 'val_loss': 1.82204, 'val_acc': 0.6702}\n",
      "Epoch 76/100\n",
      "------------------------------\n",
      "{'time': 292.4495, 'train_loss': 0.12732, 'train_acc': 0.95772, 'val_loss': 1.86807, 'val_acc': 0.673}\n",
      "Epoch 77/100\n",
      "------------------------------\n",
      "{'time': 273.7234, 'train_loss': 0.12438, 'train_acc': 0.95962, 'val_loss': 1.82899, 'val_acc': 0.6732}\n",
      "Epoch 78/100\n",
      "------------------------------\n",
      "{'time': 297.9284, 'train_loss': 0.12007, 'train_acc': 0.96138, 'val_loss': 1.83512, 'val_acc': 0.676}\n",
      "Epoch 79/100\n",
      "------------------------------\n",
      "{'time': 283.8475, 'train_loss': 0.12016, 'train_acc': 0.96058, 'val_loss': 1.80843, 'val_acc': 0.676}\n",
      "Epoch 80/100\n",
      "------------------------------\n",
      "{'time': 293.5657, 'train_loss': 0.11862, 'train_acc': 0.96138, 'val_loss': 1.85161, 'val_acc': 0.6718}\n",
      "Epoch 81/100\n",
      "------------------------------\n",
      "{'time': 284.49783, 'train_loss': 0.1233, 'train_acc': 0.95882, 'val_loss': 1.80553, 'val_acc': 0.6802}\n",
      "Epoch 82/100\n",
      "------------------------------\n",
      "{'time': 292.94728, 'train_loss': 0.1165, 'train_acc': 0.9626, 'val_loss': 1.82709, 'val_acc': 0.6726}\n",
      "Epoch 83/100\n",
      "------------------------------\n",
      "{'time': 288.82838, 'train_loss': 0.11443, 'train_acc': 0.96328, 'val_loss': 1.81598, 'val_acc': 0.6778}\n",
      "Epoch 84/100\n",
      "------------------------------\n",
      "{'time': 291.24759, 'train_loss': 0.11743, 'train_acc': 0.9618, 'val_loss': 1.84105, 'val_acc': 0.6768}\n",
      "Epoch 85/100\n",
      "------------------------------\n",
      "{'time': 290.13319, 'train_loss': 0.11913, 'train_acc': 0.96124, 'val_loss': 1.83995, 'val_acc': 0.669}\n",
      "Epoch 86/100\n",
      "------------------------------\n",
      "{'time': 284.21894, 'train_loss': 0.11396, 'train_acc': 0.96354, 'val_loss': 1.81633, 'val_acc': 0.6798}\n",
      "Epoch 87/100\n",
      "------------------------------\n",
      "{'time': 289.64888, 'train_loss': 0.11041, 'train_acc': 0.96456, 'val_loss': 1.85263, 'val_acc': 0.6746}\n",
      "Epoch 88/100\n",
      "------------------------------\n",
      "{'time': 283.64463, 'train_loss': 0.11656, 'train_acc': 0.96246, 'val_loss': 1.83055, 'val_acc': 0.6764}\n",
      "Epoch 89/100\n",
      "------------------------------\n",
      "{'time': 292.24102, 'train_loss': 0.114, 'train_acc': 0.96396, 'val_loss': 1.82872, 'val_acc': 0.6708}\n",
      "Epoch 90/100\n",
      "------------------------------\n",
      "{'time': 285.0874, 'train_loss': 0.11497, 'train_acc': 0.96238, 'val_loss': 1.83222, 'val_acc': 0.6758}\n",
      "Epoch 91/100\n",
      "------------------------------\n",
      "{'time': 301.18161, 'train_loss': 0.11188, 'train_acc': 0.96326, 'val_loss': 1.87739, 'val_acc': 0.6682}\n",
      "Epoch 92/100\n",
      "------------------------------\n",
      "{'time': 277.15532, 'train_loss': 0.11178, 'train_acc': 0.96354, 'val_loss': 1.88991, 'val_acc': 0.674}\n",
      "Epoch 93/100\n",
      "------------------------------\n",
      "{'time': 299.03186, 'train_loss': 0.11039, 'train_acc': 0.96428, 'val_loss': 1.85052, 'val_acc': 0.6714}\n",
      "Epoch 94/100\n",
      "------------------------------\n",
      "{'time': 281.86666, 'train_loss': 0.11058, 'train_acc': 0.96456, 'val_loss': 1.82174, 'val_acc': 0.6804}\n",
      "Epoch 95/100\n",
      "------------------------------\n",
      "{'time': 292.1187, 'train_loss': 0.10889, 'train_acc': 0.96408, 'val_loss': 1.87142, 'val_acc': 0.6752}\n",
      "Epoch 96/100\n",
      "------------------------------\n",
      "{'time': 282.92721, 'train_loss': 0.1094, 'train_acc': 0.96426, 'val_loss': 1.86753, 'val_acc': 0.6704}\n",
      "Epoch 97/100\n",
      "------------------------------\n",
      "{'time': 293.1435, 'train_loss': 0.10918, 'train_acc': 0.96476, 'val_loss': 1.85241, 'val_acc': 0.6754}\n",
      "Epoch 98/100\n",
      "------------------------------\n",
      "{'time': 284.76393, 'train_loss': 0.10892, 'train_acc': 0.9648, 'val_loss': 1.81242, 'val_acc': 0.6806}\n",
      "Epoch 99/100\n",
      "------------------------------\n",
      "{'time': 284.86908, 'train_loss': 0.10961, 'train_acc': 0.96366, 'val_loss': 1.85882, 'val_acc': 0.6762}\n",
      "Epoch 100/100\n",
      "------------------------------\n",
      "{'time': 286.55164, 'train_loss': 0.10903, 'train_acc': 0.96464, 'val_loss': 1.91931, 'val_acc': 0.6756}\n",
      "----------------------------- Test --------------------------------\n",
      "{'time': 294.80619, 'test_loss': 1.82962, 'test_acc': 0.671}\n"
     ]
    }
   ],
   "source": [
    "### Train loop + validation/ also test at the end\n",
    "print(\"Configuration: \", \"model:ResNet(small)\", \" model_n:\", model_n, \" batch size:\", batch_size, \n",
    "      \" optimizer:SGD\", \" lr:\", lr, \" epochs:\", epochs)\n",
    "\n",
    "all_epoch_loss = {\"train\": [], \"validation\": []}\n",
    "all_epoch_acc = {\"train\":  [], \"validation\": []}\n",
    "\n",
    "print(\"----------------------------- Train --------------------------------\")\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    \n",
    "    epoch_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    epoch_acc = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    \n",
    "    running_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    running_corrects = {\"train\": 0, \"validation\": 0}\n",
    "    \n",
    "    for phase in [\"train\", \"validation\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train(True)\n",
    "        else:\n",
    "            model.train(False)\n",
    "        \n",
    "        for data in data_loaders[phase]:\n",
    "            inputs, labels = data \n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # clear all gradients\n",
    "            \n",
    "            outputs = model(inputs) # batch_size x num_classes\n",
    "            _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                loss.backward()  # compute gradients\n",
    "                optimizer.step() # update weights/biases\n",
    "               \n",
    "            running_loss[phase] += loss.data.item() * inputs.size(0)\n",
    "            running_corrects[phase] += torch.sum(preds == labels.data).item()\n",
    "        \n",
    "        all_epoch_loss[phase].append(running_loss[phase] / dataset_sizes[phase])\n",
    "        all_epoch_acc[phase].append(running_corrects[phase] / dataset_sizes[phase])\n",
    "        \n",
    "        epoch_loss[phase] = running_loss[phase] / dataset_sizes[phase]\n",
    "        epoch_acc[phase] =  running_corrects[phase] / dataset_sizes[phase]\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "        'time': np.round(time.time()-start_time, 5),\n",
    "        'train_loss': np.round(epoch_loss[\"train\"], 5),\n",
    "        'train_acc': np.round(epoch_acc[\"train\"], 5),\n",
    "        'val_loss': np.round(epoch_loss[\"validation\"], 5),\n",
    "        'val_acc': np.round(epoch_acc[\"validation\"], 5),\n",
    "    })\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    \n",
    "with open('DSNet_16_bs_32_opt_SGD_lr_1_epochs_100_loss.txt', 'w') as f_loss:\n",
    "    print(all_epoch_loss, file=f_loss)\n",
    "    \n",
    "with open('DSNet_16_bs_32_opt_SGD_lr_1_epochs_100_acc.txt', 'w') as f_acc:\n",
    "    print(all_epoch_acc, file=f_acc)\n",
    "    \n",
    "### evaluating the model with test set\n",
    "print(\"----------------------------- Test --------------------------------\")\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # clear all gradients\n",
    "\n",
    "        outputs = model(inputs) # batch_size x num_classes\n",
    "        _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        running_loss += loss.data.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "    'time': np.round(time.time()-start_time, 5),\n",
    "    'test_loss': np.round(running_loss/ dataset_sizes['test'], 5),\n",
    "    'test_acc': np.round(running_corrects/ dataset_sizes['test'], 5),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7c296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b30a8995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:  model:ResNet(small)  model_n: 3  batch size: 32  optimizer:SGD  lr: 0.1  epochs: 100\n",
      "----------------------------- Train --------------------------------\n",
      "Epoch 1/100\n",
      "------------------------------\n",
      "{'time': 43.03238, 'train_loss': 3.93896, 'train_acc': 0.08844, 'val_loss': 3.7258, 'val_acc': 0.1158}\n",
      "Epoch 2/100\n",
      "------------------------------\n",
      "{'time': 39.82316, 'train_loss': 3.32306, 'train_acc': 0.18918, 'val_loss': 3.63235, 'val_acc': 0.1532}\n",
      "Epoch 3/100\n",
      "------------------------------\n",
      "{'time': 39.09446, 'train_loss': 2.93877, 'train_acc': 0.25852, 'val_loss': 2.92494, 'val_acc': 0.2666}\n",
      "Epoch 4/100\n",
      "------------------------------\n",
      "{'time': 38.06372, 'train_loss': 2.67967, 'train_acc': 0.3105, 'val_loss': 2.64212, 'val_acc': 0.314}\n",
      "Epoch 5/100\n",
      "------------------------------\n",
      "{'time': 38.86179, 'train_loss': 2.48919, 'train_acc': 0.35034, 'val_loss': 2.58991, 'val_acc': 0.3532}\n",
      "Epoch 6/100\n",
      "------------------------------\n",
      "{'time': 40.15102, 'train_loss': 2.33412, 'train_acc': 0.38354, 'val_loss': 2.45319, 'val_acc': 0.359}\n",
      "Epoch 7/100\n",
      "------------------------------\n",
      "{'time': 38.90187, 'train_loss': 2.20876, 'train_acc': 0.4111, 'val_loss': 2.1928, 'val_acc': 0.4112}\n",
      "Epoch 8/100\n",
      "------------------------------\n",
      "{'time': 38.5698, 'train_loss': 2.10945, 'train_acc': 0.43194, 'val_loss': 2.17309, 'val_acc': 0.4338}\n",
      "Epoch 9/100\n",
      "------------------------------\n",
      "{'time': 47.87181, 'train_loss': 2.00775, 'train_acc': 0.45706, 'val_loss': 2.09093, 'val_acc': 0.4474}\n",
      "Epoch 10/100\n",
      "------------------------------\n",
      "{'time': 39.13417, 'train_loss': 1.93641, 'train_acc': 0.4761, 'val_loss': 2.04997, 'val_acc': 0.4566}\n",
      "Epoch 11/100\n",
      "------------------------------\n",
      "{'time': 38.09655, 'train_loss': 1.86965, 'train_acc': 0.48778, 'val_loss': 2.04516, 'val_acc': 0.457}\n",
      "Epoch 12/100\n",
      "------------------------------\n",
      "{'time': 38.77539, 'train_loss': 1.80146, 'train_acc': 0.50738, 'val_loss': 2.01634, 'val_acc': 0.4594}\n",
      "Epoch 13/100\n",
      "------------------------------\n",
      "{'time': 38.26553, 'train_loss': 1.74928, 'train_acc': 0.51912, 'val_loss': 1.89245, 'val_acc': 0.4906}\n",
      "Epoch 14/100\n",
      "------------------------------\n",
      "{'time': 38.10599, 'train_loss': 1.70303, 'train_acc': 0.52704, 'val_loss': 1.86801, 'val_acc': 0.5016}\n",
      "Epoch 15/100\n",
      "------------------------------\n",
      "{'time': 38.63295, 'train_loss': 1.65837, 'train_acc': 0.54028, 'val_loss': 1.76982, 'val_acc': 0.5162}\n",
      "Epoch 16/100\n",
      "------------------------------\n",
      "{'time': 38.64886, 'train_loss': 1.62023, 'train_acc': 0.54514, 'val_loss': 1.87202, 'val_acc': 0.4994}\n",
      "Epoch 17/100\n",
      "------------------------------\n",
      "{'time': 46.70167, 'train_loss': 1.58396, 'train_acc': 0.55624, 'val_loss': 1.7489, 'val_acc': 0.537}\n",
      "Epoch 18/100\n",
      "------------------------------\n",
      "{'time': 38.4437, 'train_loss': 1.54278, 'train_acc': 0.56464, 'val_loss': 1.69313, 'val_acc': 0.5362}\n",
      "Epoch 19/100\n",
      "------------------------------\n",
      "{'time': 38.57095, 'train_loss': 1.51365, 'train_acc': 0.57544, 'val_loss': 1.71501, 'val_acc': 0.5288}\n",
      "Epoch 20/100\n",
      "------------------------------\n",
      "{'time': 38.03091, 'train_loss': 1.47977, 'train_acc': 0.58132, 'val_loss': 1.67841, 'val_acc': 0.5428}\n",
      "Epoch 21/100\n",
      "------------------------------\n",
      "{'time': 38.93164, 'train_loss': 1.45027, 'train_acc': 0.58914, 'val_loss': 1.66592, 'val_acc': 0.5472}\n",
      "Epoch 22/100\n",
      "------------------------------\n",
      "{'time': 37.84934, 'train_loss': 1.4339, 'train_acc': 0.59266, 'val_loss': 1.70843, 'val_acc': 0.5326}\n",
      "Epoch 23/100\n",
      "------------------------------\n",
      "{'time': 37.77347, 'train_loss': 1.40658, 'train_acc': 0.5994, 'val_loss': 1.72793, 'val_acc': 0.541}\n",
      "Epoch 24/100\n",
      "------------------------------\n",
      "{'time': 38.53587, 'train_loss': 1.38299, 'train_acc': 0.60538, 'val_loss': 1.6719, 'val_acc': 0.5486}\n",
      "Epoch 25/100\n",
      "------------------------------\n",
      "{'time': 46.94427, 'train_loss': 1.35295, 'train_acc': 0.61384, 'val_loss': 1.67629, 'val_acc': 0.5476}\n",
      "Epoch 26/100\n",
      "------------------------------\n",
      "{'time': 38.67256, 'train_loss': 1.33946, 'train_acc': 0.617, 'val_loss': 1.65068, 'val_acc': 0.557}\n",
      "Epoch 27/100\n",
      "------------------------------\n",
      "{'time': 39.27332, 'train_loss': 1.31088, 'train_acc': 0.62456, 'val_loss': 1.62287, 'val_acc': 0.5572}\n",
      "Epoch 28/100\n",
      "------------------------------\n",
      "{'time': 38.70668, 'train_loss': 1.29831, 'train_acc': 0.62596, 'val_loss': 1.59433, 'val_acc': 0.561}\n",
      "Epoch 29/100\n",
      "------------------------------\n",
      "{'time': 38.05832, 'train_loss': 1.2724, 'train_acc': 0.63368, 'val_loss': 1.61883, 'val_acc': 0.5718}\n",
      "Epoch 30/100\n",
      "------------------------------\n",
      "{'time': 38.46212, 'train_loss': 1.2572, 'train_acc': 0.63892, 'val_loss': 1.56778, 'val_acc': 0.577}\n",
      "Epoch 31/100\n",
      "------------------------------\n",
      "{'time': 38.60746, 'train_loss': 1.24543, 'train_acc': 0.63988, 'val_loss': 1.57974, 'val_acc': 0.5764}\n",
      "Epoch 32/100\n",
      "------------------------------\n",
      "{'time': 39.61236, 'train_loss': 1.22999, 'train_acc': 0.64318, 'val_loss': 1.60151, 'val_acc': 0.5696}\n",
      "Epoch 33/100\n",
      "------------------------------\n",
      "{'time': 47.58643, 'train_loss': 1.2146, 'train_acc': 0.64522, 'val_loss': 1.58309, 'val_acc': 0.5744}\n",
      "Epoch 34/100\n",
      "------------------------------\n",
      "{'time': 38.72386, 'train_loss': 1.1964, 'train_acc': 0.65158, 'val_loss': 1.54857, 'val_acc': 0.5866}\n",
      "Epoch 35/100\n",
      "------------------------------\n",
      "{'time': 40.01852, 'train_loss': 1.18564, 'train_acc': 0.6539, 'val_loss': 1.57275, 'val_acc': 0.5748}\n",
      "Epoch 36/100\n",
      "------------------------------\n",
      "{'time': 38.57664, 'train_loss': 1.17784, 'train_acc': 0.65664, 'val_loss': 1.57326, 'val_acc': 0.5732}\n",
      "Epoch 37/100\n",
      "------------------------------\n",
      "{'time': 38.30792, 'train_loss': 1.16077, 'train_acc': 0.66004, 'val_loss': 1.52994, 'val_acc': 0.5912}\n",
      "Epoch 38/100\n",
      "------------------------------\n",
      "{'time': 39.44287, 'train_loss': 1.14421, 'train_acc': 0.66466, 'val_loss': 1.59784, 'val_acc': 0.5726}\n",
      "Epoch 39/100\n",
      "------------------------------\n",
      "{'time': 38.87141, 'train_loss': 1.13333, 'train_acc': 0.66642, 'val_loss': 1.56823, 'val_acc': 0.5832}\n",
      "Epoch 40/100\n",
      "------------------------------\n",
      "{'time': 38.79294, 'train_loss': 1.11828, 'train_acc': 0.67272, 'val_loss': 1.56857, 'val_acc': 0.583}\n",
      "Epoch 41/100\n",
      "------------------------------\n",
      "{'time': 47.09243, 'train_loss': 1.11023, 'train_acc': 0.67462, 'val_loss': 1.56686, 'val_acc': 0.5862}\n",
      "Epoch 42/100\n",
      "------------------------------\n",
      "{'time': 38.29665, 'train_loss': 1.09804, 'train_acc': 0.67676, 'val_loss': 1.55635, 'val_acc': 0.582}\n",
      "Epoch 43/100\n",
      "------------------------------\n",
      "{'time': 40.06135, 'train_loss': 1.09129, 'train_acc': 0.6786, 'val_loss': 1.58857, 'val_acc': 0.59}\n",
      "Epoch 44/100\n",
      "------------------------------\n",
      "{'time': 38.64249, 'train_loss': 1.08245, 'train_acc': 0.68006, 'val_loss': 1.52782, 'val_acc': 0.5918}\n",
      "Epoch 45/100\n",
      "------------------------------\n",
      "{'time': 38.9115, 'train_loss': 1.066, 'train_acc': 0.6857, 'val_loss': 1.54412, 'val_acc': 0.5864}\n",
      "Epoch 46/100\n",
      "------------------------------\n",
      "{'time': 39.06345, 'train_loss': 1.05935, 'train_acc': 0.68714, 'val_loss': 1.55394, 'val_acc': 0.5968}\n",
      "Epoch 47/100\n",
      "------------------------------\n",
      "{'time': 39.63196, 'train_loss': 1.05876, 'train_acc': 0.6875, 'val_loss': 1.56537, 'val_acc': 0.5988}\n",
      "Epoch 48/100\n",
      "------------------------------\n",
      "{'time': 38.58465, 'train_loss': 1.04062, 'train_acc': 0.69276, 'val_loss': 1.51012, 'val_acc': 0.6024}\n",
      "Epoch 49/100\n",
      "------------------------------\n",
      "{'time': 46.4688, 'train_loss': 1.03044, 'train_acc': 0.69688, 'val_loss': 1.56188, 'val_acc': 0.591}\n",
      "Epoch 50/100\n",
      "------------------------------\n",
      "{'time': 39.27644, 'train_loss': 1.02212, 'train_acc': 0.69596, 'val_loss': 1.5002, 'val_acc': 0.611}\n",
      "Epoch 51/100\n",
      "------------------------------\n",
      "{'time': 39.5289, 'train_loss': 0.83566, 'train_acc': 0.75016, 'val_loss': 1.38565, 'val_acc': 0.628}\n",
      "Epoch 52/100\n",
      "------------------------------\n",
      "{'time': 38.39441, 'train_loss': 0.79484, 'train_acc': 0.75912, 'val_loss': 1.40191, 'val_acc': 0.6294}\n",
      "Epoch 53/100\n",
      "------------------------------\n",
      "{'time': 39.02765, 'train_loss': 0.77671, 'train_acc': 0.76558, 'val_loss': 1.39852, 'val_acc': 0.6356}\n",
      "Epoch 54/100\n",
      "------------------------------\n",
      "{'time': 39.00978, 'train_loss': 0.76913, 'train_acc': 0.76594, 'val_loss': 1.42796, 'val_acc': 0.6326}\n",
      "Epoch 55/100\n",
      "------------------------------\n",
      "{'time': 39.95291, 'train_loss': 0.75972, 'train_acc': 0.77064, 'val_loss': 1.44903, 'val_acc': 0.6302}\n",
      "Epoch 56/100\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 38.4888, 'train_loss': 0.74898, 'train_acc': 0.77456, 'val_loss': 1.45537, 'val_acc': 0.6276}\n",
      "Epoch 57/100\n",
      "------------------------------\n",
      "{'time': 47.45086, 'train_loss': 0.74303, 'train_acc': 0.7748, 'val_loss': 1.43415, 'val_acc': 0.6318}\n",
      "Epoch 58/100\n",
      "------------------------------\n",
      "{'time': 39.86901, 'train_loss': 0.74032, 'train_acc': 0.77564, 'val_loss': 1.41765, 'val_acc': 0.638}\n",
      "Epoch 59/100\n",
      "------------------------------\n",
      "{'time': 39.40376, 'train_loss': 0.73168, 'train_acc': 0.77786, 'val_loss': 1.41036, 'val_acc': 0.634}\n",
      "Epoch 60/100\n",
      "------------------------------\n",
      "{'time': 39.53482, 'train_loss': 0.72805, 'train_acc': 0.77914, 'val_loss': 1.44855, 'val_acc': 0.6326}\n",
      "Epoch 61/100\n",
      "------------------------------\n",
      "{'time': 37.93341, 'train_loss': 0.72714, 'train_acc': 0.77912, 'val_loss': 1.44036, 'val_acc': 0.6354}\n",
      "Epoch 62/100\n",
      "------------------------------\n",
      "{'time': 38.85932, 'train_loss': 0.71507, 'train_acc': 0.78402, 'val_loss': 1.45224, 'val_acc': 0.6284}\n",
      "Epoch 63/100\n",
      "------------------------------\n",
      "{'time': 37.06815, 'train_loss': 0.71656, 'train_acc': 0.7825, 'val_loss': 1.4599, 'val_acc': 0.63}\n",
      "Epoch 64/100\n",
      "------------------------------\n",
      "{'time': 37.86697, 'train_loss': 0.72499, 'train_acc': 0.77934, 'val_loss': 1.43811, 'val_acc': 0.633}\n",
      "Epoch 65/100\n",
      "------------------------------\n",
      "{'time': 46.90214, 'train_loss': 0.71108, 'train_acc': 0.7834, 'val_loss': 1.43724, 'val_acc': 0.6386}\n",
      "Epoch 66/100\n",
      "------------------------------\n",
      "{'time': 40.57571, 'train_loss': 0.70559, 'train_acc': 0.78368, 'val_loss': 1.46173, 'val_acc': 0.628}\n",
      "Epoch 67/100\n",
      "------------------------------\n",
      "{'time': 40.45961, 'train_loss': 0.70009, 'train_acc': 0.7861, 'val_loss': 1.44522, 'val_acc': 0.6408}\n",
      "Epoch 68/100\n",
      "------------------------------\n",
      "{'time': 38.53663, 'train_loss': 0.70367, 'train_acc': 0.78488, 'val_loss': 1.45828, 'val_acc': 0.627}\n",
      "Epoch 69/100\n",
      "------------------------------\n",
      "{'time': 38.94065, 'train_loss': 0.70121, 'train_acc': 0.78732, 'val_loss': 1.47791, 'val_acc': 0.6284}\n",
      "Epoch 70/100\n",
      "------------------------------\n",
      "{'time': 38.4203, 'train_loss': 0.6974, 'train_acc': 0.78468, 'val_loss': 1.48876, 'val_acc': 0.625}\n",
      "Epoch 71/100\n",
      "------------------------------\n",
      "{'time': 38.60664, 'train_loss': 0.68976, 'train_acc': 0.7864, 'val_loss': 1.50572, 'val_acc': 0.627}\n",
      "Epoch 72/100\n",
      "------------------------------\n",
      "{'time': 39.81211, 'train_loss': 0.69586, 'train_acc': 0.78554, 'val_loss': 1.47777, 'val_acc': 0.6354}\n",
      "Epoch 73/100\n",
      "------------------------------\n",
      "{'time': 42.91479, 'train_loss': 0.68675, 'train_acc': 0.78856, 'val_loss': 1.46209, 'val_acc': 0.6312}\n",
      "Epoch 74/100\n",
      "------------------------------\n",
      "{'time': 39.08562, 'train_loss': 0.68608, 'train_acc': 0.78904, 'val_loss': 1.47157, 'val_acc': 0.63}\n",
      "Epoch 75/100\n",
      "------------------------------\n",
      "{'time': 39.65428, 'train_loss': 0.68353, 'train_acc': 0.78818, 'val_loss': 1.49301, 'val_acc': 0.629}\n",
      "Epoch 76/100\n",
      "------------------------------\n",
      "{'time': 39.98073, 'train_loss': 0.66587, 'train_acc': 0.79546, 'val_loss': 1.47466, 'val_acc': 0.6292}\n",
      "Epoch 77/100\n",
      "------------------------------\n",
      "{'time': 39.48459, 'train_loss': 0.66165, 'train_acc': 0.79632, 'val_loss': 1.46507, 'val_acc': 0.6304}\n",
      "Epoch 78/100\n",
      "------------------------------\n",
      "{'time': 39.30558, 'train_loss': 0.66155, 'train_acc': 0.798, 'val_loss': 1.45775, 'val_acc': 0.6404}\n",
      "Epoch 79/100\n",
      "------------------------------\n",
      "{'time': 38.40469, 'train_loss': 0.65779, 'train_acc': 0.7964, 'val_loss': 1.48163, 'val_acc': 0.6332}\n",
      "Epoch 80/100\n",
      "------------------------------\n",
      "{'time': 39.57458, 'train_loss': 0.65336, 'train_acc': 0.7998, 'val_loss': 1.45687, 'val_acc': 0.6396}\n",
      "Epoch 81/100\n",
      "------------------------------\n",
      "{'time': 43.24398, 'train_loss': 0.65318, 'train_acc': 0.79952, 'val_loss': 1.47878, 'val_acc': 0.6314}\n",
      "Epoch 82/100\n",
      "------------------------------\n",
      "{'time': 38.34329, 'train_loss': 0.65514, 'train_acc': 0.79818, 'val_loss': 1.46349, 'val_acc': 0.6362}\n",
      "Epoch 83/100\n",
      "------------------------------\n",
      "{'time': 38.2539, 'train_loss': 0.65606, 'train_acc': 0.79718, 'val_loss': 1.494, 'val_acc': 0.6262}\n",
      "Epoch 84/100\n",
      "------------------------------\n",
      "{'time': 38.34038, 'train_loss': 0.64756, 'train_acc': 0.80124, 'val_loss': 1.48629, 'val_acc': 0.6282}\n",
      "Epoch 85/100\n",
      "------------------------------\n",
      "{'time': 38.28181, 'train_loss': 0.65079, 'train_acc': 0.8002, 'val_loss': 1.50272, 'val_acc': 0.6308}\n",
      "Epoch 86/100\n",
      "------------------------------\n",
      "{'time': 38.6024, 'train_loss': 0.65419, 'train_acc': 0.8003, 'val_loss': 1.45847, 'val_acc': 0.6384}\n",
      "Epoch 87/100\n",
      "------------------------------\n",
      "{'time': 39.07643, 'train_loss': 0.64686, 'train_acc': 0.80138, 'val_loss': 1.46019, 'val_acc': 0.6358}\n",
      "Epoch 88/100\n",
      "------------------------------\n",
      "{'time': 39.10784, 'train_loss': 0.65203, 'train_acc': 0.7991, 'val_loss': 1.46514, 'val_acc': 0.628}\n",
      "Epoch 89/100\n",
      "------------------------------\n",
      "{'time': 42.58697, 'train_loss': 0.64951, 'train_acc': 0.7997, 'val_loss': 1.47414, 'val_acc': 0.638}\n",
      "Epoch 90/100\n",
      "------------------------------\n",
      "{'time': 39.80876, 'train_loss': 0.64991, 'train_acc': 0.7988, 'val_loss': 1.47954, 'val_acc': 0.6384}\n",
      "Epoch 91/100\n",
      "------------------------------\n",
      "{'time': 38.41265, 'train_loss': 0.64677, 'train_acc': 0.79912, 'val_loss': 1.47396, 'val_acc': 0.6322}\n",
      "Epoch 92/100\n",
      "------------------------------\n",
      "{'time': 39.76431, 'train_loss': 0.65175, 'train_acc': 0.79972, 'val_loss': 1.49054, 'val_acc': 0.6372}\n",
      "Epoch 93/100\n",
      "------------------------------\n",
      "{'time': 39.47665, 'train_loss': 0.64581, 'train_acc': 0.80026, 'val_loss': 1.47715, 'val_acc': 0.6324}\n",
      "Epoch 94/100\n",
      "------------------------------\n",
      "{'time': 39.80616, 'train_loss': 0.64974, 'train_acc': 0.79928, 'val_loss': 1.48489, 'val_acc': 0.6372}\n",
      "Epoch 95/100\n",
      "------------------------------\n",
      "{'time': 39.81483, 'train_loss': 0.64862, 'train_acc': 0.80174, 'val_loss': 1.4899, 'val_acc': 0.63}\n",
      "Epoch 96/100\n",
      "------------------------------\n",
      "{'time': 39.92754, 'train_loss': 0.65014, 'train_acc': 0.79948, 'val_loss': 1.47833, 'val_acc': 0.6354}\n",
      "Epoch 97/100\n",
      "------------------------------\n",
      "{'time': 42.43529, 'train_loss': 0.64411, 'train_acc': 0.80064, 'val_loss': 1.48978, 'val_acc': 0.6332}\n",
      "Epoch 98/100\n",
      "------------------------------\n",
      "{'time': 38.73421, 'train_loss': 0.64621, 'train_acc': 0.80124, 'val_loss': 1.45455, 'val_acc': 0.6382}\n",
      "Epoch 99/100\n",
      "------------------------------\n",
      "{'time': 38.7508, 'train_loss': 0.65373, 'train_acc': 0.79804, 'val_loss': 1.49491, 'val_acc': 0.6336}\n",
      "Epoch 100/100\n",
      "------------------------------\n",
      "{'time': 38.02149, 'train_loss': 0.64204, 'train_acc': 0.80172, 'val_loss': 1.45746, 'val_acc': 0.6472}\n",
      "----------------------------- Test --------------------------------\n",
      "{'time': 39.32361, 'test_loss': 1.48141, 'test_acc': 0.6214}\n"
     ]
    }
   ],
   "source": [
    "### Train loop + validation/ also test at the end\n",
    "print(\"Configuration: \", \"model:ResNet(small)\", \" model_n:\", model_n, \" batch size:\", batch_size, \n",
    "      \" optimizer:SGD\", \" lr:\", lr, \" epochs:\", epochs)\n",
    "\n",
    "all_epoch_loss = {\"train\": [], \"validation\": []}\n",
    "all_epoch_acc = {\"train\":  [], \"validation\": []}\n",
    "\n",
    "print(\"----------------------------- Train --------------------------------\")\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    \n",
    "    epoch_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    epoch_acc = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    \n",
    "    running_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    running_corrects = {\"train\": 0, \"validation\": 0}\n",
    "    \n",
    "    for phase in [\"train\", \"validation\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train(True)\n",
    "        else:\n",
    "            model.train(False)\n",
    "        \n",
    "        for data in data_loaders[phase]:\n",
    "            inputs, labels = data \n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # clear all gradients\n",
    "            \n",
    "            outputs = model(inputs) # batch_size x num_classes\n",
    "            _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                loss.backward()  # compute gradients\n",
    "                optimizer.step() # update weights/biases\n",
    "               \n",
    "            running_loss[phase] += loss.data.item() * inputs.size(0)\n",
    "            running_corrects[phase] += torch.sum(preds == labels.data).item()\n",
    "        \n",
    "        all_epoch_loss[phase].append(running_loss[phase] / dataset_sizes[phase])\n",
    "        all_epoch_acc[phase].append(running_corrects[phase] / dataset_sizes[phase])\n",
    "        \n",
    "        epoch_loss[phase] = running_loss[phase] / dataset_sizes[phase]\n",
    "        epoch_acc[phase] =  running_corrects[phase] / dataset_sizes[phase]\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "        'time': np.round(time.time()-start_time, 5),\n",
    "        'train_loss': np.round(epoch_loss[\"train\"], 5),\n",
    "        'train_acc': np.round(epoch_acc[\"train\"], 5),\n",
    "        'val_loss': np.round(epoch_loss[\"validation\"], 5),\n",
    "        'val_acc': np.round(epoch_acc[\"validation\"], 5),\n",
    "    })\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    \n",
    "with open('DSNet_3_bs_32_opt_SGD_lr_1_epochs_100_loss.txt', 'w') as f_loss:\n",
    "    print(all_epoch_loss, file=f_loss)\n",
    "    \n",
    "with open('DSNet_3_bs_32_opt_SGD_lr_1_epochs_100_acc.txt', 'w') as f_acc:\n",
    "    print(all_epoch_acc, file=f_acc)\n",
    "    \n",
    "### evaluating the model with test set\n",
    "print(\"----------------------------- Test --------------------------------\")\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # clear all gradients\n",
    "\n",
    "        outputs = model(inputs) # batch_size x num_classes\n",
    "        _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        running_loss += loss.data.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "    'time': np.round(time.time()-start_time, 5),\n",
    "    'test_loss': np.round(running_loss/ dataset_sizes['test'], 5),\n",
    "    'test_acc': np.round(running_corrects/ dataset_sizes['test'], 5),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a82616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f39f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23333cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5a1e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:  model:ResNet(small)  model_n: 8  batch size: 32  optimizer:SGD  lr: 0.1  epochs: 100\n",
      "----------------------------- Train --------------------------------\n",
      "Epoch 1/100\n",
      "------------------------------\n",
      "{'time': 110.94868, 'train_loss': 4.0303, 'train_acc': 0.07534, 'val_loss': 3.60206, 'val_acc': 0.1384}\n",
      "Epoch 2/100\n",
      "------------------------------\n",
      "{'time': 101.95567, 'train_loss': 3.30763, 'train_acc': 0.18874, 'val_loss': 3.16371, 'val_acc': 0.2228}\n",
      "Epoch 3/100\n",
      "------------------------------\n",
      "{'time': 104.07128, 'train_loss': 2.88058, 'train_acc': 0.27182, 'val_loss': 2.70973, 'val_acc': 0.3124}\n",
      "Epoch 4/100\n",
      "------------------------------\n",
      "{'time': 110.60394, 'train_loss': 2.60071, 'train_acc': 0.3298, 'val_loss': 2.5096, 'val_acc': 0.3434}\n",
      "Epoch 5/100\n",
      "------------------------------\n",
      "{'time': 102.97587, 'train_loss': 2.41331, 'train_acc': 0.36602, 'val_loss': 2.43238, 'val_acc': 0.3772}\n",
      "Epoch 6/100\n",
      "------------------------------\n",
      "{'time': 101.99725, 'train_loss': 2.25006, 'train_acc': 0.40192, 'val_loss': 2.32722, 'val_acc': 0.3962}\n",
      "Epoch 7/100\n",
      "------------------------------\n",
      "{'time': 110.95751, 'train_loss': 2.13579, 'train_acc': 0.43088, 'val_loss': 2.17304, 'val_acc': 0.433}\n",
      "Epoch 8/100\n",
      "------------------------------\n",
      "{'time': 103.27335, 'train_loss': 2.0261, 'train_acc': 0.45126, 'val_loss': 2.09133, 'val_acc': 0.4432}\n",
      "Epoch 9/100\n",
      "------------------------------\n",
      "{'time': 101.70973, 'train_loss': 1.93963, 'train_acc': 0.47352, 'val_loss': 2.02831, 'val_acc': 0.466}\n",
      "Epoch 10/100\n",
      "------------------------------\n",
      "{'time': 112.57108, 'train_loss': 1.85819, 'train_acc': 0.49048, 'val_loss': 1.95634, 'val_acc': 0.4692}\n",
      "Epoch 11/100\n",
      "------------------------------\n",
      "{'time': 105.36226, 'train_loss': 1.79409, 'train_acc': 0.50282, 'val_loss': 1.89407, 'val_acc': 0.482}\n",
      "Epoch 12/100\n",
      "------------------------------\n",
      "{'time': 102.59079, 'train_loss': 1.71495, 'train_acc': 0.52462, 'val_loss': 1.85653, 'val_acc': 0.5006}\n",
      "Epoch 13/100\n",
      "------------------------------\n",
      "{'time': 114.39049, 'train_loss': 1.66954, 'train_acc': 0.53448, 'val_loss': 1.83384, 'val_acc': 0.5134}\n",
      "Epoch 14/100\n",
      "------------------------------\n",
      "{'time': 108.43433, 'train_loss': 1.60781, 'train_acc': 0.54998, 'val_loss': 1.83198, 'val_acc': 0.5062}\n",
      "Epoch 15/100\n",
      "------------------------------\n",
      "{'time': 109.13816, 'train_loss': 1.55684, 'train_acc': 0.56312, 'val_loss': 1.74049, 'val_acc': 0.528}\n",
      "Epoch 16/100\n",
      "------------------------------\n",
      "{'time': 115.33022, 'train_loss': 1.51714, 'train_acc': 0.57136, 'val_loss': 1.72048, 'val_acc': 0.5402}\n",
      "Epoch 17/100\n",
      "------------------------------\n",
      "{'time': 113.33337, 'train_loss': 1.46968, 'train_acc': 0.58232, 'val_loss': 1.72086, 'val_acc': 0.5388}\n",
      "Epoch 18/100\n",
      "------------------------------\n",
      "{'time': 112.72853, 'train_loss': 1.42823, 'train_acc': 0.59432, 'val_loss': 1.67424, 'val_acc': 0.548}\n",
      "Epoch 19/100\n",
      "------------------------------\n",
      "{'time': 116.15812, 'train_loss': 1.39718, 'train_acc': 0.60068, 'val_loss': 1.73397, 'val_acc': 0.5382}\n",
      "Epoch 20/100\n",
      "------------------------------\n",
      "{'time': 109.02306, 'train_loss': 1.36113, 'train_acc': 0.60986, 'val_loss': 1.68522, 'val_acc': 0.5522}\n",
      "Epoch 21/100\n",
      "------------------------------\n",
      "{'time': 113.0823, 'train_loss': 1.32732, 'train_acc': 0.61906, 'val_loss': 1.71593, 'val_acc': 0.5486}\n",
      "Epoch 22/100\n",
      "------------------------------\n",
      "{'time': 109.51413, 'train_loss': 1.29785, 'train_acc': 0.62618, 'val_loss': 1.68724, 'val_acc': 0.5468}\n",
      "Epoch 23/100\n",
      "------------------------------\n",
      "{'time': 107.89072, 'train_loss': 1.25818, 'train_acc': 0.63658, 'val_loss': 1.61945, 'val_acc': 0.569}\n",
      "Epoch 24/100\n",
      "------------------------------\n",
      "{'time': 114.5351, 'train_loss': 1.23382, 'train_acc': 0.63874, 'val_loss': 1.59967, 'val_acc': 0.5724}\n",
      "Epoch 25/100\n",
      "------------------------------\n",
      "{'time': 113.34821, 'train_loss': 1.20423, 'train_acc': 0.6504, 'val_loss': 1.60158, 'val_acc': 0.576}\n",
      "Epoch 26/100\n",
      "------------------------------\n",
      "{'time': 112.43552, 'train_loss': 1.18324, 'train_acc': 0.6525, 'val_loss': 1.64424, 'val_acc': 0.5692}\n",
      "Epoch 27/100\n",
      "------------------------------\n",
      "{'time': 115.55485, 'train_loss': 1.164, 'train_acc': 0.65862, 'val_loss': 1.62797, 'val_acc': 0.5678}\n",
      "Epoch 28/100\n",
      "------------------------------\n",
      "{'time': 107.38312, 'train_loss': 1.13358, 'train_acc': 0.667, 'val_loss': 1.61411, 'val_acc': 0.5716}\n",
      "Epoch 29/100\n",
      "------------------------------\n",
      "{'time': 110.72312, 'train_loss': 1.10388, 'train_acc': 0.67354, 'val_loss': 1.59437, 'val_acc': 0.5722}\n",
      "Epoch 30/100\n",
      "------------------------------\n",
      "{'time': 110.6308, 'train_loss': 1.08472, 'train_acc': 0.67892, 'val_loss': 1.61357, 'val_acc': 0.5762}\n",
      "Epoch 31/100\n",
      "------------------------------\n",
      "{'time': 107.47452, 'train_loss': 1.06762, 'train_acc': 0.68406, 'val_loss': 1.50907, 'val_acc': 0.597}\n",
      "Epoch 32/100\n",
      "------------------------------\n",
      "{'time': 108.50164, 'train_loss': 1.05112, 'train_acc': 0.68862, 'val_loss': 1.6376, 'val_acc': 0.573}\n",
      "Epoch 33/100\n",
      "------------------------------\n",
      "{'time': 118.50907, 'train_loss': 1.03664, 'train_acc': 0.69308, 'val_loss': 1.62221, 'val_acc': 0.5788}\n",
      "Epoch 34/100\n",
      "------------------------------\n",
      "{'time': 113.74683, 'train_loss': 1.00847, 'train_acc': 0.70088, 'val_loss': 1.6233, 'val_acc': 0.5858}\n",
      "Epoch 35/100\n",
      "------------------------------\n",
      "{'time': 112.8464, 'train_loss': 0.99175, 'train_acc': 0.70486, 'val_loss': 1.60399, 'val_acc': 0.5834}\n",
      "Epoch 36/100\n",
      "------------------------------\n",
      "{'time': 115.67313, 'train_loss': 0.98079, 'train_acc': 0.70554, 'val_loss': 1.54639, 'val_acc': 0.5904}\n",
      "Epoch 37/100\n",
      "------------------------------\n",
      "{'time': 113.80191, 'train_loss': 0.95195, 'train_acc': 0.71284, 'val_loss': 1.61397, 'val_acc': 0.5906}\n",
      "Epoch 38/100\n",
      "------------------------------\n",
      "{'time': 115.01407, 'train_loss': 0.94931, 'train_acc': 0.7153, 'val_loss': 1.61717, 'val_acc': 0.5804}\n",
      "Epoch 39/100\n",
      "------------------------------\n",
      "{'time': 115.33703, 'train_loss': 0.9295, 'train_acc': 0.71898, 'val_loss': 1.55702, 'val_acc': 0.6028}\n",
      "Epoch 40/100\n",
      "------------------------------\n",
      "{'time': 112.292, 'train_loss': 0.9166, 'train_acc': 0.72132, 'val_loss': 1.58286, 'val_acc': 0.592}\n",
      "Epoch 41/100\n",
      "------------------------------\n",
      "{'time': 114.42215, 'train_loss': 0.90284, 'train_acc': 0.72612, 'val_loss': 1.52769, 'val_acc': 0.6004}\n",
      "Epoch 42/100\n",
      "------------------------------\n",
      "{'time': 120.78909, 'train_loss': 0.87724, 'train_acc': 0.73242, 'val_loss': 1.63081, 'val_acc': 0.5904}\n",
      "Epoch 43/100\n",
      "------------------------------\n",
      "{'time': 112.79947, 'train_loss': 0.87608, 'train_acc': 0.73448, 'val_loss': 1.53905, 'val_acc': 0.605}\n",
      "Epoch 44/100\n",
      "------------------------------\n",
      "{'time': 111.45617, 'train_loss': 0.8592, 'train_acc': 0.73718, 'val_loss': 1.76167, 'val_acc': 0.58}\n",
      "Epoch 45/100\n",
      "------------------------------\n",
      "{'time': 105.3172, 'train_loss': 0.83872, 'train_acc': 0.74274, 'val_loss': 1.64271, 'val_acc': 0.592}\n",
      "Epoch 46/100\n",
      "------------------------------\n",
      "{'time': 103.08905, 'train_loss': 0.83136, 'train_acc': 0.74508, 'val_loss': 1.63995, 'val_acc': 0.5918}\n",
      "Epoch 47/100\n",
      "------------------------------\n",
      "{'time': 108.3798, 'train_loss': 0.81472, 'train_acc': 0.75134, 'val_loss': 1.56248, 'val_acc': 0.6012}\n",
      "Epoch 48/100\n",
      "------------------------------\n",
      "{'time': 104.24692, 'train_loss': 0.80787, 'train_acc': 0.75088, 'val_loss': 1.67592, 'val_acc': 0.5972}\n",
      "Epoch 49/100\n",
      "------------------------------\n",
      "{'time': 104.10245, 'train_loss': 0.79841, 'train_acc': 0.75482, 'val_loss': 1.59376, 'val_acc': 0.6066}\n",
      "Epoch 50/100\n",
      "------------------------------\n",
      "{'time': 109.07309, 'train_loss': 0.78243, 'train_acc': 0.76094, 'val_loss': 1.642, 'val_acc': 0.5864}\n",
      "Epoch 51/100\n",
      "------------------------------\n",
      "{'time': 113.8391, 'train_loss': 0.57925, 'train_acc': 0.8193, 'val_loss': 1.4606, 'val_acc': 0.6376}\n",
      "Epoch 52/100\n",
      "------------------------------\n",
      "{'time': 110.55934, 'train_loss': 0.50985, 'train_acc': 0.84118, 'val_loss': 1.49376, 'val_acc': 0.6388}\n",
      "Epoch 53/100\n",
      "------------------------------\n",
      "{'time': 112.81292, 'train_loss': 0.48885, 'train_acc': 0.84788, 'val_loss': 1.5224, 'val_acc': 0.6374}\n",
      "Epoch 54/100\n",
      "------------------------------\n",
      "{'time': 110.9078, 'train_loss': 0.47249, 'train_acc': 0.85208, 'val_loss': 1.5249, 'val_acc': 0.6362}\n",
      "Epoch 55/100\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 114.85168, 'train_loss': 0.45815, 'train_acc': 0.85558, 'val_loss': 1.51351, 'val_acc': 0.6424}\n",
      "Epoch 56/100\n",
      "------------------------------\n",
      "{'time': 110.55982, 'train_loss': 0.44635, 'train_acc': 0.85822, 'val_loss': 1.53981, 'val_acc': 0.6498}\n",
      "Epoch 57/100\n",
      "------------------------------\n",
      "{'time': 109.60731, 'train_loss': 0.43899, 'train_acc': 0.86186, 'val_loss': 1.57624, 'val_acc': 0.6422}\n",
      "Epoch 58/100\n",
      "------------------------------\n",
      "{'time': 115.54546, 'train_loss': 0.42794, 'train_acc': 0.86502, 'val_loss': 1.56144, 'val_acc': 0.645}\n",
      "Epoch 59/100\n",
      "------------------------------\n",
      "{'time': 112.33629, 'train_loss': 0.42334, 'train_acc': 0.865, 'val_loss': 1.5888, 'val_acc': 0.644}\n",
      "Epoch 60/100\n",
      "------------------------------\n",
      "{'time': 109.69264, 'train_loss': 0.41142, 'train_acc': 0.86996, 'val_loss': 1.58164, 'val_acc': 0.6442}\n",
      "Epoch 61/100\n",
      "------------------------------\n",
      "{'time': 108.66302, 'train_loss': 0.41188, 'train_acc': 0.8681, 'val_loss': 1.60067, 'val_acc': 0.6418}\n",
      "Epoch 62/100\n",
      "------------------------------\n",
      "{'time': 107.88987, 'train_loss': 0.40047, 'train_acc': 0.87196, 'val_loss': 1.57404, 'val_acc': 0.6462}\n",
      "Epoch 63/100\n",
      "------------------------------\n",
      "{'time': 112.3319, 'train_loss': 0.39607, 'train_acc': 0.87172, 'val_loss': 1.61354, 'val_acc': 0.6442}\n",
      "Epoch 64/100\n",
      "------------------------------\n",
      "{'time': 103.21371, 'train_loss': 0.38853, 'train_acc': 0.87568, 'val_loss': 1.58109, 'val_acc': 0.646}\n",
      "Epoch 65/100\n",
      "------------------------------\n",
      "{'time': 103.8976, 'train_loss': 0.38634, 'train_acc': 0.87618, 'val_loss': 1.62581, 'val_acc': 0.6498}\n",
      "Epoch 66/100\n",
      "------------------------------\n",
      "{'time': 107.52857, 'train_loss': 0.37803, 'train_acc': 0.8792, 'val_loss': 1.62788, 'val_acc': 0.643}\n",
      "Epoch 67/100\n",
      "------------------------------\n",
      "{'time': 109.57324, 'train_loss': 0.37907, 'train_acc': 0.8779, 'val_loss': 1.61801, 'val_acc': 0.6484}\n",
      "Epoch 68/100\n",
      "------------------------------\n",
      "{'time': 109.24589, 'train_loss': 0.37176, 'train_acc': 0.8807, 'val_loss': 1.64928, 'val_acc': 0.6388}\n",
      "Epoch 69/100\n",
      "------------------------------\n",
      "{'time': 108.9595, 'train_loss': 0.36601, 'train_acc': 0.88186, 'val_loss': 1.6344, 'val_acc': 0.6412}\n",
      "Epoch 70/100\n",
      "------------------------------\n",
      "{'time': 111.2213, 'train_loss': 0.3615, 'train_acc': 0.88384, 'val_loss': 1.70049, 'val_acc': 0.6356}\n",
      "Epoch 71/100\n",
      "------------------------------\n",
      "{'time': 114.56233, 'train_loss': 0.35754, 'train_acc': 0.88508, 'val_loss': 1.67089, 'val_acc': 0.6396}\n",
      "Epoch 72/100\n",
      "------------------------------\n",
      "{'time': 120.75309, 'train_loss': 0.35177, 'train_acc': 0.88566, 'val_loss': 1.68387, 'val_acc': 0.636}\n",
      "Epoch 73/100\n",
      "------------------------------\n",
      "{'time': 102.98027, 'train_loss': 0.34836, 'train_acc': 0.88594, 'val_loss': 1.71694, 'val_acc': 0.6348}\n",
      "Epoch 74/100\n",
      "------------------------------\n",
      "{'time': 110.08235, 'train_loss': 0.34647, 'train_acc': 0.88852, 'val_loss': 1.71404, 'val_acc': 0.6316}\n",
      "Epoch 75/100\n",
      "------------------------------\n",
      "{'time': 103.27648, 'train_loss': 0.34061, 'train_acc': 0.8889, 'val_loss': 1.73373, 'val_acc': 0.6378}\n",
      "Epoch 76/100\n",
      "------------------------------\n",
      "{'time': 103.22411, 'train_loss': 0.32504, 'train_acc': 0.89512, 'val_loss': 1.72692, 'val_acc': 0.6442}\n",
      "Epoch 77/100\n",
      "------------------------------\n",
      "{'time': 113.92842, 'train_loss': 0.321, 'train_acc': 0.8964, 'val_loss': 1.72222, 'val_acc': 0.636}\n",
      "Epoch 78/100\n",
      "------------------------------\n",
      "{'time': 103.019, 'train_loss': 0.31647, 'train_acc': 0.89782, 'val_loss': 1.69523, 'val_acc': 0.6468}\n",
      "Epoch 79/100\n",
      "------------------------------\n",
      "{'time': 103.73441, 'train_loss': 0.31716, 'train_acc': 0.89596, 'val_loss': 1.71038, 'val_acc': 0.6448}\n",
      "Epoch 80/100\n",
      "------------------------------\n",
      "{'time': 111.25543, 'train_loss': 0.3113, 'train_acc': 0.89954, 'val_loss': 1.69183, 'val_acc': 0.6428}\n",
      "Epoch 81/100\n",
      "------------------------------\n",
      "{'time': 104.31426, 'train_loss': 0.314, 'train_acc': 0.89668, 'val_loss': 1.66113, 'val_acc': 0.6524}\n",
      "Epoch 82/100\n",
      "------------------------------\n",
      "{'time': 103.80195, 'train_loss': 0.30744, 'train_acc': 0.90122, 'val_loss': 1.72608, 'val_acc': 0.6394}\n",
      "Epoch 83/100\n",
      "------------------------------\n",
      "{'time': 112.86774, 'train_loss': 0.3069, 'train_acc': 0.89966, 'val_loss': 1.68158, 'val_acc': 0.6442}\n",
      "Epoch 84/100\n",
      "------------------------------\n",
      "{'time': 104.28146, 'train_loss': 0.31163, 'train_acc': 0.90058, 'val_loss': 1.68254, 'val_acc': 0.6528}\n",
      "Epoch 85/100\n",
      "------------------------------\n",
      "{'time': 103.07234, 'train_loss': 0.30304, 'train_acc': 0.90154, 'val_loss': 1.72738, 'val_acc': 0.6416}\n",
      "Epoch 86/100\n",
      "------------------------------\n",
      "{'time': 111.98313, 'train_loss': 0.30422, 'train_acc': 0.9011, 'val_loss': 1.73224, 'val_acc': 0.643}\n",
      "Epoch 87/100\n",
      "------------------------------\n",
      "{'time': 102.65058, 'train_loss': 0.30942, 'train_acc': 0.90042, 'val_loss': 1.74869, 'val_acc': 0.6402}\n",
      "Epoch 88/100\n",
      "------------------------------\n",
      "{'time': 102.23639, 'train_loss': 0.30559, 'train_acc': 0.90028, 'val_loss': 1.72507, 'val_acc': 0.6456}\n",
      "Epoch 89/100\n",
      "------------------------------\n",
      "{'time': 111.83047, 'train_loss': 0.30123, 'train_acc': 0.9015, 'val_loss': 1.69364, 'val_acc': 0.6396}\n",
      "Epoch 90/100\n",
      "------------------------------\n",
      "{'time': 104.09802, 'train_loss': 0.30576, 'train_acc': 0.90048, 'val_loss': 1.7488, 'val_acc': 0.6402}\n",
      "Epoch 91/100\n",
      "------------------------------\n",
      "{'time': 102.81048, 'train_loss': 0.30043, 'train_acc': 0.9026, 'val_loss': 1.74968, 'val_acc': 0.6372}\n",
      "Epoch 92/100\n",
      "------------------------------\n",
      "{'time': 113.20203, 'train_loss': 0.30235, 'train_acc': 0.90028, 'val_loss': 1.72638, 'val_acc': 0.646}\n",
      "Epoch 93/100\n",
      "------------------------------\n",
      "{'time': 104.22192, 'train_loss': 0.30296, 'train_acc': 0.90306, 'val_loss': 1.69903, 'val_acc': 0.6464}\n",
      "Epoch 94/100\n",
      "------------------------------\n",
      "{'time': 103.65817, 'train_loss': 0.30174, 'train_acc': 0.90164, 'val_loss': 1.72417, 'val_acc': 0.6444}\n",
      "Epoch 95/100\n",
      "------------------------------\n",
      "{'time': 115.01439, 'train_loss': 0.29994, 'train_acc': 0.90246, 'val_loss': 1.70656, 'val_acc': 0.6492}\n",
      "Epoch 96/100\n",
      "------------------------------\n",
      "{'time': 102.87812, 'train_loss': 0.29953, 'train_acc': 0.90302, 'val_loss': 1.73064, 'val_acc': 0.644}\n",
      "Epoch 97/100\n",
      "------------------------------\n",
      "{'time': 102.68072, 'train_loss': 0.30177, 'train_acc': 0.90046, 'val_loss': 1.73328, 'val_acc': 0.6388}\n",
      "Epoch 98/100\n",
      "------------------------------\n",
      "{'time': 112.76827, 'train_loss': 0.29828, 'train_acc': 0.9032, 'val_loss': 1.73427, 'val_acc': 0.6386}\n",
      "Epoch 99/100\n",
      "------------------------------\n",
      "{'time': 102.70347, 'train_loss': 0.29854, 'train_acc': 0.90168, 'val_loss': 1.74934, 'val_acc': 0.6528}\n",
      "Epoch 100/100\n",
      "------------------------------\n",
      "{'time': 101.59163, 'train_loss': 0.29754, 'train_acc': 0.90406, 'val_loss': 1.73617, 'val_acc': 0.6382}\n",
      "----------------------------- Test --------------------------------\n",
      "{'time': 105.01082, 'test_loss': 1.72968, 'test_acc': 0.6406}\n"
     ]
    }
   ],
   "source": [
    "### Train loop + validation/ also test at the end\n",
    "print(\"Configuration: \", \"model:ResNet(small)\", \" model_n:\", model_n, \" batch size:\", batch_size, \n",
    "      \" optimizer:SGD\", \" lr:\", lr, \" epochs:\", epochs)\n",
    "\n",
    "all_epoch_loss = {\"train\": [], \"validation\": []}\n",
    "all_epoch_acc = {\"train\":  [], \"validation\": []}\n",
    "\n",
    "print(\"----------------------------- Train --------------------------------\")\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    \n",
    "    epoch_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    epoch_acc = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    \n",
    "    running_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    running_corrects = {\"train\": 0, \"validation\": 0}\n",
    "    \n",
    "    for phase in [\"train\", \"validation\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train(True)\n",
    "        else:\n",
    "            model.train(False)\n",
    "        \n",
    "        for data in data_loaders[phase]:\n",
    "            inputs, labels = data \n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # clear all gradients\n",
    "            \n",
    "            outputs = model(inputs) # batch_size x num_classes\n",
    "            _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                loss.backward()  # compute gradients\n",
    "                optimizer.step() # update weights/biases\n",
    "               \n",
    "            running_loss[phase] += loss.data.item() * inputs.size(0)\n",
    "            running_corrects[phase] += torch.sum(preds == labels.data).item()\n",
    "        \n",
    "        all_epoch_loss[phase].append(running_loss[phase] / dataset_sizes[phase])\n",
    "        all_epoch_acc[phase].append(running_corrects[phase] / dataset_sizes[phase])\n",
    "        \n",
    "        epoch_loss[phase] = running_loss[phase] / dataset_sizes[phase]\n",
    "        epoch_acc[phase] =  running_corrects[phase] / dataset_sizes[phase]\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "        'time': np.round(time.time()-start_time, 5),\n",
    "        'train_loss': np.round(epoch_loss[\"train\"], 5),\n",
    "        'train_acc': np.round(epoch_acc[\"train\"], 5),\n",
    "        'val_loss': np.round(epoch_loss[\"validation\"], 5),\n",
    "        'val_acc': np.round(epoch_acc[\"validation\"], 5),\n",
    "    })\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    \n",
    "with open('DSNet_8_bs_32_opt_SGD_lr_1_epochs_100_loss.txt', 'w') as f_loss:\n",
    "    print(all_epoch_loss, file=f_loss)\n",
    "    \n",
    "with open('DSNet_8_bs_32_opt_SGD_lr_1_epochs_100_acc.txt', 'w') as f_acc:\n",
    "    print(all_epoch_acc, file=f_acc)\n",
    "    \n",
    "### evaluating the model with test set\n",
    "print(\"----------------------------- Test --------------------------------\")\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # clear all gradients\n",
    "\n",
    "        outputs = model(inputs) # batch_size x num_classes\n",
    "        _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        running_loss += loss.data.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "    'time': np.round(time.time()-start_time, 5),\n",
    "    'test_loss': np.round(running_loss/ dataset_sizes['test'], 5),\n",
    "    'test_acc': np.round(running_corrects/ dataset_sizes['test'], 5),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0bca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
