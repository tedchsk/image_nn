{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ac7781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPUs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import ast\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor \n",
    "from torchsummary import summary\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPUs\")\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ed7353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "{'train': 50000, 'test': 5000, 'validation': 5000}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(43)\n",
    "batch_size = 32\n",
    "\n",
    "### for CIFAR 10\n",
    "# stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "## for CIFAR 100\n",
    "stats = ((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(*stats),\n",
    "    torchvision.transforms.RandomCrop(32, padding=4, padding_mode='constant'),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR100(root=\"data\", train=True, download=True, transform=transform)\n",
    "train_size = len(train_set)\n",
    "test_set = torchvision.datasets.CIFAR100(root=\"data\", train=False, download=True, transform=transform)\n",
    "test_set, validation_set = torch.utils.data.random_split(test_set, [5000, 5000])\n",
    "test_size = len(test_set)\n",
    "validation_size = len(validation_set)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size, num_workers=4, pin_memory=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "data_loaders = {\"train\": train_loader, \"test\": test_loader, \"validation\": validation_loader}\n",
    "dataset_sizes = {\"train\": train_size, \"test\": test_size, \"validation\": validation_size}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa29595",
   "metadata": {},
   "outputs": [],
   "source": [
    "### from https://pytorch.org/hub/pytorch_vision_resnet/\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, down=False):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, stride=stride, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.downsample = None\n",
    "        \n",
    "        if down:\n",
    "            self.downsample = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride)\n",
    "        \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, model_n, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_layers = nn.ModuleList([])\n",
    "        self.model_n = model_n\n",
    "\n",
    "        ### begining layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding='same')\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        \n",
    "        ######## ResNet blocks [16, 32, 64]\n",
    "        ### first block, 16 channels\n",
    "        for i in range(self.model_n):\n",
    "            self.residual_layers.append(BasicBlock(16, 16).to(device))\n",
    "            \n",
    "        \n",
    "        ### second block, 32 channels\n",
    "        for i in range(self.model_n):\n",
    "            if i == 0:\n",
    "                self.residual_layers.append(BasicBlock(16, 32, stride=2, down=True).to(device))\n",
    "            else:\n",
    "                self.residual_layers.append(BasicBlock(32, 32).to(device))\n",
    "                \n",
    "                \n",
    "        ### third block, 64 channels\n",
    "        for i in range(self.model_n):\n",
    "            if i == 0:\n",
    "                self.residual_layers.append(BasicBlock(32, 64, stride=2, down=True).to(device))\n",
    "                self.inplanes = 64\n",
    "            else:\n",
    "                self.residual_layers.append(BasicBlock(64, 64).to(device))\n",
    "        \n",
    "    \n",
    "        ### output layers\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        ### begining layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        ##### ResNet blocks\n",
    "        for i, layer in enumerate(self.residual_layers):\n",
    "            x = layer (x)\n",
    "            \n",
    "            \n",
    "        ### output layers\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b696f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             448\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "              ReLU-3           [-1, 16, 32, 32]               0\n",
      "            Conv2d-4           [-1, 16, 32, 32]           2,320\n",
      "       BatchNorm2d-5           [-1, 16, 32, 32]              32\n",
      "              ReLU-6           [-1, 16, 32, 32]               0\n",
      "            Conv2d-7           [-1, 16, 32, 32]           2,320\n",
      "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
      "              ReLU-9           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-10           [-1, 16, 32, 32]               0\n",
      "           Conv2d-11           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-12           [-1, 16, 32, 32]              32\n",
      "             ReLU-13           [-1, 16, 32, 32]               0\n",
      "           Conv2d-14           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-15           [-1, 16, 32, 32]              32\n",
      "             ReLU-16           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-17           [-1, 16, 32, 32]               0\n",
      "           Conv2d-18           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-19           [-1, 16, 32, 32]              32\n",
      "             ReLU-20           [-1, 16, 32, 32]               0\n",
      "           Conv2d-21           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-22           [-1, 16, 32, 32]              32\n",
      "             ReLU-23           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-24           [-1, 16, 32, 32]               0\n",
      "           Conv2d-25           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-26           [-1, 16, 32, 32]              32\n",
      "             ReLU-27           [-1, 16, 32, 32]               0\n",
      "           Conv2d-28           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-29           [-1, 16, 32, 32]              32\n",
      "             ReLU-30           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-31           [-1, 16, 32, 32]               0\n",
      "           Conv2d-32           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-33           [-1, 16, 32, 32]              32\n",
      "             ReLU-34           [-1, 16, 32, 32]               0\n",
      "           Conv2d-35           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-36           [-1, 16, 32, 32]              32\n",
      "             ReLU-37           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-38           [-1, 16, 32, 32]               0\n",
      "           Conv2d-39           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-40           [-1, 16, 32, 32]              32\n",
      "             ReLU-41           [-1, 16, 32, 32]               0\n",
      "           Conv2d-42           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-43           [-1, 16, 32, 32]              32\n",
      "             ReLU-44           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-45           [-1, 16, 32, 32]               0\n",
      "           Conv2d-46           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-47           [-1, 16, 32, 32]              32\n",
      "             ReLU-48           [-1, 16, 32, 32]               0\n",
      "           Conv2d-49           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-50           [-1, 16, 32, 32]              32\n",
      "             ReLU-51           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-52           [-1, 16, 32, 32]               0\n",
      "           Conv2d-53           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-54           [-1, 16, 32, 32]              32\n",
      "             ReLU-55           [-1, 16, 32, 32]               0\n",
      "           Conv2d-56           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-57           [-1, 16, 32, 32]              32\n",
      "             ReLU-58           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-59           [-1, 16, 32, 32]               0\n",
      "           Conv2d-60           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-61           [-1, 16, 32, 32]              32\n",
      "             ReLU-62           [-1, 16, 32, 32]               0\n",
      "           Conv2d-63           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-64           [-1, 16, 32, 32]              32\n",
      "             ReLU-65           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-66           [-1, 16, 32, 32]               0\n",
      "           Conv2d-67           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-68           [-1, 16, 32, 32]              32\n",
      "             ReLU-69           [-1, 16, 32, 32]               0\n",
      "           Conv2d-70           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-71           [-1, 16, 32, 32]              32\n",
      "             ReLU-72           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-73           [-1, 16, 32, 32]               0\n",
      "           Conv2d-74           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-75           [-1, 16, 32, 32]              32\n",
      "             ReLU-76           [-1, 16, 32, 32]               0\n",
      "           Conv2d-77           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-78           [-1, 16, 32, 32]              32\n",
      "             ReLU-79           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-80           [-1, 16, 32, 32]               0\n",
      "           Conv2d-81           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-82           [-1, 16, 32, 32]              32\n",
      "             ReLU-83           [-1, 16, 32, 32]               0\n",
      "           Conv2d-84           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-85           [-1, 16, 32, 32]              32\n",
      "             ReLU-86           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-87           [-1, 16, 32, 32]               0\n",
      "           Conv2d-88           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-89           [-1, 16, 32, 32]              32\n",
      "             ReLU-90           [-1, 16, 32, 32]               0\n",
      "           Conv2d-91           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-92           [-1, 16, 32, 32]              32\n",
      "             ReLU-93           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-94           [-1, 16, 32, 32]               0\n",
      "           Conv2d-95           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-96           [-1, 16, 32, 32]              32\n",
      "             ReLU-97           [-1, 16, 32, 32]               0\n",
      "           Conv2d-98           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-99           [-1, 16, 32, 32]              32\n",
      "            ReLU-100           [-1, 16, 32, 32]               0\n",
      "      BasicBlock-101           [-1, 16, 32, 32]               0\n",
      "          Conv2d-102           [-1, 16, 32, 32]           2,320\n",
      "     BatchNorm2d-103           [-1, 16, 32, 32]              32\n",
      "            ReLU-104           [-1, 16, 32, 32]               0\n",
      "          Conv2d-105           [-1, 16, 32, 32]           2,320\n",
      "     BatchNorm2d-106           [-1, 16, 32, 32]              32\n",
      "            ReLU-107           [-1, 16, 32, 32]               0\n",
      "      BasicBlock-108           [-1, 16, 32, 32]               0\n",
      "          Conv2d-109           [-1, 16, 32, 32]           2,320\n",
      "     BatchNorm2d-110           [-1, 16, 32, 32]              32\n",
      "            ReLU-111           [-1, 16, 32, 32]               0\n",
      "          Conv2d-112           [-1, 16, 32, 32]           2,320\n",
      "     BatchNorm2d-113           [-1, 16, 32, 32]              32\n",
      "            ReLU-114           [-1, 16, 32, 32]               0\n",
      "      BasicBlock-115           [-1, 16, 32, 32]               0\n",
      "          Conv2d-116           [-1, 32, 16, 16]           4,640\n",
      "     BatchNorm2d-117           [-1, 32, 16, 16]              64\n",
      "            ReLU-118           [-1, 32, 16, 16]               0\n",
      "          Conv2d-119           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-120           [-1, 32, 16, 16]              64\n",
      "          Conv2d-121           [-1, 32, 16, 16]             544\n",
      "            ReLU-122           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-123           [-1, 32, 16, 16]               0\n",
      "          Conv2d-124           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-125           [-1, 32, 16, 16]              64\n",
      "            ReLU-126           [-1, 32, 16, 16]               0\n",
      "          Conv2d-127           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-128           [-1, 32, 16, 16]              64\n",
      "            ReLU-129           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-130           [-1, 32, 16, 16]               0\n",
      "          Conv2d-131           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-132           [-1, 32, 16, 16]              64\n",
      "            ReLU-133           [-1, 32, 16, 16]               0\n",
      "          Conv2d-134           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-135           [-1, 32, 16, 16]              64\n",
      "            ReLU-136           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-137           [-1, 32, 16, 16]               0\n",
      "          Conv2d-138           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-139           [-1, 32, 16, 16]              64\n",
      "            ReLU-140           [-1, 32, 16, 16]               0\n",
      "          Conv2d-141           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-142           [-1, 32, 16, 16]              64\n",
      "            ReLU-143           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-144           [-1, 32, 16, 16]               0\n",
      "          Conv2d-145           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-146           [-1, 32, 16, 16]              64\n",
      "            ReLU-147           [-1, 32, 16, 16]               0\n",
      "          Conv2d-148           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-149           [-1, 32, 16, 16]              64\n",
      "            ReLU-150           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-151           [-1, 32, 16, 16]               0\n",
      "          Conv2d-152           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-153           [-1, 32, 16, 16]              64\n",
      "            ReLU-154           [-1, 32, 16, 16]               0\n",
      "          Conv2d-155           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-156           [-1, 32, 16, 16]              64\n",
      "            ReLU-157           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-158           [-1, 32, 16, 16]               0\n",
      "          Conv2d-159           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-160           [-1, 32, 16, 16]              64\n",
      "            ReLU-161           [-1, 32, 16, 16]               0\n",
      "          Conv2d-162           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-163           [-1, 32, 16, 16]              64\n",
      "            ReLU-164           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-165           [-1, 32, 16, 16]               0\n",
      "          Conv2d-166           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-167           [-1, 32, 16, 16]              64\n",
      "            ReLU-168           [-1, 32, 16, 16]               0\n",
      "          Conv2d-169           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-170           [-1, 32, 16, 16]              64\n",
      "            ReLU-171           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-172           [-1, 32, 16, 16]               0\n",
      "          Conv2d-173           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-174           [-1, 32, 16, 16]              64\n",
      "            ReLU-175           [-1, 32, 16, 16]               0\n",
      "          Conv2d-176           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-177           [-1, 32, 16, 16]              64\n",
      "            ReLU-178           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-179           [-1, 32, 16, 16]               0\n",
      "          Conv2d-180           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-181           [-1, 32, 16, 16]              64\n",
      "            ReLU-182           [-1, 32, 16, 16]               0\n",
      "          Conv2d-183           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-184           [-1, 32, 16, 16]              64\n",
      "            ReLU-185           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-186           [-1, 32, 16, 16]               0\n",
      "          Conv2d-187           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-188           [-1, 32, 16, 16]              64\n",
      "            ReLU-189           [-1, 32, 16, 16]               0\n",
      "          Conv2d-190           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-191           [-1, 32, 16, 16]              64\n",
      "            ReLU-192           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-193           [-1, 32, 16, 16]               0\n",
      "          Conv2d-194           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-195           [-1, 32, 16, 16]              64\n",
      "            ReLU-196           [-1, 32, 16, 16]               0\n",
      "          Conv2d-197           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-198           [-1, 32, 16, 16]              64\n",
      "            ReLU-199           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-200           [-1, 32, 16, 16]               0\n",
      "          Conv2d-201           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-202           [-1, 32, 16, 16]              64\n",
      "            ReLU-203           [-1, 32, 16, 16]               0\n",
      "          Conv2d-204           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-205           [-1, 32, 16, 16]              64\n",
      "            ReLU-206           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-207           [-1, 32, 16, 16]               0\n",
      "          Conv2d-208           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-209           [-1, 32, 16, 16]              64\n",
      "            ReLU-210           [-1, 32, 16, 16]               0\n",
      "          Conv2d-211           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-212           [-1, 32, 16, 16]              64\n",
      "            ReLU-213           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-214           [-1, 32, 16, 16]               0\n",
      "          Conv2d-215           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-216           [-1, 32, 16, 16]              64\n",
      "            ReLU-217           [-1, 32, 16, 16]               0\n",
      "          Conv2d-218           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-219           [-1, 32, 16, 16]              64\n",
      "            ReLU-220           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-221           [-1, 32, 16, 16]               0\n",
      "          Conv2d-222           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-223           [-1, 32, 16, 16]              64\n",
      "            ReLU-224           [-1, 32, 16, 16]               0\n",
      "          Conv2d-225           [-1, 32, 16, 16]           9,248\n",
      "     BatchNorm2d-226           [-1, 32, 16, 16]              64\n",
      "            ReLU-227           [-1, 32, 16, 16]               0\n",
      "      BasicBlock-228           [-1, 32, 16, 16]               0\n",
      "          Conv2d-229             [-1, 64, 8, 8]          18,496\n",
      "     BatchNorm2d-230             [-1, 64, 8, 8]             128\n",
      "            ReLU-231             [-1, 64, 8, 8]               0\n",
      "          Conv2d-232             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-233             [-1, 64, 8, 8]             128\n",
      "          Conv2d-234             [-1, 64, 8, 8]           2,112\n",
      "            ReLU-235             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-236             [-1, 64, 8, 8]               0\n",
      "          Conv2d-237             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-238             [-1, 64, 8, 8]             128\n",
      "            ReLU-239             [-1, 64, 8, 8]               0\n",
      "          Conv2d-240             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-241             [-1, 64, 8, 8]             128\n",
      "            ReLU-242             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-243             [-1, 64, 8, 8]               0\n",
      "          Conv2d-244             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-245             [-1, 64, 8, 8]             128\n",
      "            ReLU-246             [-1, 64, 8, 8]               0\n",
      "          Conv2d-247             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-248             [-1, 64, 8, 8]             128\n",
      "            ReLU-249             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-250             [-1, 64, 8, 8]               0\n",
      "          Conv2d-251             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-252             [-1, 64, 8, 8]             128\n",
      "            ReLU-253             [-1, 64, 8, 8]               0\n",
      "          Conv2d-254             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-255             [-1, 64, 8, 8]             128\n",
      "            ReLU-256             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-257             [-1, 64, 8, 8]               0\n",
      "          Conv2d-258             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-259             [-1, 64, 8, 8]             128\n",
      "            ReLU-260             [-1, 64, 8, 8]               0\n",
      "          Conv2d-261             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-262             [-1, 64, 8, 8]             128\n",
      "            ReLU-263             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-264             [-1, 64, 8, 8]               0\n",
      "          Conv2d-265             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-266             [-1, 64, 8, 8]             128\n",
      "            ReLU-267             [-1, 64, 8, 8]               0\n",
      "          Conv2d-268             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-269             [-1, 64, 8, 8]             128\n",
      "            ReLU-270             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-271             [-1, 64, 8, 8]               0\n",
      "          Conv2d-272             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-273             [-1, 64, 8, 8]             128\n",
      "            ReLU-274             [-1, 64, 8, 8]               0\n",
      "          Conv2d-275             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-276             [-1, 64, 8, 8]             128\n",
      "            ReLU-277             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-278             [-1, 64, 8, 8]               0\n",
      "          Conv2d-279             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-280             [-1, 64, 8, 8]             128\n",
      "            ReLU-281             [-1, 64, 8, 8]               0\n",
      "          Conv2d-282             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-283             [-1, 64, 8, 8]             128\n",
      "            ReLU-284             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-285             [-1, 64, 8, 8]               0\n",
      "          Conv2d-286             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-287             [-1, 64, 8, 8]             128\n",
      "            ReLU-288             [-1, 64, 8, 8]               0\n",
      "          Conv2d-289             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-290             [-1, 64, 8, 8]             128\n",
      "            ReLU-291             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-292             [-1, 64, 8, 8]               0\n",
      "          Conv2d-293             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-294             [-1, 64, 8, 8]             128\n",
      "            ReLU-295             [-1, 64, 8, 8]               0\n",
      "          Conv2d-296             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-297             [-1, 64, 8, 8]             128\n",
      "            ReLU-298             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-299             [-1, 64, 8, 8]               0\n",
      "          Conv2d-300             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-301             [-1, 64, 8, 8]             128\n",
      "            ReLU-302             [-1, 64, 8, 8]               0\n",
      "          Conv2d-303             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-304             [-1, 64, 8, 8]             128\n",
      "            ReLU-305             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-306             [-1, 64, 8, 8]               0\n",
      "          Conv2d-307             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-308             [-1, 64, 8, 8]             128\n",
      "            ReLU-309             [-1, 64, 8, 8]               0\n",
      "          Conv2d-310             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-311             [-1, 64, 8, 8]             128\n",
      "            ReLU-312             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-313             [-1, 64, 8, 8]               0\n",
      "          Conv2d-314             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-315             [-1, 64, 8, 8]             128\n",
      "            ReLU-316             [-1, 64, 8, 8]               0\n",
      "          Conv2d-317             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-318             [-1, 64, 8, 8]             128\n",
      "            ReLU-319             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-320             [-1, 64, 8, 8]               0\n",
      "          Conv2d-321             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-322             [-1, 64, 8, 8]             128\n",
      "            ReLU-323             [-1, 64, 8, 8]               0\n",
      "          Conv2d-324             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-325             [-1, 64, 8, 8]             128\n",
      "            ReLU-326             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-327             [-1, 64, 8, 8]               0\n",
      "          Conv2d-328             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-329             [-1, 64, 8, 8]             128\n",
      "            ReLU-330             [-1, 64, 8, 8]               0\n",
      "          Conv2d-331             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-332             [-1, 64, 8, 8]             128\n",
      "            ReLU-333             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-334             [-1, 64, 8, 8]               0\n",
      "          Conv2d-335             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-336             [-1, 64, 8, 8]             128\n",
      "            ReLU-337             [-1, 64, 8, 8]               0\n",
      "          Conv2d-338             [-1, 64, 8, 8]          36,928\n",
      "     BatchNorm2d-339             [-1, 64, 8, 8]             128\n",
      "            ReLU-340             [-1, 64, 8, 8]               0\n",
      "      BasicBlock-341             [-1, 64, 8, 8]               0\n",
      "AdaptiveAvgPool2d-342             [-1, 64, 1, 1]               0\n",
      "          Linear-343                  [-1, 100]           6,500\n",
      "================================================================\n",
      "Total params: 1,545,636\n",
      "Trainable params: 1,545,636\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 24.97\n",
      "Params size (MB): 5.90\n",
      "Estimated Total Size (MB): 30.88\n",
      "----------------------------------------------------------------\n",
      "Total Number of Parameters: 1545636\n"
     ]
    }
   ],
   "source": [
    "#### Train Configurations, based on DSNet and ResNet paper\n",
    "model_n = 16\n",
    "epochs = 100\n",
    "milestones = [int(epochs*0.5), int(epochs*0.75)]\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "gamma = 0.1\n",
    "lr = 0.1\n",
    "\n",
    "model = ResNet(model_n, num_classes=100)\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "summary(model, (3, 32, 32))\n",
    "print('Total Number of Parameters:', sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9689a4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:  model:ResNet(small)  model_n: 16  batch size: 32  optimizer:SGD  lr: 0.1  epochs: 100\n",
      "----------------------------- Train --------------------------------\n",
      "Epoch 1/100\n",
      "------------------------------\n",
      "{'time': 100.46282, 'train_loss': 4.56738, 'train_acc': 0.01744, 'val_loss': 4.43006, 'val_acc': 0.027}\n",
      "Epoch 2/100\n",
      "------------------------------\n",
      "{'time': 95.46393, 'train_loss': 4.17949, 'train_acc': 0.05478, 'val_loss': 3.99101, 'val_acc': 0.0846}\n",
      "Epoch 3/100\n",
      "------------------------------\n",
      "{'time': 96.35463, 'train_loss': 3.8713, 'train_acc': 0.09602, 'val_loss': 3.65268, 'val_acc': 0.137}\n",
      "Epoch 4/100\n",
      "------------------------------\n",
      "{'time': 103.21976, 'train_loss': 3.55425, 'train_acc': 0.14862, 'val_loss': 3.43548, 'val_acc': 0.168}\n",
      "Epoch 5/100\n",
      "------------------------------\n",
      "{'time': 99.14401, 'train_loss': 3.29222, 'train_acc': 0.19906, 'val_loss': 3.10503, 'val_acc': 0.2394}\n",
      "Epoch 6/100\n",
      "------------------------------\n",
      "{'time': 100.02316, 'train_loss': 3.07656, 'train_acc': 0.2399, 'val_loss': 3.0211, 'val_acc': 0.2534}\n",
      "Epoch 7/100\n",
      "------------------------------\n",
      "{'time': 102.23334, 'train_loss': 2.89196, 'train_acc': 0.27402, 'val_loss': 2.76682, 'val_acc': 0.2982}\n",
      "Epoch 8/100\n",
      "------------------------------\n",
      "{'time': 97.98586, 'train_loss': 2.73566, 'train_acc': 0.30638, 'val_loss': 2.69781, 'val_acc': 0.3166}\n",
      "Epoch 9/100\n",
      "------------------------------\n",
      "{'time': 97.43088, 'train_loss': 2.60313, 'train_acc': 0.33316, 'val_loss': 2.59815, 'val_acc': 0.3344}\n",
      "Epoch 10/100\n",
      "------------------------------\n",
      "{'time': 101.74924, 'train_loss': 2.48323, 'train_acc': 0.35838, 'val_loss': 2.48113, 'val_acc': 0.3542}\n",
      "Epoch 11/100\n",
      "------------------------------\n",
      "{'time': 98.87322, 'train_loss': 2.37466, 'train_acc': 0.3803, 'val_loss': 2.37875, 'val_acc': 0.3864}\n",
      "Epoch 12/100\n",
      "------------------------------\n",
      "{'time': 102.21007, 'train_loss': 2.27865, 'train_acc': 0.4018, 'val_loss': 2.30158, 'val_acc': 0.4016}\n",
      "Epoch 13/100\n",
      "------------------------------\n",
      "{'time': 103.53634, 'train_loss': 2.189, 'train_acc': 0.42056, 'val_loss': 2.25713, 'val_acc': 0.4158}\n",
      "Epoch 14/100\n",
      "------------------------------\n",
      "{'time': 96.75228, 'train_loss': 2.11042, 'train_acc': 0.43494, 'val_loss': 2.29713, 'val_acc': 0.4116}\n",
      "Epoch 15/100\n",
      "------------------------------\n",
      "{'time': 96.41215, 'train_loss': 2.03648, 'train_acc': 0.45554, 'val_loss': 2.17364, 'val_acc': 0.4358}\n",
      "Epoch 16/100\n",
      "------------------------------\n",
      "{'time': 97.56435, 'train_loss': 1.96687, 'train_acc': 0.46922, 'val_loss': 2.10052, 'val_acc': 0.4426}\n",
      "Epoch 17/100\n",
      "------------------------------\n",
      "{'time': 104.12784, 'train_loss': 1.90127, 'train_acc': 0.48798, 'val_loss': 2.20301, 'val_acc': 0.4342}\n",
      "Epoch 18/100\n",
      "------------------------------\n",
      "{'time': 100.41213, 'train_loss': 1.84074, 'train_acc': 0.50024, 'val_loss': 1.96652, 'val_acc': 0.4762}\n",
      "Epoch 19/100\n",
      "------------------------------\n",
      "{'time': 98.54342, 'train_loss': 1.7782, 'train_acc': 0.51306, 'val_loss': 1.94347, 'val_acc': 0.483}\n",
      "Epoch 20/100\n",
      "------------------------------\n",
      "{'time': 102.09255, 'train_loss': 1.7259, 'train_acc': 0.52696, 'val_loss': 1.96142, 'val_acc': 0.4854}\n",
      "Epoch 21/100\n",
      "------------------------------\n",
      "{'time': 96.03773, 'train_loss': 1.67912, 'train_acc': 0.53712, 'val_loss': 1.9446, 'val_acc': 0.4848}\n",
      "Epoch 22/100\n",
      "------------------------------\n",
      "{'time': 96.7803, 'train_loss': 1.62856, 'train_acc': 0.54844, 'val_loss': 1.87208, 'val_acc': 0.5072}\n",
      "Epoch 23/100\n",
      "------------------------------\n",
      "{'time': 101.9864, 'train_loss': 1.58536, 'train_acc': 0.5588, 'val_loss': 1.85468, 'val_acc': 0.5064}\n",
      "Epoch 24/100\n",
      "------------------------------\n",
      "{'time': 100.7855, 'train_loss': 1.54857, 'train_acc': 0.56712, 'val_loss': 1.88533, 'val_acc': 0.5156}\n",
      "Epoch 25/100\n",
      "------------------------------\n",
      "{'time': 100.7137, 'train_loss': 1.50742, 'train_acc': 0.57692, 'val_loss': 1.88702, 'val_acc': 0.506}\n",
      "Epoch 26/100\n",
      "------------------------------\n",
      "{'time': 102.57883, 'train_loss': 1.47167, 'train_acc': 0.5841, 'val_loss': 1.8379, 'val_acc': 0.5148}\n",
      "Epoch 27/100\n",
      "------------------------------\n",
      "{'time': 96.66469, 'train_loss': 1.4372, 'train_acc': 0.59274, 'val_loss': 1.84827, 'val_acc': 0.5138}\n",
      "Epoch 28/100\n",
      "------------------------------\n",
      "{'time': 97.2738, 'train_loss': 1.39736, 'train_acc': 0.603, 'val_loss': 1.83611, 'val_acc': 0.5182}\n",
      "Epoch 29/100\n",
      "------------------------------\n",
      "{'time': 99.99799, 'train_loss': 1.37018, 'train_acc': 0.6092, 'val_loss': 1.80328, 'val_acc': 0.5326}\n",
      "Epoch 30/100\n",
      "------------------------------\n",
      "{'time': 100.55058, 'train_loss': 1.33269, 'train_acc': 0.61798, 'val_loss': 1.78299, 'val_acc': 0.5278}\n",
      "Epoch 31/100\n",
      "------------------------------\n",
      "{'time': 99.70928, 'train_loss': 1.31217, 'train_acc': 0.62228, 'val_loss': 1.71547, 'val_acc': 0.5484}\n",
      "Epoch 32/100\n",
      "------------------------------\n",
      "{'time': 100.34993, 'train_loss': 1.28278, 'train_acc': 0.63024, 'val_loss': 1.80459, 'val_acc': 0.5294}\n",
      "Epoch 33/100\n",
      "------------------------------\n",
      "{'time': 97.92053, 'train_loss': 1.25287, 'train_acc': 0.63798, 'val_loss': 1.84301, 'val_acc': 0.5314}\n",
      "Epoch 34/100\n",
      "------------------------------\n",
      "{'time': 96.57204, 'train_loss': 1.23947, 'train_acc': 0.64106, 'val_loss': 2.00928, 'val_acc': 0.5028}\n",
      "Epoch 35/100\n",
      "------------------------------\n",
      "{'time': 97.96837, 'train_loss': 1.20452, 'train_acc': 0.65014, 'val_loss': 1.85986, 'val_acc': 0.535}\n",
      "Epoch 36/100\n",
      "------------------------------\n",
      "{'time': 104.94941, 'train_loss': 1.17905, 'train_acc': 0.65702, 'val_loss': 1.73781, 'val_acc': 0.5448}\n",
      "Epoch 37/100\n",
      "------------------------------\n",
      "{'time': 100.32241, 'train_loss': 1.15823, 'train_acc': 0.66068, 'val_loss': 1.73132, 'val_acc': 0.546}\n",
      "Epoch 38/100\n",
      "------------------------------\n",
      "{'time': 99.83325, 'train_loss': 1.13701, 'train_acc': 0.66738, 'val_loss': 1.73303, 'val_acc': 0.546}\n",
      "Epoch 39/100\n",
      "------------------------------\n",
      "{'time': 102.59931, 'train_loss': 1.11168, 'train_acc': 0.67182, 'val_loss': 1.74381, 'val_acc': 0.5522}\n",
      "Epoch 40/100\n",
      "------------------------------\n",
      "{'time': 96.16281, 'train_loss': 1.09257, 'train_acc': 0.67588, 'val_loss': 1.84408, 'val_acc': 0.5388}\n",
      "Epoch 41/100\n",
      "------------------------------\n",
      "{'time': 98.14473, 'train_loss': 1.07842, 'train_acc': 0.68354, 'val_loss': 1.77986, 'val_acc': 0.5498}\n",
      "Epoch 42/100\n",
      "------------------------------\n",
      "{'time': 102.94736, 'train_loss': 1.05012, 'train_acc': 0.69012, 'val_loss': 1.76267, 'val_acc': 0.5528}\n",
      "Epoch 43/100\n",
      "------------------------------\n",
      "{'time': 100.43096, 'train_loss': 1.03192, 'train_acc': 0.69394, 'val_loss': 1.77277, 'val_acc': 0.5542}\n",
      "Epoch 44/100\n",
      "------------------------------\n",
      "{'time': 100.36275, 'train_loss': 1.00946, 'train_acc': 0.70028, 'val_loss': 1.82829, 'val_acc': 0.5492}\n",
      "Epoch 45/100\n",
      "------------------------------\n",
      "{'time': 102.48851, 'train_loss': 0.98898, 'train_acc': 0.70338, 'val_loss': 1.78829, 'val_acc': 0.5542}\n",
      "Epoch 46/100\n",
      "------------------------------\n",
      "{'time': 96.76257, 'train_loss': 0.98524, 'train_acc': 0.7055, 'val_loss': 1.81819, 'val_acc': 0.5508}\n",
      "Epoch 47/100\n",
      "------------------------------\n",
      "{'time': 95.49164, 'train_loss': 0.96403, 'train_acc': 0.71112, 'val_loss': 1.78036, 'val_acc': 0.5594}\n",
      "Epoch 48/100\n",
      "------------------------------\n",
      "{'time': 99.72417, 'train_loss': 0.94437, 'train_acc': 0.71632, 'val_loss': 1.82836, 'val_acc': 0.5546}\n",
      "Epoch 49/100\n",
      "------------------------------\n",
      "{'time': 99.96635, 'train_loss': 0.93142, 'train_acc': 0.71908, 'val_loss': 1.74815, 'val_acc': 0.5596}\n",
      "Epoch 50/100\n",
      "------------------------------\n",
      "{'time': 102.66558, 'train_loss': 0.91489, 'train_acc': 0.7236, 'val_loss': 1.77062, 'val_acc': 0.5636}\n",
      "Epoch 51/100\n",
      "------------------------------\n",
      "{'time': 104.31491, 'train_loss': 0.66421, 'train_acc': 0.79828, 'val_loss': 1.61065, 'val_acc': 0.6068}\n",
      "Epoch 52/100\n",
      "------------------------------\n",
      "{'time': 93.76348, 'train_loss': 0.58983, 'train_acc': 0.81864, 'val_loss': 1.62689, 'val_acc': 0.6044}\n",
      "Epoch 53/100\n",
      "------------------------------\n",
      "{'time': 96.90152, 'train_loss': 0.56049, 'train_acc': 0.82804, 'val_loss': 1.62135, 'val_acc': 0.6084}\n",
      "Epoch 54/100\n",
      "------------------------------\n",
      "{'time': 97.12307, 'train_loss': 0.54239, 'train_acc': 0.83166, 'val_loss': 1.64685, 'val_acc': 0.61}\n",
      "Epoch 55/100\n",
      "------------------------------\n",
      "{'time': 102.82075, 'train_loss': 0.52723, 'train_acc': 0.8352, 'val_loss': 1.65406, 'val_acc': 0.6122}\n",
      "Epoch 56/100\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 100.66835, 'train_loss': 0.50847, 'train_acc': 0.84142, 'val_loss': 1.66694, 'val_acc': 0.6094}\n",
      "Epoch 57/100\n",
      "------------------------------\n",
      "{'time': 98.94228, 'train_loss': 0.50152, 'train_acc': 0.8436, 'val_loss': 1.65755, 'val_acc': 0.6026}\n",
      "Epoch 58/100\n",
      "------------------------------\n",
      "{'time': 102.47038, 'train_loss': 0.48986, 'train_acc': 0.85046, 'val_loss': 1.67844, 'val_acc': 0.61}\n",
      "Epoch 59/100\n",
      "------------------------------\n",
      "{'time': 95.39317, 'train_loss': 0.48269, 'train_acc': 0.8496, 'val_loss': 1.73498, 'val_acc': 0.6062}\n",
      "Epoch 60/100\n",
      "------------------------------\n",
      "{'time': 95.92352, 'train_loss': 0.47406, 'train_acc': 0.85156, 'val_loss': 1.6815, 'val_acc': 0.6098}\n",
      "Epoch 61/100\n",
      "------------------------------\n",
      "{'time': 100.45637, 'train_loss': 0.46628, 'train_acc': 0.85378, 'val_loss': 1.69536, 'val_acc': 0.6142}\n",
      "Epoch 62/100\n",
      "------------------------------\n",
      "{'time': 100.73071, 'train_loss': 0.45747, 'train_acc': 0.85442, 'val_loss': 1.71739, 'val_acc': 0.6148}\n",
      "Epoch 63/100\n",
      "------------------------------\n",
      "{'time': 100.14052, 'train_loss': 0.45303, 'train_acc': 0.85824, 'val_loss': 1.69654, 'val_acc': 0.6136}\n",
      "Epoch 64/100\n",
      "------------------------------\n",
      "{'time': 102.91684, 'train_loss': 0.44593, 'train_acc': 0.85934, 'val_loss': 1.74645, 'val_acc': 0.6068}\n",
      "Epoch 65/100\n",
      "------------------------------\n",
      "{'time': 98.59075, 'train_loss': 0.43545, 'train_acc': 0.86194, 'val_loss': 1.73563, 'val_acc': 0.6088}\n",
      "Epoch 66/100\n",
      "------------------------------\n",
      "{'time': 97.29506, 'train_loss': 0.43156, 'train_acc': 0.86568, 'val_loss': 1.7489, 'val_acc': 0.6176}\n",
      "Epoch 67/100\n",
      "------------------------------\n",
      "{'time': 99.55574, 'train_loss': 0.41998, 'train_acc': 0.86728, 'val_loss': 1.74606, 'val_acc': 0.6124}\n",
      "Epoch 68/100\n",
      "------------------------------\n",
      "{'time': 99.63362, 'train_loss': 0.41765, 'train_acc': 0.86708, 'val_loss': 1.75703, 'val_acc': 0.6136}\n",
      "Epoch 69/100\n",
      "------------------------------\n",
      "{'time': 99.67374, 'train_loss': 0.40843, 'train_acc': 0.87122, 'val_loss': 1.79981, 'val_acc': 0.603}\n",
      "Epoch 70/100\n",
      "------------------------------\n",
      "{'time': 99.82974, 'train_loss': 0.40507, 'train_acc': 0.87238, 'val_loss': 1.76263, 'val_acc': 0.6082}\n",
      "Epoch 71/100\n",
      "------------------------------\n",
      "{'time': 101.65789, 'train_loss': 0.40262, 'train_acc': 0.87136, 'val_loss': 1.81576, 'val_acc': 0.605}\n",
      "Epoch 72/100\n",
      "------------------------------\n",
      "{'time': 95.81243, 'train_loss': 0.40216, 'train_acc': 0.87196, 'val_loss': 1.79688, 'val_acc': 0.6068}\n",
      "Epoch 73/100\n",
      "------------------------------\n",
      "{'time': 96.20301, 'train_loss': 0.39587, 'train_acc': 0.87536, 'val_loss': 1.80258, 'val_acc': 0.6082}\n",
      "Epoch 74/100\n",
      "------------------------------\n",
      "{'time': 101.95351, 'train_loss': 0.38671, 'train_acc': 0.87758, 'val_loss': 1.7925, 'val_acc': 0.6134}\n",
      "Epoch 75/100\n",
      "------------------------------\n",
      "{'time': 102.06334, 'train_loss': 0.381, 'train_acc': 0.87796, 'val_loss': 1.82248, 'val_acc': 0.613}\n",
      "Epoch 76/100\n",
      "------------------------------\n",
      "{'time': 100.35815, 'train_loss': 0.35523, 'train_acc': 0.88674, 'val_loss': 1.83499, 'val_acc': 0.606}\n",
      "Epoch 77/100\n",
      "------------------------------\n",
      "{'time': 102.99423, 'train_loss': 0.35236, 'train_acc': 0.88584, 'val_loss': 1.82151, 'val_acc': 0.6186}\n",
      "Epoch 78/100\n",
      "------------------------------\n",
      "{'time': 96.1271, 'train_loss': 0.35034, 'train_acc': 0.88896, 'val_loss': 1.85389, 'val_acc': 0.61}\n",
      "Epoch 79/100\n",
      "------------------------------\n",
      "{'time': 96.3865, 'train_loss': 0.34969, 'train_acc': 0.8888, 'val_loss': 1.80258, 'val_acc': 0.6172}\n",
      "Epoch 80/100\n",
      "------------------------------\n",
      "{'time': 101.75028, 'train_loss': 0.35028, 'train_acc': 0.88818, 'val_loss': 1.81735, 'val_acc': 0.617}\n",
      "Epoch 81/100\n",
      "------------------------------\n",
      "{'time': 99.97683, 'train_loss': 0.35013, 'train_acc': 0.88812, 'val_loss': 1.79898, 'val_acc': 0.6164}\n",
      "Epoch 82/100\n",
      "------------------------------\n",
      "{'time': 99.38595, 'train_loss': 0.34698, 'train_acc': 0.8904, 'val_loss': 1.86116, 'val_acc': 0.6104}\n",
      "Epoch 83/100\n",
      "------------------------------\n",
      "{'time': 102.75906, 'train_loss': 0.34145, 'train_acc': 0.89098, 'val_loss': 1.82036, 'val_acc': 0.611}\n",
      "Epoch 84/100\n",
      "------------------------------\n",
      "{'time': 96.21491, 'train_loss': 0.34279, 'train_acc': 0.8903, 'val_loss': 1.82995, 'val_acc': 0.6128}\n",
      "Epoch 85/100\n",
      "------------------------------\n",
      "{'time': 97.65884, 'train_loss': 0.33842, 'train_acc': 0.89258, 'val_loss': 1.82602, 'val_acc': 0.6184}\n",
      "Epoch 86/100\n",
      "------------------------------\n",
      "{'time': 97.83125, 'train_loss': 0.34288, 'train_acc': 0.89072, 'val_loss': 1.82845, 'val_acc': 0.6064}\n",
      "Epoch 87/100\n",
      "------------------------------\n",
      "{'time': 105.14726, 'train_loss': 0.3396, 'train_acc': 0.89236, 'val_loss': 1.84963, 'val_acc': 0.6144}\n",
      "Epoch 88/100\n",
      "------------------------------\n",
      "{'time': 100.43613, 'train_loss': 0.34159, 'train_acc': 0.88984, 'val_loss': 1.83785, 'val_acc': 0.605}\n",
      "Epoch 89/100\n",
      "------------------------------\n",
      "{'time': 99.70475, 'train_loss': 0.3375, 'train_acc': 0.89194, 'val_loss': 1.78561, 'val_acc': 0.6152}\n",
      "Epoch 90/100\n",
      "------------------------------\n",
      "{'time': 101.0342, 'train_loss': 0.33485, 'train_acc': 0.89266, 'val_loss': 1.84706, 'val_acc': 0.614}\n",
      "Epoch 91/100\n",
      "------------------------------\n",
      "{'time': 96.87844, 'train_loss': 0.3382, 'train_acc': 0.89226, 'val_loss': 1.84755, 'val_acc': 0.6096}\n",
      "Epoch 92/100\n",
      "------------------------------\n",
      "{'time': 97.50724, 'train_loss': 0.33687, 'train_acc': 0.89268, 'val_loss': 1.85147, 'val_acc': 0.6096}\n",
      "Epoch 93/100\n",
      "------------------------------\n",
      "{'time': 103.31445, 'train_loss': 0.33149, 'train_acc': 0.8941, 'val_loss': 1.82895, 'val_acc': 0.6084}\n",
      "Epoch 94/100\n",
      "------------------------------\n",
      "{'time': 99.79144, 'train_loss': 0.33363, 'train_acc': 0.89304, 'val_loss': 1.86546, 'val_acc': 0.606}\n",
      "Epoch 95/100\n",
      "------------------------------\n",
      "{'time': 100.7784, 'train_loss': 0.3334, 'train_acc': 0.8943, 'val_loss': 1.83276, 'val_acc': 0.6134}\n",
      "Epoch 96/100\n",
      "------------------------------\n",
      "{'time': 103.44121, 'train_loss': 0.33127, 'train_acc': 0.89598, 'val_loss': 1.80393, 'val_acc': 0.6164}\n",
      "Epoch 97/100\n",
      "------------------------------\n",
      "{'time': 95.70981, 'train_loss': 0.33154, 'train_acc': 0.89452, 'val_loss': 1.81963, 'val_acc': 0.6152}\n",
      "Epoch 98/100\n",
      "------------------------------\n",
      "{'time': 95.0155, 'train_loss': 0.33324, 'train_acc': 0.89478, 'val_loss': 1.82817, 'val_acc': 0.6096}\n",
      "Epoch 99/100\n",
      "------------------------------\n",
      "{'time': 101.78289, 'train_loss': 0.33333, 'train_acc': 0.89432, 'val_loss': 1.85381, 'val_acc': 0.6076}\n",
      "Epoch 100/100\n",
      "------------------------------\n",
      "{'time': 102.24406, 'train_loss': 0.33085, 'train_acc': 0.89456, 'val_loss': 1.83571, 'val_acc': 0.6058}\n",
      "----------------------------- Test --------------------------------\n",
      "{'time': 105.37301, 'test_loss': 1.84985, 'test_acc': 0.6146}\n"
     ]
    }
   ],
   "source": [
    "### Train loop + validation/ also test at the end\n",
    "print(\"Configuration: \", \"model:ResNet(small)\", \" model_n:\", model_n, \" batch size:\", batch_size, \n",
    "      \" optimizer:SGD\", \" lr:\", lr, \" epochs:\", epochs)\n",
    "\n",
    "all_epoch_loss = {\"train\": [], \"validation\": []}\n",
    "all_epoch_acc = {\"train\":  [], \"validation\": []}\n",
    "\n",
    "print(\"----------------------------- Train --------------------------------\")\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    \n",
    "    epoch_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    epoch_acc = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    \n",
    "    running_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    running_corrects = {\"train\": 0, \"validation\": 0}\n",
    "    \n",
    "    for phase in [\"train\", \"validation\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train(True)\n",
    "        else:\n",
    "            model.train(False)\n",
    "        \n",
    "        for data in data_loaders[phase]:\n",
    "            inputs, labels = data \n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # clear all gradients\n",
    "            \n",
    "            outputs = model(inputs) # batch_size x num_classes\n",
    "            _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                loss.backward()  # compute gradients\n",
    "                optimizer.step() # update weights/biases\n",
    "               \n",
    "            running_loss[phase] += loss.data.item() * inputs.size(0)\n",
    "            running_corrects[phase] += torch.sum(preds == labels.data).item()\n",
    "        \n",
    "        all_epoch_loss[phase].append(running_loss[phase] / dataset_sizes[phase])\n",
    "        all_epoch_acc[phase].append(running_corrects[phase] / dataset_sizes[phase])\n",
    "        \n",
    "        epoch_loss[phase] = running_loss[phase] / dataset_sizes[phase]\n",
    "        epoch_acc[phase] =  running_corrects[phase] / dataset_sizes[phase]\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "        'time': np.round(time.time()-start_time, 5),\n",
    "        'train_loss': np.round(epoch_loss[\"train\"], 5),\n",
    "        'train_acc': np.round(epoch_acc[\"train\"], 5),\n",
    "        'val_loss': np.round(epoch_loss[\"validation\"], 5),\n",
    "        'val_acc': np.round(epoch_acc[\"validation\"], 5),\n",
    "    })\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    \n",
    "with open('ResNet_16_bs_32_opt_SGD_lr_1_epochs_100_loss.txt', 'w') as f_loss:\n",
    "    print(all_epoch_loss, file=f_loss)\n",
    "    \n",
    "with open('ResNet_16_bs_32_opt_SGD_lr_1_epochs_100_acc.txt', 'w') as f_acc:\n",
    "    print(all_epoch_acc, file=f_acc)\n",
    "    \n",
    "### evaluating the model with test set\n",
    "print(\"----------------------------- Test --------------------------------\")\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # clear all gradients\n",
    "\n",
    "        outputs = model(inputs) # batch_size x num_classes\n",
    "        _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        running_loss += loss.data.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "    'time': np.round(time.time()-start_time, 5),\n",
    "    'test_loss': np.round(running_loss/ dataset_sizes['test'], 5),\n",
    "    'test_acc': np.round(running_corrects/ dataset_sizes['test'], 5),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adc86c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = open('ResNet_16_bs_32_opt_SGD_lr_1_epochs_100_loss.txt', 'r').read()\n",
    "ast.literal_eval(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a39bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:  model:ResNet(small)  model_n: 3  batch size: 32  optimizer:SGD  lr: 0.1  epochs: 100\n",
      "----------------------------- Train --------------------------------\n",
      "Epoch 1/100\n",
      "------------------------------\n",
      "{'time': 34.01057, 'train_loss': 4.09702, 'train_acc': 0.06634, 'val_loss': 3.74329, 'val_acc': 0.12}\n",
      "Epoch 2/100\n",
      "------------------------------\n",
      "{'time': 30.53767, 'train_loss': 3.57187, 'train_acc': 0.14674, 'val_loss': 3.32252, 'val_acc': 0.1924}\n",
      "Epoch 3/100\n",
      "------------------------------\n",
      "{'time': 29.13071, 'train_loss': 3.21325, 'train_acc': 0.213, 'val_loss': 3.12238, 'val_acc': 0.2362}\n",
      "Epoch 4/100\n",
      "------------------------------\n",
      "{'time': 29.05568, 'train_loss': 2.93294, 'train_acc': 0.26122, 'val_loss': 2.89824, 'val_acc': 0.2798}\n",
      "Epoch 5/100\n",
      "------------------------------\n",
      "{'time': 31.18892, 'train_loss': 2.69874, 'train_acc': 0.30884, 'val_loss': 2.61728, 'val_acc': 0.3282}\n",
      "Epoch 6/100\n",
      "------------------------------\n",
      "{'time': 29.74719, 'train_loss': 2.50385, 'train_acc': 0.35274, 'val_loss': 2.53288, 'val_acc': 0.3508}\n",
      "Epoch 7/100\n",
      "------------------------------\n",
      "{'time': 30.17769, 'train_loss': 2.35162, 'train_acc': 0.3796, 'val_loss': 2.32369, 'val_acc': 0.3928}\n",
      "Epoch 8/100\n",
      "------------------------------\n",
      "{'time': 29.78436, 'train_loss': 2.23685, 'train_acc': 0.40766, 'val_loss': 2.20447, 'val_acc': 0.4216}\n",
      "Epoch 9/100\n",
      "------------------------------\n",
      "{'time': 29.05318, 'train_loss': 2.13465, 'train_acc': 0.42892, 'val_loss': 2.13573, 'val_acc': 0.434}\n",
      "Epoch 10/100\n",
      "------------------------------\n",
      "{'time': 30.14749, 'train_loss': 2.05512, 'train_acc': 0.44464, 'val_loss': 2.07091, 'val_acc': 0.4404}\n",
      "Epoch 11/100\n",
      "------------------------------\n",
      "{'time': 30.82301, 'train_loss': 1.9875, 'train_acc': 0.46244, 'val_loss': 2.01969, 'val_acc': 0.4626}\n",
      "Epoch 12/100\n",
      "------------------------------\n",
      "{'time': 33.6248, 'train_loss': 1.92504, 'train_acc': 0.47752, 'val_loss': 2.01897, 'val_acc': 0.4666}\n",
      "Epoch 13/100\n",
      "------------------------------\n",
      "{'time': 28.6141, 'train_loss': 1.87347, 'train_acc': 0.48898, 'val_loss': 1.91896, 'val_acc': 0.4872}\n",
      "Epoch 14/100\n",
      "------------------------------\n",
      "{'time': 28.60321, 'train_loss': 1.82897, 'train_acc': 0.49922, 'val_loss': 1.95981, 'val_acc': 0.4908}\n",
      "Epoch 15/100\n",
      "------------------------------\n",
      "{'time': 28.85338, 'train_loss': 1.78706, 'train_acc': 0.51194, 'val_loss': 1.87534, 'val_acc': 0.4982}\n",
      "Epoch 16/100\n",
      "------------------------------\n",
      "{'time': 30.75382, 'train_loss': 1.75438, 'train_acc': 0.51566, 'val_loss': 1.84139, 'val_acc': 0.4992}\n",
      "Epoch 17/100\n",
      "------------------------------\n",
      "{'time': 29.14234, 'train_loss': 1.71148, 'train_acc': 0.52798, 'val_loss': 1.87966, 'val_acc': 0.502}\n",
      "Epoch 18/100\n",
      "------------------------------\n",
      "{'time': 28.76154, 'train_loss': 1.67579, 'train_acc': 0.53562, 'val_loss': 1.80789, 'val_acc': 0.5132}\n",
      "Epoch 19/100\n",
      "------------------------------\n",
      "{'time': 28.75456, 'train_loss': 1.6472, 'train_acc': 0.54118, 'val_loss': 1.87933, 'val_acc': 0.503}\n",
      "Epoch 20/100\n",
      "------------------------------\n",
      "{'time': 29.94462, 'train_loss': 1.62163, 'train_acc': 0.54884, 'val_loss': 1.97599, 'val_acc': 0.4856}\n",
      "Epoch 21/100\n",
      "------------------------------\n",
      "{'time': 27.95447, 'train_loss': 1.59516, 'train_acc': 0.55382, 'val_loss': 1.78741, 'val_acc': 0.5108}\n",
      "Epoch 22/100\n",
      "------------------------------\n",
      "{'time': 36.59772, 'train_loss': 1.57011, 'train_acc': 0.55966, 'val_loss': 1.78265, 'val_acc': 0.518}\n",
      "Epoch 23/100\n",
      "------------------------------\n",
      "{'time': 28.7635, 'train_loss': 1.54992, 'train_acc': 0.56538, 'val_loss': 1.80666, 'val_acc': 0.517}\n",
      "Epoch 24/100\n",
      "------------------------------\n",
      "{'time': 28.37991, 'train_loss': 1.52859, 'train_acc': 0.56954, 'val_loss': 1.82584, 'val_acc': 0.5174}\n",
      "Epoch 25/100\n",
      "------------------------------\n",
      "{'time': 28.62931, 'train_loss': 1.51152, 'train_acc': 0.57486, 'val_loss': 1.75078, 'val_acc': 0.5254}\n",
      "Epoch 26/100\n",
      "------------------------------\n",
      "{'time': 28.92739, 'train_loss': 1.48614, 'train_acc': 0.58226, 'val_loss': 1.78771, 'val_acc': 0.5138}\n",
      "Epoch 27/100\n",
      "------------------------------\n",
      "{'time': 28.17737, 'train_loss': 1.47314, 'train_acc': 0.58294, 'val_loss': 1.74903, 'val_acc': 0.5276}\n",
      "Epoch 28/100\n",
      "------------------------------\n",
      "{'time': 29.16026, 'train_loss': 1.45114, 'train_acc': 0.58702, 'val_loss': 1.71155, 'val_acc': 0.5352}\n",
      "Epoch 29/100\n",
      "------------------------------\n",
      "{'time': 29.7063, 'train_loss': 1.43977, 'train_acc': 0.58812, 'val_loss': 1.71913, 'val_acc': 0.544}\n",
      "Epoch 30/100\n",
      "------------------------------\n",
      "{'time': 28.88144, 'train_loss': 1.41432, 'train_acc': 0.59862, 'val_loss': 1.74185, 'val_acc': 0.535}\n",
      "Epoch 31/100\n",
      "------------------------------\n",
      "{'time': 29.75099, 'train_loss': 1.40476, 'train_acc': 0.59878, 'val_loss': 1.65785, 'val_acc': 0.5532}\n",
      "Epoch 32/100\n",
      "------------------------------\n",
      "{'time': 28.87934, 'train_loss': 1.38926, 'train_acc': 0.6058, 'val_loss': 1.71124, 'val_acc': 0.541}\n",
      "Epoch 33/100\n",
      "------------------------------\n",
      "{'time': 37.0879, 'train_loss': 1.37297, 'train_acc': 0.60726, 'val_loss': 1.64192, 'val_acc': 0.5606}\n",
      "Epoch 34/100\n",
      "------------------------------\n",
      "{'time': 28.74794, 'train_loss': 1.35707, 'train_acc': 0.61094, 'val_loss': 1.69549, 'val_acc': 0.5422}\n",
      "Epoch 35/100\n",
      "------------------------------\n",
      "{'time': 28.19209, 'train_loss': 1.35128, 'train_acc': 0.61268, 'val_loss': 1.64005, 'val_acc': 0.561}\n",
      "Epoch 36/100\n",
      "------------------------------\n",
      "{'time': 28.60488, 'train_loss': 1.33573, 'train_acc': 0.61912, 'val_loss': 1.66706, 'val_acc': 0.5474}\n",
      "Epoch 37/100\n",
      "------------------------------\n",
      "{'time': 28.5169, 'train_loss': 1.32736, 'train_acc': 0.61776, 'val_loss': 1.64195, 'val_acc': 0.553}\n",
      "Epoch 38/100\n",
      "------------------------------\n",
      "{'time': 28.3569, 'train_loss': 1.32628, 'train_acc': 0.61796, 'val_loss': 1.69351, 'val_acc': 0.5452}\n",
      "Epoch 39/100\n",
      "------------------------------\n",
      "{'time': 28.72355, 'train_loss': 1.29922, 'train_acc': 0.6261, 'val_loss': 1.66066, 'val_acc': 0.5616}\n",
      "Epoch 40/100\n",
      "------------------------------\n",
      "{'time': 28.87409, 'train_loss': 1.28605, 'train_acc': 0.63028, 'val_loss': 1.68979, 'val_acc': 0.5512}\n",
      "Epoch 41/100\n",
      "------------------------------\n",
      "{'time': 28.15939, 'train_loss': 1.28147, 'train_acc': 0.63058, 'val_loss': 1.6262, 'val_acc': 0.5676}\n",
      "Epoch 42/100\n",
      "------------------------------\n",
      "{'time': 29.61708, 'train_loss': 1.26975, 'train_acc': 0.63368, 'val_loss': 1.69039, 'val_acc': 0.5536}\n",
      "Epoch 43/100\n",
      "------------------------------\n",
      "{'time': 28.38822, 'train_loss': 1.26658, 'train_acc': 0.63536, 'val_loss': 1.63775, 'val_acc': 0.5634}\n",
      "Epoch 44/100\n",
      "------------------------------\n",
      "{'time': 37.49921, 'train_loss': 1.25169, 'train_acc': 0.63826, 'val_loss': 1.6268, 'val_acc': 0.5652}\n",
      "Epoch 45/100\n",
      "------------------------------\n",
      "{'time': 28.68466, 'train_loss': 1.23884, 'train_acc': 0.64052, 'val_loss': 1.64102, 'val_acc': 0.564}\n",
      "Epoch 46/100\n",
      "------------------------------\n",
      "{'time': 28.68911, 'train_loss': 1.23094, 'train_acc': 0.64404, 'val_loss': 1.64392, 'val_acc': 0.5522}\n",
      "Epoch 47/100\n",
      "------------------------------\n",
      "{'time': 28.21819, 'train_loss': 1.23323, 'train_acc': 0.6435, 'val_loss': 1.66477, 'val_acc': 0.5602}\n",
      "Epoch 48/100\n",
      "------------------------------\n",
      "{'time': 28.71721, 'train_loss': 1.21744, 'train_acc': 0.6438, 'val_loss': 1.6308, 'val_acc': 0.5744}\n",
      "Epoch 49/100\n",
      "------------------------------\n",
      "{'time': 28.25554, 'train_loss': 1.21305, 'train_acc': 0.64492, 'val_loss': 1.68039, 'val_acc': 0.5568}\n",
      "Epoch 50/100\n",
      "------------------------------\n",
      "{'time': 28.68805, 'train_loss': 1.20164, 'train_acc': 0.6522, 'val_loss': 1.62691, 'val_acc': 0.5696}\n",
      "Epoch 51/100\n",
      "------------------------------\n",
      "{'time': 30.10438, 'train_loss': 0.98812, 'train_acc': 0.7084, 'val_loss': 1.46105, 'val_acc': 0.6104}\n",
      "Epoch 52/100\n",
      "------------------------------\n",
      "{'time': 30.34683, 'train_loss': 0.93768, 'train_acc': 0.72196, 'val_loss': 1.45684, 'val_acc': 0.6096}\n",
      "Epoch 53/100\n",
      "------------------------------\n",
      "{'time': 29.80403, 'train_loss': 0.92075, 'train_acc': 0.7275, 'val_loss': 1.45614, 'val_acc': 0.6196}\n",
      "Epoch 54/100\n",
      "------------------------------\n",
      "{'time': 28.73601, 'train_loss': 0.90514, 'train_acc': 0.72952, 'val_loss': 1.46817, 'val_acc': 0.6156}\n",
      "Epoch 55/100\n",
      "------------------------------\n",
      "{'time': 37.26406, 'train_loss': 0.89024, 'train_acc': 0.73422, 'val_loss': 1.5076, 'val_acc': 0.6068}\n",
      "Epoch 56/100\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 29.10951, 'train_loss': 0.88921, 'train_acc': 0.7356, 'val_loss': 1.4694, 'val_acc': 0.6074}\n",
      "Epoch 57/100\n",
      "------------------------------\n",
      "{'time': 28.05627, 'train_loss': 0.87839, 'train_acc': 0.738, 'val_loss': 1.44947, 'val_acc': 0.6162}\n",
      "Epoch 58/100\n",
      "------------------------------\n",
      "{'time': 28.38957, 'train_loss': 0.87642, 'train_acc': 0.73786, 'val_loss': 1.48212, 'val_acc': 0.6104}\n",
      "Epoch 59/100\n",
      "------------------------------\n",
      "{'time': 29.40691, 'train_loss': 0.87187, 'train_acc': 0.7386, 'val_loss': 1.47409, 'val_acc': 0.6156}\n",
      "Epoch 60/100\n",
      "------------------------------\n",
      "{'time': 28.66238, 'train_loss': 0.86628, 'train_acc': 0.74078, 'val_loss': 1.47077, 'val_acc': 0.6084}\n",
      "Epoch 61/100\n",
      "------------------------------\n",
      "{'time': 28.59666, 'train_loss': 0.86225, 'train_acc': 0.74422, 'val_loss': 1.46337, 'val_acc': 0.6198}\n",
      "Epoch 62/100\n",
      "------------------------------\n",
      "{'time': 28.79412, 'train_loss': 0.85252, 'train_acc': 0.74358, 'val_loss': 1.4827, 'val_acc': 0.6144}\n",
      "Epoch 63/100\n",
      "------------------------------\n",
      "{'time': 29.65861, 'train_loss': 0.85225, 'train_acc': 0.74456, 'val_loss': 1.47607, 'val_acc': 0.6212}\n",
      "Epoch 64/100\n",
      "------------------------------\n",
      "{'time': 28.92873, 'train_loss': 0.84925, 'train_acc': 0.7452, 'val_loss': 1.46664, 'val_acc': 0.6182}\n",
      "Epoch 65/100\n",
      "------------------------------\n",
      "{'time': 28.40301, 'train_loss': 0.84309, 'train_acc': 0.74632, 'val_loss': 1.48298, 'val_acc': 0.6174}\n",
      "Epoch 66/100\n",
      "------------------------------\n",
      "{'time': 36.9194, 'train_loss': 0.84621, 'train_acc': 0.7444, 'val_loss': 1.48377, 'val_acc': 0.6104}\n",
      "Epoch 67/100\n",
      "------------------------------\n",
      "{'time': 28.65725, 'train_loss': 0.83794, 'train_acc': 0.74842, 'val_loss': 1.49275, 'val_acc': 0.6122}\n",
      "Epoch 68/100\n",
      "------------------------------\n",
      "{'time': 28.71456, 'train_loss': 0.83457, 'train_acc': 0.748, 'val_loss': 1.46945, 'val_acc': 0.6138}\n",
      "Epoch 69/100\n",
      "------------------------------\n",
      "{'time': 29.21451, 'train_loss': 0.83443, 'train_acc': 0.74844, 'val_loss': 1.47481, 'val_acc': 0.6204}\n",
      "Epoch 70/100\n",
      "------------------------------\n",
      "{'time': 29.08324, 'train_loss': 0.82956, 'train_acc': 0.75014, 'val_loss': 1.50076, 'val_acc': 0.6184}\n",
      "Epoch 71/100\n",
      "------------------------------\n",
      "{'time': 28.09022, 'train_loss': 0.82659, 'train_acc': 0.7521, 'val_loss': 1.47916, 'val_acc': 0.6164}\n",
      "Epoch 72/100\n",
      "------------------------------\n",
      "{'time': 29.38232, 'train_loss': 0.82984, 'train_acc': 0.7497, 'val_loss': 1.48484, 'val_acc': 0.6162}\n",
      "Epoch 73/100\n",
      "------------------------------\n",
      "{'time': 28.76471, 'train_loss': 0.8205, 'train_acc': 0.75124, 'val_loss': 1.47122, 'val_acc': 0.6212}\n",
      "Epoch 74/100\n",
      "------------------------------\n",
      "{'time': 29.28034, 'train_loss': 0.82213, 'train_acc': 0.75262, 'val_loss': 1.51428, 'val_acc': 0.613}\n",
      "Epoch 75/100\n",
      "------------------------------\n",
      "{'time': 30.76933, 'train_loss': 0.82493, 'train_acc': 0.7532, 'val_loss': 1.49081, 'val_acc': 0.6132}\n",
      "Epoch 76/100\n",
      "------------------------------\n",
      "{'time': 33.0185, 'train_loss': 0.79079, 'train_acc': 0.76142, 'val_loss': 1.49849, 'val_acc': 0.6114}\n",
      "Epoch 77/100\n",
      "------------------------------\n",
      "{'time': 34.16348, 'train_loss': 0.78734, 'train_acc': 0.76268, 'val_loss': 1.46561, 'val_acc': 0.6184}\n",
      "Epoch 78/100\n",
      "------------------------------\n",
      "{'time': 28.83234, 'train_loss': 0.79037, 'train_acc': 0.76068, 'val_loss': 1.48996, 'val_acc': 0.6118}\n",
      "Epoch 79/100\n",
      "------------------------------\n",
      "{'time': 28.54937, 'train_loss': 0.79036, 'train_acc': 0.7623, 'val_loss': 1.47754, 'val_acc': 0.6276}\n",
      "Epoch 80/100\n",
      "------------------------------\n",
      "{'time': 28.18794, 'train_loss': 0.78129, 'train_acc': 0.76334, 'val_loss': 1.44675, 'val_acc': 0.6212}\n",
      "Epoch 81/100\n",
      "------------------------------\n",
      "{'time': 28.69068, 'train_loss': 0.78806, 'train_acc': 0.76182, 'val_loss': 1.47297, 'val_acc': 0.6166}\n",
      "Epoch 82/100\n",
      "------------------------------\n",
      "{'time': 28.94325, 'train_loss': 0.78014, 'train_acc': 0.76338, 'val_loss': 1.48226, 'val_acc': 0.618}\n",
      "Epoch 83/100\n",
      "------------------------------\n",
      "{'time': 29.20773, 'train_loss': 0.78038, 'train_acc': 0.76364, 'val_loss': 1.47433, 'val_acc': 0.6176}\n",
      "Epoch 84/100\n",
      "------------------------------\n",
      "{'time': 28.7488, 'train_loss': 0.78523, 'train_acc': 0.76162, 'val_loss': 1.48454, 'val_acc': 0.6108}\n",
      "Epoch 85/100\n",
      "------------------------------\n",
      "{'time': 29.47609, 'train_loss': 0.78017, 'train_acc': 0.76322, 'val_loss': 1.43822, 'val_acc': 0.6148}\n",
      "Epoch 86/100\n",
      "------------------------------\n",
      "{'time': 29.07489, 'train_loss': 0.78125, 'train_acc': 0.76282, 'val_loss': 1.47178, 'val_acc': 0.6212}\n",
      "Epoch 87/100\n",
      "------------------------------\n",
      "{'time': 36.05684, 'train_loss': 0.7816, 'train_acc': 0.7627, 'val_loss': 1.47206, 'val_acc': 0.6252}\n",
      "Epoch 88/100\n",
      "------------------------------\n",
      "{'time': 30.9766, 'train_loss': 0.78315, 'train_acc': 0.76344, 'val_loss': 1.46272, 'val_acc': 0.6194}\n",
      "Epoch 89/100\n",
      "------------------------------\n",
      "{'time': 29.32082, 'train_loss': 0.77866, 'train_acc': 0.7658, 'val_loss': 1.49148, 'val_acc': 0.6194}\n",
      "Epoch 90/100\n",
      "------------------------------\n",
      "{'time': 28.5845, 'train_loss': 0.78187, 'train_acc': 0.76306, 'val_loss': 1.44727, 'val_acc': 0.6272}\n",
      "Epoch 91/100\n",
      "------------------------------\n",
      "{'time': 28.54897, 'train_loss': 0.77638, 'train_acc': 0.76542, 'val_loss': 1.4544, 'val_acc': 0.6262}\n",
      "Epoch 92/100\n",
      "------------------------------\n",
      "{'time': 28.60542, 'train_loss': 0.78104, 'train_acc': 0.76386, 'val_loss': 1.47458, 'val_acc': 0.6216}\n",
      "Epoch 93/100\n",
      "------------------------------\n",
      "{'time': 28.7422, 'train_loss': 0.78029, 'train_acc': 0.76296, 'val_loss': 1.4698, 'val_acc': 0.6186}\n",
      "Epoch 94/100\n",
      "------------------------------\n",
      "{'time': 28.80463, 'train_loss': 0.77598, 'train_acc': 0.76348, 'val_loss': 1.47235, 'val_acc': 0.6186}\n",
      "Epoch 95/100\n",
      "------------------------------\n",
      "{'time': 29.17913, 'train_loss': 0.77882, 'train_acc': 0.76372, 'val_loss': 1.48281, 'val_acc': 0.618}\n",
      "Epoch 96/100\n",
      "------------------------------\n",
      "{'time': 29.87735, 'train_loss': 0.77452, 'train_acc': 0.76592, 'val_loss': 1.45896, 'val_acc': 0.6226}\n",
      "Epoch 97/100\n",
      "------------------------------\n",
      "{'time': 29.77729, 'train_loss': 0.77633, 'train_acc': 0.7645, 'val_loss': 1.49927, 'val_acc': 0.6104}\n",
      "Epoch 98/100\n",
      "------------------------------\n",
      "{'time': 37.65707, 'train_loss': 0.78167, 'train_acc': 0.76354, 'val_loss': 1.4733, 'val_acc': 0.6174}\n",
      "Epoch 99/100\n",
      "------------------------------\n",
      "{'time': 30.2999, 'train_loss': 0.77539, 'train_acc': 0.7645, 'val_loss': 1.46884, 'val_acc': 0.6106}\n",
      "Epoch 100/100\n",
      "------------------------------\n",
      "{'time': 29.81237, 'train_loss': 0.77607, 'train_acc': 0.76654, 'val_loss': 1.47474, 'val_acc': 0.619}\n",
      "----------------------------- Test --------------------------------\n",
      "{'time': 31.00973, 'test_loss': 1.47474, 'test_acc': 0.6124}\n"
     ]
    }
   ],
   "source": [
    "### Train loop + validation/ also test at the end\n",
    "print(\"Configuration: \", \"model:ResNet(small)\", \" model_n:\", model_n, \" batch size:\", batch_size, \n",
    "      \" optimizer:SGD\", \" lr:\", lr, \" epochs:\", epochs)\n",
    "\n",
    "all_epoch_loss = {\"train\": [], \"validation\": []}\n",
    "all_epoch_acc = {\"train\":  [], \"validation\": []}\n",
    "\n",
    "print(\"----------------------------- Train --------------------------------\")\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    \n",
    "    epoch_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    epoch_acc = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    \n",
    "    running_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    running_corrects = {\"train\": 0, \"validation\": 0}\n",
    "    \n",
    "    for phase in [\"train\", \"validation\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train(True)\n",
    "        else:\n",
    "            model.train(False)\n",
    "        \n",
    "        for data in data_loaders[phase]:\n",
    "            inputs, labels = data \n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # clear all gradients\n",
    "            \n",
    "            outputs = model(inputs) # batch_size x num_classes\n",
    "            _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                loss.backward()  # compute gradients\n",
    "                optimizer.step() # update weights/biases\n",
    "               \n",
    "            running_loss[phase] += loss.data.item() * inputs.size(0)\n",
    "            running_corrects[phase] += torch.sum(preds == labels.data).item()\n",
    "        \n",
    "        all_epoch_loss[phase].append(running_loss[phase] / dataset_sizes[phase])\n",
    "        all_epoch_acc[phase].append(running_corrects[phase] / dataset_sizes[phase])\n",
    "        \n",
    "        epoch_loss[phase] = running_loss[phase] / dataset_sizes[phase]\n",
    "        epoch_acc[phase] =  running_corrects[phase] / dataset_sizes[phase]\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "        'time': np.round(time.time()-start_time, 5),\n",
    "        'train_loss': np.round(epoch_loss[\"train\"], 5),\n",
    "        'train_acc': np.round(epoch_acc[\"train\"], 5),\n",
    "        'val_loss': np.round(epoch_loss[\"validation\"], 5),\n",
    "        'val_acc': np.round(epoch_acc[\"validation\"], 5),\n",
    "    })\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    \n",
    "with open('ResNet_3_bs_32_opt_SGD_lr_1_epochs_100_loss.txt', 'w') as f_loss:\n",
    "    print(all_epoch_loss, file=f_loss)\n",
    "    \n",
    "with open('ResNet_3_bs_32_opt_SGD_lr_1_epochs_100_acc.txt', 'w') as f_acc:\n",
    "    print(all_epoch_acc, file=f_acc)\n",
    "    \n",
    "### evaluating the model with test set\n",
    "print(\"----------------------------- Test --------------------------------\")\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # clear all gradients\n",
    "\n",
    "        outputs = model(inputs) # batch_size x num_classes\n",
    "        _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        running_loss += loss.data.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "    'time': np.round(time.time()-start_time, 5),\n",
    "    'test_loss': np.round(running_loss/ dataset_sizes['test'], 5),\n",
    "    'test_acc': np.round(running_corrects/ dataset_sizes['test'], 5),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1572d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bdd860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da86d8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:  model:ResNet(small)  model_n: 8  batch size: 32  optimizer:SGD  lr: 0.1  epochs: 100\n",
      "----------------------------- Train --------------------------------\n",
      "Epoch 1/100\n",
      "------------------------------\n",
      "{'time': 59.92077, 'train_loss': 4.32535, 'train_acc': 0.0387, 'val_loss': 4.07001, 'val_acc': 0.0672}\n",
      "Epoch 2/100\n",
      "------------------------------\n",
      "{'time': 55.63084, 'train_loss': 3.82518, 'train_acc': 0.10106, 'val_loss': 3.65821, 'val_acc': 0.1388}\n",
      "Epoch 3/100\n",
      "------------------------------\n",
      "{'time': 54.97735, 'train_loss': 3.45851, 'train_acc': 0.16812, 'val_loss': 3.29394, 'val_acc': 0.199}\n",
      "Epoch 4/100\n",
      "------------------------------\n",
      "{'time': 54.96699, 'train_loss': 3.11303, 'train_acc': 0.22886, 'val_loss': 2.93745, 'val_acc': 0.26}\n",
      "Epoch 5/100\n",
      "------------------------------\n",
      "{'time': 55.37949, 'train_loss': 2.82663, 'train_acc': 0.28456, 'val_loss': 2.661, 'val_acc': 0.32}\n",
      "Epoch 6/100\n",
      "------------------------------\n",
      "{'time': 59.1742, 'train_loss': 2.61062, 'train_acc': 0.3263, 'val_loss': 2.58621, 'val_acc': 0.3346}\n",
      "Epoch 7/100\n",
      "------------------------------\n",
      "{'time': 53.61004, 'train_loss': 2.44059, 'train_acc': 0.36368, 'val_loss': 2.40017, 'val_acc': 0.3744}\n",
      "Epoch 8/100\n",
      "------------------------------\n",
      "{'time': 54.06606, 'train_loss': 2.2902, 'train_acc': 0.3937, 'val_loss': 2.26374, 'val_acc': 0.3974}\n",
      "Epoch 9/100\n",
      "------------------------------\n",
      "{'time': 54.02471, 'train_loss': 2.17901, 'train_acc': 0.42328, 'val_loss': 2.11392, 'val_acc': 0.441}\n",
      "Epoch 10/100\n",
      "------------------------------\n",
      "{'time': 53.86204, 'train_loss': 2.07581, 'train_acc': 0.44612, 'val_loss': 2.17877, 'val_acc': 0.4328}\n",
      "Epoch 11/100\n",
      "------------------------------\n",
      "{'time': 52.41131, 'train_loss': 1.97547, 'train_acc': 0.46626, 'val_loss': 2.17521, 'val_acc': 0.4338}\n",
      "Epoch 12/100\n",
      "------------------------------\n",
      "{'time': 58.34659, 'train_loss': 1.9002, 'train_acc': 0.4825, 'val_loss': 1.97979, 'val_acc': 0.4774}\n",
      "Epoch 13/100\n",
      "------------------------------\n",
      "{'time': 55.81706, 'train_loss': 1.81996, 'train_acc': 0.50024, 'val_loss': 1.91577, 'val_acc': 0.4902}\n",
      "Epoch 14/100\n",
      "------------------------------\n",
      "{'time': 55.75595, 'train_loss': 1.76146, 'train_acc': 0.51502, 'val_loss': 1.89196, 'val_acc': 0.4866}\n",
      "Epoch 15/100\n",
      "------------------------------\n",
      "{'time': 54.37045, 'train_loss': 1.7155, 'train_acc': 0.52684, 'val_loss': 1.81301, 'val_acc': 0.5074}\n",
      "Epoch 16/100\n",
      "------------------------------\n",
      "{'time': 57.06074, 'train_loss': 1.65629, 'train_acc': 0.53664, 'val_loss': 1.77222, 'val_acc': 0.5152}\n",
      "Epoch 17/100\n",
      "------------------------------\n",
      "{'time': 57.36463, 'train_loss': 1.61234, 'train_acc': 0.54762, 'val_loss': 1.75736, 'val_acc': 0.5246}\n",
      "Epoch 18/100\n",
      "------------------------------\n",
      "{'time': 58.76411, 'train_loss': 1.56614, 'train_acc': 0.55918, 'val_loss': 1.76484, 'val_acc': 0.52}\n",
      "Epoch 19/100\n",
      "------------------------------\n",
      "{'time': 54.54374, 'train_loss': 1.53154, 'train_acc': 0.56646, 'val_loss': 1.81889, 'val_acc': 0.5136}\n",
      "Epoch 20/100\n",
      "------------------------------\n",
      "{'time': 53.85212, 'train_loss': 1.49084, 'train_acc': 0.57938, 'val_loss': 1.69637, 'val_acc': 0.5456}\n",
      "Epoch 21/100\n",
      "------------------------------\n",
      "{'time': 54.37919, 'train_loss': 1.45558, 'train_acc': 0.58712, 'val_loss': 1.68501, 'val_acc': 0.5426}\n",
      "Epoch 22/100\n",
      "------------------------------\n",
      "{'time': 55.22354, 'train_loss': 1.42227, 'train_acc': 0.59516, 'val_loss': 1.69401, 'val_acc': 0.5374}\n",
      "Epoch 23/100\n",
      "------------------------------\n",
      "{'time': 54.20075, 'train_loss': 1.39068, 'train_acc': 0.6038, 'val_loss': 1.67998, 'val_acc': 0.552}\n",
      "Epoch 24/100\n",
      "------------------------------\n",
      "{'time': 62.97344, 'train_loss': 1.35856, 'train_acc': 0.60942, 'val_loss': 1.6652, 'val_acc': 0.5526}\n",
      "Epoch 25/100\n",
      "------------------------------\n",
      "{'time': 53.63943, 'train_loss': 1.32692, 'train_acc': 0.6182, 'val_loss': 1.63645, 'val_acc': 0.551}\n",
      "Epoch 26/100\n",
      "------------------------------\n",
      "{'time': 53.0588, 'train_loss': 1.30753, 'train_acc': 0.62272, 'val_loss': 1.63775, 'val_acc': 0.5562}\n",
      "Epoch 27/100\n",
      "------------------------------\n",
      "{'time': 55.06518, 'train_loss': 1.2801, 'train_acc': 0.62876, 'val_loss': 1.69122, 'val_acc': 0.5436}\n",
      "Epoch 28/100\n",
      "------------------------------\n",
      "{'time': 53.77982, 'train_loss': 1.25173, 'train_acc': 0.63724, 'val_loss': 1.64663, 'val_acc': 0.5544}\n",
      "Epoch 29/100\n",
      "------------------------------\n",
      "{'time': 62.71641, 'train_loss': 1.2336, 'train_acc': 0.64252, 'val_loss': 1.58982, 'val_acc': 0.5726}\n",
      "Epoch 30/100\n",
      "------------------------------\n",
      "{'time': 54.19303, 'train_loss': 1.21774, 'train_acc': 0.64408, 'val_loss': 1.61261, 'val_acc': 0.571}\n",
      "Epoch 31/100\n",
      "------------------------------\n",
      "{'time': 54.53206, 'train_loss': 1.19188, 'train_acc': 0.65348, 'val_loss': 1.58392, 'val_acc': 0.5742}\n",
      "Epoch 32/100\n",
      "------------------------------\n",
      "{'time': 54.75363, 'train_loss': 1.1695, 'train_acc': 0.65648, 'val_loss': 1.60374, 'val_acc': 0.5692}\n",
      "Epoch 33/100\n",
      "------------------------------\n",
      "{'time': 54.90032, 'train_loss': 1.14772, 'train_acc': 0.66394, 'val_loss': 1.61761, 'val_acc': 0.5708}\n",
      "Epoch 34/100\n",
      "------------------------------\n",
      "{'time': 54.16471, 'train_loss': 1.13444, 'train_acc': 0.6668, 'val_loss': 1.62812, 'val_acc': 0.574}\n",
      "Epoch 35/100\n",
      "------------------------------\n",
      "{'time': 62.92322, 'train_loss': 1.11453, 'train_acc': 0.67284, 'val_loss': 1.62526, 'val_acc': 0.5664}\n",
      "Epoch 36/100\n",
      "------------------------------\n",
      "{'time': 53.92893, 'train_loss': 1.09628, 'train_acc': 0.67508, 'val_loss': 1.64624, 'val_acc': 0.5518}\n",
      "Epoch 37/100\n",
      "------------------------------\n",
      "{'time': 53.89556, 'train_loss': 1.07614, 'train_acc': 0.68194, 'val_loss': 1.57258, 'val_acc': 0.5806}\n",
      "Epoch 38/100\n",
      "------------------------------\n",
      "{'time': 54.3027, 'train_loss': 1.06249, 'train_acc': 0.68574, 'val_loss': 1.69613, 'val_acc': 0.5658}\n",
      "Epoch 39/100\n",
      "------------------------------\n",
      "{'time': 55.17956, 'train_loss': 1.04369, 'train_acc': 0.6901, 'val_loss': 1.61448, 'val_acc': 0.579}\n",
      "Epoch 40/100\n",
      "------------------------------\n",
      "{'time': 54.38978, 'train_loss': 1.02855, 'train_acc': 0.69382, 'val_loss': 1.6106, 'val_acc': 0.5822}\n",
      "Epoch 41/100\n",
      "------------------------------\n",
      "{'time': 63.42315, 'train_loss': 1.01628, 'train_acc': 0.69428, 'val_loss': 1.62621, 'val_acc': 0.5838}\n",
      "Epoch 42/100\n",
      "------------------------------\n",
      "{'time': 55.52893, 'train_loss': 1.00166, 'train_acc': 0.6991, 'val_loss': 1.63746, 'val_acc': 0.5788}\n",
      "Epoch 43/100\n",
      "------------------------------\n",
      "{'time': 53.5882, 'train_loss': 0.98594, 'train_acc': 0.70304, 'val_loss': 1.57928, 'val_acc': 0.5888}\n",
      "Epoch 44/100\n",
      "------------------------------\n",
      "{'time': 52.77356, 'train_loss': 0.97351, 'train_acc': 0.71038, 'val_loss': 1.66524, 'val_acc': 0.5756}\n",
      "Epoch 45/100\n",
      "------------------------------\n",
      "{'time': 56.54764, 'train_loss': 0.95535, 'train_acc': 0.71368, 'val_loss': 1.60215, 'val_acc': 0.5808}\n",
      "Epoch 46/100\n",
      "------------------------------\n",
      "{'time': 63.61328, 'train_loss': 0.94994, 'train_acc': 0.71368, 'val_loss': 1.5976, 'val_acc': 0.583}\n",
      "Epoch 47/100\n",
      "------------------------------\n",
      "{'time': 62.70974, 'train_loss': 0.92689, 'train_acc': 0.71936, 'val_loss': 1.61675, 'val_acc': 0.5872}\n",
      "Epoch 48/100\n",
      "------------------------------\n",
      "{'time': 61.50378, 'train_loss': 0.91893, 'train_acc': 0.72222, 'val_loss': 1.62109, 'val_acc': 0.5922}\n",
      "Epoch 49/100\n",
      "------------------------------\n",
      "{'time': 60.18165, 'train_loss': 0.90823, 'train_acc': 0.72526, 'val_loss': 1.6142, 'val_acc': 0.59}\n",
      "Epoch 50/100\n",
      "------------------------------\n",
      "{'time': 60.64255, 'train_loss': 0.88771, 'train_acc': 0.73366, 'val_loss': 1.67745, 'val_acc': 0.5716}\n",
      "Epoch 51/100\n",
      "------------------------------\n",
      "{'time': 61.04779, 'train_loss': 0.67431, 'train_acc': 0.79336, 'val_loss': 1.45185, 'val_acc': 0.6314}\n",
      "Epoch 52/100\n",
      "------------------------------\n",
      "{'time': 63.8242, 'train_loss': 0.60644, 'train_acc': 0.81344, 'val_loss': 1.43439, 'val_acc': 0.6404}\n",
      "Epoch 53/100\n",
      "------------------------------\n",
      "{'time': 58.90556, 'train_loss': 0.58539, 'train_acc': 0.81892, 'val_loss': 1.44934, 'val_acc': 0.6358}\n",
      "Epoch 54/100\n",
      "------------------------------\n",
      "{'time': 58.61566, 'train_loss': 0.55944, 'train_acc': 0.82676, 'val_loss': 1.45937, 'val_acc': 0.6396}\n",
      "Epoch 55/100\n",
      "------------------------------\n",
      "{'time': 59.98735, 'train_loss': 0.55133, 'train_acc': 0.82916, 'val_loss': 1.47031, 'val_acc': 0.6368}\n",
      "Epoch 56/100\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 59.08249, 'train_loss': 0.53819, 'train_acc': 0.83212, 'val_loss': 1.45325, 'val_acc': 0.6398}\n",
      "Epoch 57/100\n",
      "------------------------------\n",
      "{'time': 63.60274, 'train_loss': 0.53428, 'train_acc': 0.83316, 'val_loss': 1.47784, 'val_acc': 0.638}\n",
      "Epoch 58/100\n",
      "------------------------------\n",
      "{'time': 62.4783, 'train_loss': 0.51871, 'train_acc': 0.83724, 'val_loss': 1.49112, 'val_acc': 0.6346}\n",
      "Epoch 59/100\n",
      "------------------------------\n",
      "{'time': 62.42422, 'train_loss': 0.51499, 'train_acc': 0.83898, 'val_loss': 1.47314, 'val_acc': 0.6358}\n",
      "Epoch 60/100\n",
      "------------------------------\n",
      "{'time': 61.96302, 'train_loss': 0.50197, 'train_acc': 0.84164, 'val_loss': 1.48724, 'val_acc': 0.645}\n",
      "Epoch 61/100\n",
      "------------------------------\n",
      "{'time': 64.10068, 'train_loss': 0.5001, 'train_acc': 0.84406, 'val_loss': 1.48238, 'val_acc': 0.6414}\n",
      "Epoch 62/100\n",
      "------------------------------\n",
      "{'time': 60.23529, 'train_loss': 0.49169, 'train_acc': 0.84508, 'val_loss': 1.50742, 'val_acc': 0.6334}\n",
      "Epoch 63/100\n",
      "------------------------------\n",
      "{'time': 61.01221, 'train_loss': 0.48854, 'train_acc': 0.84484, 'val_loss': 1.50753, 'val_acc': 0.6348}\n",
      "Epoch 64/100\n",
      "------------------------------\n",
      "{'time': 61.00808, 'train_loss': 0.47734, 'train_acc': 0.8503, 'val_loss': 1.49784, 'val_acc': 0.6338}\n",
      "Epoch 65/100\n",
      "------------------------------\n",
      "{'time': 60.61077, 'train_loss': 0.47215, 'train_acc': 0.85116, 'val_loss': 1.54812, 'val_acc': 0.6342}\n",
      "Epoch 66/100\n",
      "------------------------------\n",
      "{'time': 64.03282, 'train_loss': 0.47538, 'train_acc': 0.84902, 'val_loss': 1.5475, 'val_acc': 0.6344}\n",
      "Epoch 67/100\n",
      "------------------------------\n",
      "{'time': 59.06833, 'train_loss': 0.46202, 'train_acc': 0.85486, 'val_loss': 1.50143, 'val_acc': 0.64}\n",
      "Epoch 68/100\n",
      "------------------------------\n",
      "{'time': 60.14919, 'train_loss': 0.45951, 'train_acc': 0.85254, 'val_loss': 1.5256, 'val_acc': 0.6406}\n",
      "Epoch 69/100\n",
      "------------------------------\n",
      "{'time': 59.28041, 'train_loss': 0.45187, 'train_acc': 0.857, 'val_loss': 1.5382, 'val_acc': 0.6414}\n",
      "Epoch 70/100\n",
      "------------------------------\n",
      "{'time': 57.5176, 'train_loss': 0.45205, 'train_acc': 0.85584, 'val_loss': 1.56809, 'val_acc': 0.6358}\n",
      "Epoch 71/100\n",
      "------------------------------\n",
      "{'time': 62.3875, 'train_loss': 0.44853, 'train_acc': 0.85678, 'val_loss': 1.55808, 'val_acc': 0.639}\n",
      "Epoch 72/100\n",
      "------------------------------\n",
      "{'time': 61.14097, 'train_loss': 0.44698, 'train_acc': 0.8569, 'val_loss': 1.55209, 'val_acc': 0.6378}\n",
      "Epoch 73/100\n",
      "------------------------------\n",
      "{'time': 62.00041, 'train_loss': 0.44045, 'train_acc': 0.85932, 'val_loss': 1.59917, 'val_acc': 0.6372}\n",
      "Epoch 74/100\n",
      "------------------------------\n",
      "{'time': 60.7531, 'train_loss': 0.43436, 'train_acc': 0.86168, 'val_loss': 1.56749, 'val_acc': 0.6344}\n",
      "Epoch 75/100\n",
      "------------------------------\n",
      "{'time': 60.98402, 'train_loss': 0.42627, 'train_acc': 0.86392, 'val_loss': 1.58712, 'val_acc': 0.6362}\n",
      "Epoch 76/100\n",
      "------------------------------\n",
      "{'time': 64.773, 'train_loss': 0.41139, 'train_acc': 0.86834, 'val_loss': 1.59384, 'val_acc': 0.6328}\n",
      "Epoch 77/100\n",
      "------------------------------\n",
      "{'time': 60.38651, 'train_loss': 0.40072, 'train_acc': 0.87126, 'val_loss': 1.53883, 'val_acc': 0.6464}\n",
      "Epoch 78/100\n",
      "------------------------------\n",
      "{'time': 62.616, 'train_loss': 0.40447, 'train_acc': 0.87216, 'val_loss': 1.55203, 'val_acc': 0.6446}\n",
      "Epoch 79/100\n",
      "------------------------------\n",
      "{'time': 61.50957, 'train_loss': 0.40248, 'train_acc': 0.87042, 'val_loss': 1.59432, 'val_acc': 0.6318}\n",
      "Epoch 80/100\n",
      "------------------------------\n",
      "{'time': 60.81586, 'train_loss': 0.40335, 'train_acc': 0.8717, 'val_loss': 1.57498, 'val_acc': 0.6412}\n",
      "Epoch 81/100\n",
      "------------------------------\n",
      "{'time': 63.3315, 'train_loss': 0.39847, 'train_acc': 0.8732, 'val_loss': 1.57357, 'val_acc': 0.6396}\n",
      "Epoch 82/100\n",
      "------------------------------\n",
      "{'time': 58.0271, 'train_loss': 0.39829, 'train_acc': 0.87368, 'val_loss': 1.58057, 'val_acc': 0.643}\n",
      "Epoch 83/100\n",
      "------------------------------\n",
      "{'time': 58.81052, 'train_loss': 0.39797, 'train_acc': 0.8741, 'val_loss': 1.58448, 'val_acc': 0.6294}\n",
      "Epoch 84/100\n",
      "------------------------------\n",
      "{'time': 59.1168, 'train_loss': 0.39403, 'train_acc': 0.8752, 'val_loss': 1.58962, 'val_acc': 0.6448}\n",
      "Epoch 85/100\n",
      "------------------------------\n",
      "{'time': 59.14336, 'train_loss': 0.39411, 'train_acc': 0.87416, 'val_loss': 1.55868, 'val_acc': 0.6418}\n",
      "Epoch 86/100\n",
      "------------------------------\n",
      "{'time': 61.54948, 'train_loss': 0.39249, 'train_acc': 0.87536, 'val_loss': 1.54846, 'val_acc': 0.6378}\n",
      "Epoch 87/100\n",
      "------------------------------\n",
      "{'time': 59.72786, 'train_loss': 0.40006, 'train_acc': 0.87182, 'val_loss': 1.56952, 'val_acc': 0.6458}\n",
      "Epoch 88/100\n",
      "------------------------------\n",
      "{'time': 59.04028, 'train_loss': 0.39346, 'train_acc': 0.87538, 'val_loss': 1.59413, 'val_acc': 0.6334}\n",
      "Epoch 89/100\n",
      "------------------------------\n",
      "{'time': 58.62094, 'train_loss': 0.39529, 'train_acc': 0.87312, 'val_loss': 1.56784, 'val_acc': 0.641}\n",
      "Epoch 90/100\n",
      "------------------------------\n",
      "{'time': 58.99941, 'train_loss': 0.38913, 'train_acc': 0.87518, 'val_loss': 1.54647, 'val_acc': 0.6396}\n",
      "Epoch 91/100\n",
      "------------------------------\n",
      "{'time': 58.56665, 'train_loss': 0.39351, 'train_acc': 0.87404, 'val_loss': 1.58252, 'val_acc': 0.64}\n",
      "Epoch 92/100\n",
      "------------------------------\n",
      "{'time': 63.12694, 'train_loss': 0.38934, 'train_acc': 0.87554, 'val_loss': 1.58403, 'val_acc': 0.6374}\n",
      "Epoch 93/100\n",
      "------------------------------\n",
      "{'time': 65.4848, 'train_loss': 0.39273, 'train_acc': 0.87414, 'val_loss': 1.62511, 'val_acc': 0.6436}\n",
      "Epoch 94/100\n",
      "------------------------------\n",
      "{'time': 59.56239, 'train_loss': 0.3912, 'train_acc': 0.87586, 'val_loss': 1.57856, 'val_acc': 0.6316}\n",
      "Epoch 95/100\n",
      "------------------------------\n",
      "{'time': 60.62989, 'train_loss': 0.38474, 'train_acc': 0.87686, 'val_loss': 1.58887, 'val_acc': 0.6402}\n",
      "Epoch 96/100\n",
      "------------------------------\n",
      "{'time': 59.18642, 'train_loss': 0.38503, 'train_acc': 0.87704, 'val_loss': 1.58925, 'val_acc': 0.6464}\n",
      "Epoch 97/100\n",
      "------------------------------\n",
      "{'time': 59.75105, 'train_loss': 0.38877, 'train_acc': 0.87698, 'val_loss': 1.59365, 'val_acc': 0.6342}\n",
      "Epoch 98/100\n",
      "------------------------------\n",
      "{'time': 62.39727, 'train_loss': 0.38255, 'train_acc': 0.87736, 'val_loss': 1.60497, 'val_acc': 0.6416}\n",
      "Epoch 99/100\n",
      "------------------------------\n",
      "{'time': 64.28338, 'train_loss': 0.3825, 'train_acc': 0.87786, 'val_loss': 1.57483, 'val_acc': 0.643}\n",
      "Epoch 100/100\n",
      "------------------------------\n",
      "{'time': 58.705, 'train_loss': 0.38858, 'train_acc': 0.87468, 'val_loss': 1.58672, 'val_acc': 0.6394}\n",
      "----------------------------- Test --------------------------------\n",
      "{'time': 61.04928, 'test_loss': 1.6317, 'test_acc': 0.6326}\n"
     ]
    }
   ],
   "source": [
    "### Train loop + validation/ also test at the end\n",
    "print(\"Configuration: \", \"model:ResNet(small)\", \" model_n:\", model_n, \" batch size:\", batch_size, \n",
    "      \" optimizer:SGD\", \" lr:\", lr, \" epochs:\", epochs)\n",
    "\n",
    "all_epoch_loss = {\"train\": [], \"validation\": []}\n",
    "all_epoch_acc = {\"train\":  [], \"validation\": []}\n",
    "\n",
    "print(\"----------------------------- Train --------------------------------\")\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    \n",
    "    epoch_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    epoch_acc = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    \n",
    "    running_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    running_corrects = {\"train\": 0, \"validation\": 0}\n",
    "    \n",
    "    for phase in [\"train\", \"validation\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train(True)\n",
    "        else:\n",
    "            model.train(False)\n",
    "        \n",
    "        for data in data_loaders[phase]:\n",
    "            inputs, labels = data \n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # clear all gradients\n",
    "            \n",
    "            outputs = model(inputs) # batch_size x num_classes\n",
    "            _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                loss.backward()  # compute gradients\n",
    "                optimizer.step() # update weights/biases\n",
    "               \n",
    "            running_loss[phase] += loss.data.item() * inputs.size(0)\n",
    "            running_corrects[phase] += torch.sum(preds == labels.data).item()\n",
    "        \n",
    "        all_epoch_loss[phase].append(running_loss[phase] / dataset_sizes[phase])\n",
    "        all_epoch_acc[phase].append(running_corrects[phase] / dataset_sizes[phase])\n",
    "        \n",
    "        epoch_loss[phase] = running_loss[phase] / dataset_sizes[phase]\n",
    "        epoch_acc[phase] =  running_corrects[phase] / dataset_sizes[phase]\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "        'time': np.round(time.time()-start_time, 5),\n",
    "        'train_loss': np.round(epoch_loss[\"train\"], 5),\n",
    "        'train_acc': np.round(epoch_acc[\"train\"], 5),\n",
    "        'val_loss': np.round(epoch_loss[\"validation\"], 5),\n",
    "        'val_acc': np.round(epoch_acc[\"validation\"], 5),\n",
    "    })\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    \n",
    "with open('ResNet_8_bs_32_opt_SGD_lr_1_epochs_100_loss.txt', 'w') as f_loss:\n",
    "    print(all_epoch_loss, file=f_loss)\n",
    "    \n",
    "with open('ResNet_8_bs_32_opt_SGD_lr_1_epochs_100_acc.txt', 'w') as f_acc:\n",
    "    print(all_epoch_acc, file=f_acc)\n",
    "    \n",
    "### evaluating the model with test set\n",
    "print(\"----------------------------- Test --------------------------------\")\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # clear all gradients\n",
    "\n",
    "        outputs = model(inputs) # batch_size x num_classes\n",
    "        _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        running_loss += loss.data.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "    'time': np.round(time.time()-start_time, 5),\n",
    "    'test_loss': np.round(running_loss/ dataset_sizes['test'], 5),\n",
    "    'test_acc': np.round(running_corrects/ dataset_sizes['test'], 5),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce199e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6595f849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:  model:ResNet(small)  model_n: 16  batch size: 32  optimizer:SGD  lr: 0.1  epochs: 100\n",
      "----------------------------- Train --------------------------------\n",
      "Epoch 1/100\n",
      "------------------------------\n",
      "{'time': 114.81999, 'train_loss': 4.52993, 'train_acc': 0.01976, 'val_loss': 4.44816, 'val_acc': 0.022}\n",
      "Epoch 2/100\n",
      "------------------------------\n",
      "{'time': 110.86615, 'train_loss': 4.09644, 'train_acc': 0.06298, 'val_loss': 3.87522, 'val_acc': 0.0898}\n",
      "Epoch 3/100\n",
      "------------------------------\n",
      "{'time': 115.13954, 'train_loss': 3.75482, 'train_acc': 0.11598, 'val_loss': 3.51611, 'val_acc': 0.1582}\n",
      "Epoch 4/100\n",
      "------------------------------\n",
      "{'time': 108.28486, 'train_loss': 3.4868, 'train_acc': 0.1633, 'val_loss': 3.36987, 'val_acc': 0.1916}\n",
      "Epoch 5/100\n",
      "------------------------------\n",
      "{'time': 107.96477, 'train_loss': 3.26434, 'train_acc': 0.2033, 'val_loss': 3.1193, 'val_acc': 0.233}\n",
      "Epoch 6/100\n",
      "------------------------------\n",
      "{'time': 107.58795, 'train_loss': 3.06294, 'train_acc': 0.24326, 'val_loss': 2.98248, 'val_acc': 0.2606}\n",
      "Epoch 7/100\n",
      "------------------------------\n",
      "{'time': 97.51186, 'train_loss': 2.85978, 'train_acc': 0.28188, 'val_loss': 2.77609, 'val_acc': 0.301}\n",
      "Epoch 8/100\n",
      "------------------------------\n",
      "{'time': 98.23522, 'train_loss': 2.67385, 'train_acc': 0.31674, 'val_loss': 2.73233, 'val_acc': 0.31}\n",
      "Epoch 9/100\n",
      "------------------------------\n",
      "{'time': 101.4829, 'train_loss': 2.51285, 'train_acc': 0.35104, 'val_loss': 2.47614, 'val_acc': 0.3676}\n",
      "Epoch 10/100\n",
      "------------------------------\n",
      "{'time': 99.3674, 'train_loss': 2.38276, 'train_acc': 0.37674, 'val_loss': 2.34069, 'val_acc': 0.3924}\n",
      "Epoch 11/100\n",
      "------------------------------\n",
      "{'time': 99.24666, 'train_loss': 2.26565, 'train_acc': 0.40292, 'val_loss': 2.29386, 'val_acc': 0.4058}\n",
      "Epoch 12/100\n",
      "------------------------------\n",
      "{'time': 102.69822, 'train_loss': 2.15356, 'train_acc': 0.42916, 'val_loss': 2.21936, 'val_acc': 0.4218}\n",
      "Epoch 13/100\n",
      "------------------------------\n",
      "{'time': 102.69192, 'train_loss': 2.05254, 'train_acc': 0.44998, 'val_loss': 2.2277, 'val_acc': 0.4234}\n",
      "Epoch 14/100\n",
      "------------------------------\n",
      "{'time': 102.86172, 'train_loss': 1.96517, 'train_acc': 0.46842, 'val_loss': 2.05333, 'val_acc': 0.4566}\n",
      "Epoch 15/100\n",
      "------------------------------\n",
      "{'time': 101.11565, 'train_loss': 1.88178, 'train_acc': 0.48868, 'val_loss': 2.05861, 'val_acc': 0.4602}\n",
      "Epoch 16/100\n",
      "------------------------------\n",
      "{'time': 102.5829, 'train_loss': 1.80909, 'train_acc': 0.50364, 'val_loss': 2.01573, 'val_acc': 0.4664}\n",
      "Epoch 17/100\n",
      "------------------------------\n",
      "{'time': 103.04621, 'train_loss': 1.74589, 'train_acc': 0.51984, 'val_loss': 1.98012, 'val_acc': 0.4772}\n",
      "Epoch 18/100\n",
      "------------------------------\n",
      "{'time': 98.88412, 'train_loss': 1.69942, 'train_acc': 0.5282, 'val_loss': 1.95805, 'val_acc': 0.4862}\n",
      "Epoch 19/100\n",
      "------------------------------\n",
      "{'time': 96.61367, 'train_loss': 1.64607, 'train_acc': 0.54116, 'val_loss': 1.93455, 'val_acc': 0.4838}\n",
      "Epoch 20/100\n",
      "------------------------------\n",
      "{'time': 115.37768, 'train_loss': 1.59411, 'train_acc': 0.55416, 'val_loss': 1.83484, 'val_acc': 0.509}\n",
      "Epoch 21/100\n",
      "------------------------------\n",
      "{'time': 114.27696, 'train_loss': 1.54817, 'train_acc': 0.56506, 'val_loss': 1.82741, 'val_acc': 0.5076}\n",
      "Epoch 22/100\n",
      "------------------------------\n",
      "{'time': 96.47324, 'train_loss': 1.49787, 'train_acc': 0.5779, 'val_loss': 1.89507, 'val_acc': 0.5008}\n",
      "Epoch 23/100\n",
      "------------------------------\n",
      "{'time': 107.30683, 'train_loss': 1.45968, 'train_acc': 0.58638, 'val_loss': 1.84055, 'val_acc': 0.523}\n",
      "Epoch 24/100\n",
      "------------------------------\n",
      "{'time': 96.87803, 'train_loss': 1.43042, 'train_acc': 0.59482, 'val_loss': 1.76962, 'val_acc': 0.5336}\n",
      "Epoch 25/100\n",
      "------------------------------\n",
      "{'time': 97.34134, 'train_loss': 1.39549, 'train_acc': 0.60158, 'val_loss': 1.80745, 'val_acc': 0.5266}\n",
      "Epoch 26/100\n",
      "------------------------------\n",
      "{'time': 108.2863, 'train_loss': 1.35386, 'train_acc': 0.61444, 'val_loss': 1.75381, 'val_acc': 0.5332}\n",
      "Epoch 27/100\n",
      "------------------------------\n",
      "{'time': 97.34118, 'train_loss': 1.32402, 'train_acc': 0.6192, 'val_loss': 1.72094, 'val_acc': 0.5492}\n",
      "Epoch 28/100\n",
      "------------------------------\n",
      "{'time': 97.19384, 'train_loss': 1.29021, 'train_acc': 0.63034, 'val_loss': 1.79228, 'val_acc': 0.542}\n",
      "Epoch 29/100\n",
      "------------------------------\n",
      "{'time': 104.73326, 'train_loss': 1.26263, 'train_acc': 0.63486, 'val_loss': 1.79173, 'val_acc': 0.5416}\n",
      "Epoch 30/100\n",
      "------------------------------\n",
      "{'time': 98.69518, 'train_loss': 1.23486, 'train_acc': 0.64434, 'val_loss': 1.74498, 'val_acc': 0.5518}\n",
      "Epoch 31/100\n",
      "------------------------------\n",
      "{'time': 97.96775, 'train_loss': 1.20866, 'train_acc': 0.64848, 'val_loss': 1.72808, 'val_acc': 0.5542}\n",
      "Epoch 32/100\n",
      "------------------------------\n",
      "{'time': 98.27276, 'train_loss': 1.17455, 'train_acc': 0.65916, 'val_loss': 1.74812, 'val_acc': 0.5512}\n",
      "Epoch 33/100\n",
      "------------------------------\n",
      "{'time': 107.76745, 'train_loss': 1.14272, 'train_acc': 0.6662, 'val_loss': 1.75096, 'val_acc': 0.5416}\n",
      "Epoch 34/100\n",
      "------------------------------\n",
      "{'time': 96.48531, 'train_loss': 1.12645, 'train_acc': 0.6676, 'val_loss': 1.75755, 'val_acc': 0.5484}\n",
      "Epoch 35/100\n",
      "------------------------------\n",
      "{'time': 96.48457, 'train_loss': 1.10107, 'train_acc': 0.67546, 'val_loss': 1.73334, 'val_acc': 0.5628}\n",
      "Epoch 36/100\n",
      "------------------------------\n",
      "{'time': 105.20366, 'train_loss': 1.08418, 'train_acc': 0.68024, 'val_loss': 1.70969, 'val_acc': 0.553}\n",
      "Epoch 37/100\n",
      "------------------------------\n",
      "{'time': 97.00424, 'train_loss': 1.06024, 'train_acc': 0.68726, 'val_loss': 1.74577, 'val_acc': 0.5498}\n",
      "Epoch 38/100\n",
      "------------------------------\n",
      "{'time': 97.79444, 'train_loss': 1.0387, 'train_acc': 0.69256, 'val_loss': 1.71211, 'val_acc': 0.572}\n",
      "Epoch 39/100\n",
      "------------------------------\n",
      "{'time': 107.07865, 'train_loss': 1.02425, 'train_acc': 0.6969, 'val_loss': 1.72328, 'val_acc': 0.5554}\n",
      "Epoch 40/100\n",
      "------------------------------\n",
      "{'time': 97.39994, 'train_loss': 0.99486, 'train_acc': 0.70228, 'val_loss': 1.74469, 'val_acc': 0.5604}\n",
      "Epoch 41/100\n",
      "------------------------------\n",
      "{'time': 97.58561, 'train_loss': 0.97247, 'train_acc': 0.7083, 'val_loss': 1.74621, 'val_acc': 0.561}\n",
      "Epoch 42/100\n",
      "------------------------------\n",
      "{'time': 107.57757, 'train_loss': 0.9618, 'train_acc': 0.71274, 'val_loss': 1.73505, 'val_acc': 0.5684}\n",
      "Epoch 43/100\n",
      "------------------------------\n",
      "{'time': 97.97969, 'train_loss': 0.94619, 'train_acc': 0.71606, 'val_loss': 1.74274, 'val_acc': 0.5662}\n",
      "Epoch 44/100\n",
      "------------------------------\n",
      "{'time': 97.52294, 'train_loss': 0.92477, 'train_acc': 0.72174, 'val_loss': 1.74829, 'val_acc': 0.5838}\n",
      "Epoch 45/100\n",
      "------------------------------\n",
      "{'time': 108.05701, 'train_loss': 0.9056, 'train_acc': 0.7254, 'val_loss': 1.74774, 'val_acc': 0.5728}\n",
      "Epoch 46/100\n",
      "------------------------------\n",
      "{'time': 96.82138, 'train_loss': 0.88929, 'train_acc': 0.73122, 'val_loss': 1.69156, 'val_acc': 0.5772}\n",
      "Epoch 47/100\n",
      "------------------------------\n",
      "{'time': 98.06289, 'train_loss': 0.86716, 'train_acc': 0.73816, 'val_loss': 1.79082, 'val_acc': 0.559}\n",
      "Epoch 48/100\n",
      "------------------------------\n",
      "{'time': 97.23806, 'train_loss': 0.85549, 'train_acc': 0.73986, 'val_loss': 1.76459, 'val_acc': 0.5688}\n",
      "Epoch 49/100\n",
      "------------------------------\n",
      "{'time': 105.04795, 'train_loss': 0.83373, 'train_acc': 0.74632, 'val_loss': 1.72883, 'val_acc': 0.5706}\n",
      "Epoch 50/100\n",
      "------------------------------\n",
      "{'time': 97.45451, 'train_loss': 0.82034, 'train_acc': 0.74948, 'val_loss': 1.84377, 'val_acc': 0.5534}\n",
      "Epoch 51/100\n",
      "------------------------------\n",
      "{'time': 96.3784, 'train_loss': 0.57929, 'train_acc': 0.82054, 'val_loss': 1.59655, 'val_acc': 0.6194}\n",
      "Epoch 52/100\n",
      "------------------------------\n",
      "{'time': 103.94655, 'train_loss': 0.50474, 'train_acc': 0.84388, 'val_loss': 1.61031, 'val_acc': 0.6136}\n",
      "Epoch 53/100\n",
      "------------------------------\n",
      "{'time': 103.19039, 'train_loss': 0.47801, 'train_acc': 0.8509, 'val_loss': 1.61326, 'val_acc': 0.6182}\n",
      "Epoch 54/100\n",
      "------------------------------\n",
      "{'time': 100.88325, 'train_loss': 0.45602, 'train_acc': 0.8575, 'val_loss': 1.66678, 'val_acc': 0.616}\n",
      "Epoch 55/100\n",
      "------------------------------\n",
      "{'time': 100.86494, 'train_loss': 0.44081, 'train_acc': 0.86164, 'val_loss': 1.67938, 'val_acc': 0.6216}\n",
      "Epoch 56/100\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 98.66107, 'train_loss': 0.43433, 'train_acc': 0.86398, 'val_loss': 1.67446, 'val_acc': 0.6266}\n",
      "Epoch 57/100\n",
      "------------------------------\n",
      "{'time': 97.24948, 'train_loss': 0.41584, 'train_acc': 0.86862, 'val_loss': 1.68098, 'val_acc': 0.6208}\n",
      "Epoch 58/100\n",
      "------------------------------\n",
      "{'time': 103.74064, 'train_loss': 0.41192, 'train_acc': 0.87014, 'val_loss': 1.69139, 'val_acc': 0.6194}\n",
      "Epoch 59/100\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ccbc23641bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update weights/biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new-tf-gpu/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new-tf-gpu/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### again 16\n",
    "### Train loop + validation/ also test at the end\n",
    "print(\"Configuration: \", \"model:ResNet(small)\", \" model_n:\", model_n, \" batch size:\", batch_size, \n",
    "      \" optimizer:SGD\", \" lr:\", lr, \" epochs:\", epochs)\n",
    "\n",
    "all_epoch_loss = {\"train\": [], \"validation\": []}\n",
    "all_epoch_acc = {\"train\":  [], \"validation\": []}\n",
    "\n",
    "print(\"----------------------------- Train --------------------------------\")\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    \n",
    "    epoch_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    epoch_acc = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    \n",
    "    running_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    running_corrects = {\"train\": 0, \"validation\": 0}\n",
    "    \n",
    "    for phase in [\"train\", \"validation\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train(True)\n",
    "        else:\n",
    "            model.train(False)\n",
    "        \n",
    "        for data in data_loaders[phase]:\n",
    "            inputs, labels = data \n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # clear all gradients\n",
    "            \n",
    "            outputs = model(inputs) # batch_size x num_classes\n",
    "            _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                loss.backward()  # compute gradients\n",
    "                optimizer.step() # update weights/biases\n",
    "               \n",
    "            running_loss[phase] += loss.data.item() * inputs.size(0)\n",
    "            running_corrects[phase] += torch.sum(preds == labels.data).item()\n",
    "        \n",
    "        all_epoch_loss[phase].append(running_loss[phase] / dataset_sizes[phase])\n",
    "        all_epoch_acc[phase].append(running_corrects[phase] / dataset_sizes[phase])\n",
    "        \n",
    "        epoch_loss[phase] = running_loss[phase] / dataset_sizes[phase]\n",
    "        epoch_acc[phase] =  running_corrects[phase] / dataset_sizes[phase]\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "        'time': np.round(time.time()-start_time, 5),\n",
    "        'train_loss': np.round(epoch_loss[\"train\"], 5),\n",
    "        'train_acc': np.round(epoch_acc[\"train\"], 5),\n",
    "        'val_loss': np.round(epoch_loss[\"validation\"], 5),\n",
    "        'val_acc': np.round(epoch_acc[\"validation\"], 5),\n",
    "    })\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    \n",
    "with open('ResNet_16_bs_32_opt_SGD_lr_1_epochs_100_loss.txt', 'w') as f_loss:\n",
    "    print(all_epoch_loss, file=f_loss)\n",
    "    \n",
    "with open('ResNet_16_bs_32_opt_SGD_lr_1_epochs_100_acc.txt', 'w') as f_acc:\n",
    "    print(all_epoch_acc, file=f_acc)\n",
    "    \n",
    "### evaluating the model with test set\n",
    "print(\"----------------------------- Test --------------------------------\")\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # clear all gradients\n",
    "\n",
    "        outputs = model(inputs) # batch_size x num_classes\n",
    "        _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        running_loss += loss.data.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "    'time': np.round(time.time()-start_time, 5),\n",
    "    'test_loss': np.round(running_loss/ dataset_sizes['test'], 5),\n",
    "    'test_acc': np.round(running_corrects/ dataset_sizes['test'], 5),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700a225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
