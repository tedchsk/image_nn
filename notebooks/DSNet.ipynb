{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a54325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPUs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor \n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPUs\")\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a8267d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "{'train': 50000, 'test': 5000, 'validation': 5000}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(43)\n",
    "batch_size = 128\n",
    "\n",
    "### for CIFAR 10\n",
    "# stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "## for CIFAR 100\n",
    "stats = ((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(*stats),\n",
    "    torchvision.transforms.RandomCrop(32, padding=4, padding_mode='constant'),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR100(root=\"data\", train=True, download=True, transform=transform)\n",
    "train_size = len(train_set)\n",
    "test_set = torchvision.datasets.CIFAR100(root=\"data\", train=False, download=True, transform=transform)\n",
    "test_set, validation_set = torch.utils.data.random_split(test_set, [5000, 5000])\n",
    "test_size = len(test_set)\n",
    "validation_size = len(validation_set)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size, num_workers=4, pin_memory=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "data_loaders = {\"train\": train_loader, \"test\": test_loader, \"validation\": validation_loader}\n",
    "dataset_sizes = {\"train\": train_size, \"test\": test_size, \"validation\": validation_size}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aff92af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/hub/pytorch_vision_resnet/\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic DSNet. Given input [in_channels, height, width], \n",
    "    - First pass through Conv2d(in_channels, in_channels) + BatchNorm + ReLU \n",
    "        -> Output dimensions: [in_channels, height, width] (1)\n",
    "    - Then, add with the (\"normalization and channel-wise weight\")(input)\n",
    "        -> Output dimensions: [in_channels, height width] (2)\n",
    "    - Pass through another Conv2d(outchannels, outchannels) + BN + ReLU\n",
    "        -> Output dimensions: [in_channels, height, width] (3)\n",
    "    - Add again with (\"normalized + channel-wise weight\")(1) and (\"normalized + channel-wise weight\")(2)\n",
    "        -> Output dimensions: [in_channels, height, width]\n",
    "    Caveat: The normalization and channel-wise weight is not shared.\n",
    "    Attributes:\n",
    "        in_planes: # of Input channels\n",
    "        n_models: Number of layers. Have to specify here as we need to connect all the layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, n_models, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        print(\"in block \", inplanes)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.channel_wise_w_list = []  # Result is list of list of weights at each steps\n",
    "        self.norm_layers = []\n",
    "        for i in range(n_models):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv2d(inplanes, inplanes, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(inplanes),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(inplanes, inplanes, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(inplanes)\n",
    "            ))\n",
    "\n",
    "            self.norm_layers.append(\n",
    "                [nn.GroupNorm(num_groups=4, num_channels=inplanes).to(device) for _ in range(i+1)]\n",
    "            )\n",
    "\n",
    "\n",
    "            # One variable for each channel for each time, [[w00], [w10, w11], [w20, w21, w22], ...]\n",
    "            self.channel_wise_w_list.append(\n",
    "                [torch.autograd.Variable(torch.randn(1, inplanes, 1, 1).to(device), requires_grad=True)\n",
    "                 ##nn.Parameter(torch.)\n",
    "                 for _ in range(i+1)]\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Consisting of output of each layer.\n",
    "        \n",
    "        print(\"---- in forward \")\n",
    "        print(\"len norm: \", len(self.norm_layers))\n",
    "        print(\"len weight: \", len(self.channel_wise_w_list))\n",
    "        print(\"len conv: \", len(self.layers))\n",
    "        outputs = [x]\n",
    "        for (layer, ch_ws, norm_layer) in zip(self.layers, self.channel_wise_w_list, self.norm_layers):\n",
    "            output = layer(outputs[-1])\n",
    "\n",
    "            assert len(outputs) == len(ch_ws), \"Length not equal\"\n",
    "            dense_normalized_inputs = [norm(x) * ch_weight\n",
    "                                       for output, ch_weight, norm in zip(outputs, ch_ws, norm_layer)]\n",
    "            for dense_normalized_input in dense_normalized_inputs:\n",
    "                output += dense_normalized_input\n",
    "\n",
    "            output = self.relu(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "        return outputs[-1]\n",
    "\n",
    "\n",
    "class TransitionBlock(nn.Module):\n",
    "    \"\"\"A transition block to reduce channels of [input, w, h] to [outplanes, w//2, h//2]\n",
    "    Attributes:\n",
    "        inplanes: # of Input channels\n",
    "        outplanes: # of Output channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, outplanes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, outplanes, stride=2, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(outplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(outplanes, outplanes, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(outplanes)\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            self.conv1, self.bn1, self.relu, self.conv2, self.bn2)\n",
    "\n",
    "        self.downsample = nn.Conv2d(\n",
    "            inplanes, outplanes, kernel_size=1, stride=2)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.block(x) + identity\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class DSNet(nn.Module):\n",
    "    \"\"\"Defining the whole model. \n",
    "    In high level: \n",
    "        - Input -> [batch, 3, height, width]\n",
    "        - Beginning Layer -> [batch, 3, height, width]\n",
    "        - First Block: n*BasicBlock(16) -> [batch, 16, height, width]\n",
    "        - Transition: TransitionBlock(16, 32) -> [batch, 32, height, width]\n",
    "        - Second Block: n*BasicBlock(32) -> [batch, 32, height, width]\n",
    "        - Transition: TransitionBlock(32, 64) -> [batch, 32, height, width]\n",
    "        - Third Block: n*BasicBlock(64) -> [batch, 64, height, width]\n",
    "        - FinalLayer: AdaptiveAvgPool2d + Linear(64, num_classes)\n",
    "    Attributes:\n",
    "        model_n: # of layers, based on CIFAR-ResNet \n",
    "        num_classes: Number of classes\n",
    "        device: needed for GPU vs CPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_n, num_classes: int = 10, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_layers = nn.ModuleList([])\n",
    "        self.model_n = model_n\n",
    "        self.device = device\n",
    "\n",
    "        # begining layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # ResNet blocks [16, 32, 64]\n",
    "        # first block, 16 channels\n",
    "        self.residual_layers.append(BasicBlock(\n",
    "            16, self.model_n, device).to(device))\n",
    "        self.residual_layers.append(TransitionBlock(16, 32).to(device))\n",
    "\n",
    "        # second block, 32 channels\n",
    "        self.residual_layers.append(BasicBlock(\n",
    "            32, self.model_n, device).to(device))\n",
    "        self.residual_layers.append(TransitionBlock(32, 64).to(device))\n",
    "\n",
    "        # third block, 64 channels\n",
    "        self.residual_layers.append(BasicBlock(\n",
    "            64, self.model_n, device).to(device))\n",
    "        self.residual_layers.append(TransitionBlock(64, 64).to(device))\n",
    "\n",
    "        # output layers\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # begining layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # DSNet blocks\n",
    "        for i, layer in enumerate(self.residual_layers):\n",
    "            x = layer(x)\n",
    "\n",
    "        # output layers\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94a632af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in block  16\n",
      " i  0\n",
      "len norm:  1\n",
      "len weight:  1\n",
      " i  1\n",
      "len norm:  2\n",
      "len weight:  2\n",
      " i  2\n",
      "len norm:  3\n",
      "len weight:  3\n",
      "in block  32\n",
      " i  0\n",
      "len norm:  1\n",
      "len weight:  1\n",
      " i  1\n",
      "len norm:  2\n",
      "len weight:  2\n",
      " i  2\n",
      "len norm:  3\n",
      "len weight:  3\n",
      "in block  64\n",
      " i  0\n",
      "len norm:  1\n",
      "len weight:  1\n",
      " i  1\n",
      "len norm:  2\n",
      "len weight:  2\n",
      " i  2\n",
      "len norm:  3\n",
      "len weight:  3\n"
     ]
    }
   ],
   "source": [
    "#### Train Configurations, based on DSNet and ResNet paper\n",
    "model_n = 3\n",
    "milestones = [90, 135]\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "gamma = 0.1\n",
    "lr = 0.1\n",
    "epochs = 100 ### should be 180\n",
    "\n",
    "model = DSNet(model_n, num_classes=100, device=device)\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aff90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:  model:DSNet(small)  model_n: 3  batch size: 128  optimizer:SGD  lr: 0.1  epochs: 100\n",
      "----------------------------- Train --------------------------------\n",
      "Epoch 1/100\n",
      "------------------------------\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n",
      "---- in forward \n",
      "len norm:  3\n",
      "len weight:  3\n",
      "len conv:  3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2668d7d4d9a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# clear all gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch_size x num_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# values, indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new-tf-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-1cfd25e759fe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# begining layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new-tf-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new-tf-gpu/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new-tf-gpu/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Train loop + validation/ also test at the end\n",
    "print(\"Configuration: \", \"model:DSNet(small)\", \" model_n:\", model_n, \" batch size:\", batch_size, \n",
    "      \" optimizer:SGD\", \" lr:\", lr, \" epochs:\", epochs)\n",
    "\n",
    "print(\"----------------------------- Train --------------------------------\")\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    \n",
    "    epoch_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    epoch_acc = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    \n",
    "    running_loss = {\"train\": 0.0, \"validation\": 0.0}\n",
    "    running_corrects = {\"train\": 0, \"validation\": 0}\n",
    "    \n",
    "    for phase in [\"train\", \"validation\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train(True)\n",
    "        else:\n",
    "            model.train(False)\n",
    "        \n",
    "        for data in data_loaders[phase]:\n",
    "            inputs, labels = data \n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # clear all gradients\n",
    "            \n",
    "            outputs = model(inputs) # batch_size x num_classes\n",
    "            _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                loss.backward()  # compute gradients\n",
    "                optimizer.step() # update weights/biases\n",
    "               \n",
    "            running_loss[phase] += loss.data.item() * inputs.size(0)\n",
    "            running_corrects[phase] += torch.sum(preds == labels.data).item()\n",
    "        \n",
    "        epoch_loss[phase] = running_loss[phase] / dataset_sizes[phase]\n",
    "        epoch_acc[phase] =  running_corrects[phase] / dataset_sizes[phase]\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "        'time': np.round(time.time()-start_time, 5),\n",
    "        'train_loss': np.round(epoch_loss[\"train\"], 5),\n",
    "        'train_acc': np.round(epoch_acc[\"train\"], 5),\n",
    "        'val_loss': np.round(epoch_loss[\"validation\"], 5),\n",
    "        'val_acc': np.round(epoch_acc[\"validation\"], 5),\n",
    "    })\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    \n",
    "### evaluating the model with test set\n",
    "print(\"----------------------------- Test --------------------------------\")\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # clear all gradients\n",
    "\n",
    "        outputs = model(inputs) # batch_size x num_classes\n",
    "        _, preds = torch.max(outputs.data, 1) # values, indices\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        running_loss += loss.data.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "    # Visualize the loss and accuracy values.\n",
    "    print({\n",
    "    'time': np.round(time.time()-start_time, 5),\n",
    "    'test_loss': np.round(running_loss/ dataset_sizes['test'], 5),\n",
    "    'test_acc': np.round(running_corrects/ dataset_sizes['test'], 5),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9786e9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             448\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "              ReLU-3           [-1, 16, 32, 32]               0\n",
      "            Conv2d-4           [-1, 16, 32, 32]           2,320\n",
      "       BatchNorm2d-5           [-1, 16, 32, 32]              32\n",
      "              ReLU-6           [-1, 16, 32, 32]               0\n",
      "            Conv2d-7           [-1, 16, 32, 32]           2,320\n",
      "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
      "              ReLU-9           [-1, 16, 32, 32]               0\n",
      "           Conv2d-10           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-11           [-1, 16, 32, 32]              32\n",
      "             ReLU-12           [-1, 16, 32, 32]               0\n",
      "           Conv2d-13           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-14           [-1, 16, 32, 32]              32\n",
      "             ReLU-15           [-1, 16, 32, 32]               0\n",
      "           Conv2d-16           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-17           [-1, 16, 32, 32]              32\n",
      "             ReLU-18           [-1, 16, 32, 32]               0\n",
      "           Conv2d-19           [-1, 16, 32, 32]           2,320\n",
      "      BatchNorm2d-20           [-1, 16, 32, 32]              32\n",
      "             ReLU-21           [-1, 16, 32, 32]               0\n",
      "       BasicBlock-22           [-1, 16, 32, 32]               0\n",
      "           Conv2d-23           [-1, 32, 16, 16]             544\n",
      "           Conv2d-24           [-1, 32, 16, 16]           4,640\n",
      "           Conv2d-25           [-1, 32, 16, 16]           4,640\n",
      "      BatchNorm2d-26           [-1, 32, 16, 16]              64\n",
      "      BatchNorm2d-27           [-1, 32, 16, 16]              64\n",
      "             ReLU-28           [-1, 32, 16, 16]               0\n",
      "             ReLU-29           [-1, 32, 16, 16]               0\n",
      "           Conv2d-30           [-1, 32, 16, 16]           9,248\n",
      "           Conv2d-31           [-1, 32, 16, 16]           9,248\n",
      "      BatchNorm2d-32           [-1, 32, 16, 16]              64\n",
      "      BatchNorm2d-33           [-1, 32, 16, 16]              64\n",
      "             ReLU-34           [-1, 32, 16, 16]               0\n",
      "             ReLU-35           [-1, 32, 16, 16]               0\n",
      "  TransitionBlock-36           [-1, 32, 16, 16]               0\n",
      "           Conv2d-37           [-1, 32, 16, 16]           9,248\n",
      "      BatchNorm2d-38           [-1, 32, 16, 16]              64\n",
      "             ReLU-39           [-1, 32, 16, 16]               0\n",
      "           Conv2d-40           [-1, 32, 16, 16]           9,248\n",
      "      BatchNorm2d-41           [-1, 32, 16, 16]              64\n",
      "             ReLU-42           [-1, 32, 16, 16]               0\n",
      "           Conv2d-43           [-1, 32, 16, 16]           9,248\n",
      "      BatchNorm2d-44           [-1, 32, 16, 16]              64\n",
      "             ReLU-45           [-1, 32, 16, 16]               0\n",
      "           Conv2d-46           [-1, 32, 16, 16]           9,248\n",
      "      BatchNorm2d-47           [-1, 32, 16, 16]              64\n",
      "             ReLU-48           [-1, 32, 16, 16]               0\n",
      "           Conv2d-49           [-1, 32, 16, 16]           9,248\n",
      "      BatchNorm2d-50           [-1, 32, 16, 16]              64\n",
      "             ReLU-51           [-1, 32, 16, 16]               0\n",
      "           Conv2d-52           [-1, 32, 16, 16]           9,248\n",
      "      BatchNorm2d-53           [-1, 32, 16, 16]              64\n",
      "             ReLU-54           [-1, 32, 16, 16]               0\n",
      "       BasicBlock-55           [-1, 32, 16, 16]               0\n",
      "           Conv2d-56             [-1, 64, 8, 8]           2,112\n",
      "           Conv2d-57             [-1, 64, 8, 8]          18,496\n",
      "           Conv2d-58             [-1, 64, 8, 8]          18,496\n",
      "      BatchNorm2d-59             [-1, 64, 8, 8]             128\n",
      "      BatchNorm2d-60             [-1, 64, 8, 8]             128\n",
      "             ReLU-61             [-1, 64, 8, 8]               0\n",
      "             ReLU-62             [-1, 64, 8, 8]               0\n",
      "           Conv2d-63             [-1, 64, 8, 8]          36,928\n",
      "           Conv2d-64             [-1, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-65             [-1, 64, 8, 8]             128\n",
      "      BatchNorm2d-66             [-1, 64, 8, 8]             128\n",
      "             ReLU-67             [-1, 64, 8, 8]               0\n",
      "             ReLU-68             [-1, 64, 8, 8]               0\n",
      "  TransitionBlock-69             [-1, 64, 8, 8]               0\n",
      "           Conv2d-70             [-1, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-71             [-1, 64, 8, 8]             128\n",
      "             ReLU-72             [-1, 64, 8, 8]               0\n",
      "           Conv2d-73             [-1, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-74             [-1, 64, 8, 8]             128\n",
      "             ReLU-75             [-1, 64, 8, 8]               0\n",
      "           Conv2d-76             [-1, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-77             [-1, 64, 8, 8]             128\n",
      "             ReLU-78             [-1, 64, 8, 8]               0\n",
      "           Conv2d-79             [-1, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-80             [-1, 64, 8, 8]             128\n",
      "             ReLU-81             [-1, 64, 8, 8]               0\n",
      "           Conv2d-82             [-1, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-83             [-1, 64, 8, 8]             128\n",
      "             ReLU-84             [-1, 64, 8, 8]               0\n",
      "           Conv2d-85             [-1, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-86             [-1, 64, 8, 8]             128\n",
      "             ReLU-87             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-88             [-1, 64, 8, 8]               0\n",
      "           Conv2d-89             [-1, 64, 4, 4]           4,160\n",
      "           Conv2d-90             [-1, 64, 4, 4]          36,928\n",
      "           Conv2d-91             [-1, 64, 4, 4]          36,928\n",
      "      BatchNorm2d-92             [-1, 64, 4, 4]             128\n",
      "      BatchNorm2d-93             [-1, 64, 4, 4]             128\n",
      "             ReLU-94             [-1, 64, 4, 4]               0\n",
      "             ReLU-95             [-1, 64, 4, 4]               0\n",
      "           Conv2d-96             [-1, 64, 4, 4]          36,928\n",
      "           Conv2d-97             [-1, 64, 4, 4]          36,928\n",
      "      BatchNorm2d-98             [-1, 64, 4, 4]             128\n",
      "      BatchNorm2d-99             [-1, 64, 4, 4]             128\n",
      "            ReLU-100             [-1, 64, 4, 4]               0\n",
      "            ReLU-101             [-1, 64, 4, 4]               0\n",
      " TransitionBlock-102             [-1, 64, 4, 4]               0\n",
      "AdaptiveAvgPool2d-103             [-1, 64, 1, 1]               0\n",
      "          Linear-104                  [-1, 100]           6,500\n",
      "================================================================\n",
      "Total params: 593,732\n",
      "Trainable params: 593,732\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.95\n",
      "Params size (MB): 2.26\n",
      "Estimated Total Size (MB): 8.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1497a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
